{
  
    
        "post0": {
            "title": "T5 论文笔记",
            "content": "论文地址: https://arxiv.org/abs/1910.10683 . 1. Introduction . 这一节简单介绍了T5的背景和目标。从图中可以看到, T5是一个text to text model, 直接把任务名和问题一起输入模型，模型就能输出答案。 这样设计的好处是，同一个模型可以用来解决不同的问题，使得multi task learning成为可能。 . . 2. Setup . 这一节主要介绍T5使用的模型和数据。 . 2.1 Model . 这一节介绍了T5使用的模型。早期的一些模型基于RNN, 作为例子这里引用了ULMFiT。目前有一种观点认为RNN(包括LSTM)已经过时了，现在必须用Transformer才够潮。我不同意这种观点：如果你比较贫穷，又想pretrain的话，ULMFiT仍然是个很不错的选择，它在比较少的数据和使用单个GPU就能达到很不错的效果。并且Transformer比RNN效果好并不是因为它在原理上更高明，而是在工程上它更容易scale到超大的模型和超大的数据集(也许这和在推荐系统里DNN能击败Xgboost有类似的原因)。如果有人能找到更快训练RNN的方法，也许哪一天RNN也能重新流行起来，事实上Transformer-XL里有一些结构就有RNN的影子。 . T5基于Transformer，和BERT不同的是，BERT只用了Transformer Encoder. 这样的问题导致BERT在做classification和sequence labeling(例如entity recognition)上效果不错，但是在seq2seq上(例如QA，Translation)效果就没那么好。所以在T5里同时使用了encoder和decoder, 并且把所有问题都统一为text to text. . T5的Transformer和最早的版本只有很小的区别: . remove layer norm bias | normalization outside residule path (原始transformer是先add(input, output)再norm, 这里变成了先norm(input)再add(input,output), 大概是这个意思。) | relative position embedding | 同时论文里也指出，他们没有在实验中验证这些区别能对performance产生多大影响。 . 2.2 The Colossal Clean Crawled Corpus . 这一节介绍T5 pretrain使用的dataset: C4. C4是Common Crawl的一个子集，只包含英文的data(使用langdetect判断), 并且使用了一些cleaning, 去掉重复、质量低的文章，把20TB的数据砍到了750GB. 具体处理见论文。 . pretrain只包含英文，可能是T5在translation上表现不佳的原因。 . 2.3 Downstream Tasks . 这里介绍了finetuning使用的task. 包括GLUE, SuperGLUE, SQuAD, WMT等等。 . 2.4 Input and Output Format . 这里介绍了是怎么样把各种不同的任务dataset转成T5所使用的input format. . 例如: . Translation task, Input 是”translate English to German: That is good.”, output是“Das ist gut.” | Classification task, 直接output label. | 等等. . 3. Experiments . T5最大的模型有11B个参数，这么大的模型不可能反复实验和调参，所以在pretrain之前，T5 team在小模型上做了很多实验，以找出最合适的model和pretrain task. 他们首先建立了一个baseline model，然后在上面应用各种实验，观察finetune的效果有什么变化。 . 这些实验包括: . model architectures (Section 3.2) | unsupervised objectives (Section 3.3) | pre-training data sets (Section 3.4) | transfer approaches (Section 3.5) | scaling (Section 3.6) | 3.1 Baseline . 这一节介绍baseline model. . 3.1.1 Model . (注意，这个是用来做实验的model，并不是最终的T5 model) 实验中使用了encoder-decoder Transformer model. 结构和BERT - base基本相同，所以参数量大约为后者的两倍。 . 3.1.2 Training . 这里介绍了training使用的一些参数, 这节比较有趣的地方在于给出了一些参数例如steps是怎么来的. . seq len = 512, batch size = 128, 使用了packing以后，一个batch里大约有512∗128=65536=216512*128 = 65536 = 2^{16}512∗128=65536=216个token. 训练的step=219step = 2^{19}step=219 , 所以整个training一共看了235=34B2^{35} = 34B235=34B个token. 这个模型仍然是在C4上train的，而34B远远小于C4的token数，所以模型并没有看过重复的data. . 在pretrain中，他们使用了learning_rate=1/max(n,k)learning _rate = 1/ sqrt{max(n,k)}learning_rate=1/max(n,k)​的方式, n为step, k为一个常数10410^4104 . 他们也提到了Howard等人提出的triangular learning rate会比这个效果稍好。顺便说一下可以去看看fast.ai上的讲解，Howard在图像和NLP模型里都用这个方法达到了不错的效果。之所以这里的experiment没有用这种方法是因为他们的一些实验需要training steps是变化的，而triangular需要一开始知道要训练多少步。 . 在finetune中，训练的step=218step = 2^{18}step=218. 因为finetune使用的数据集有大有小，这个step数是一个考虑到各个数据集的tradeoff. 使用了恒定的learning_rate=0.001learning _rate = 0.001learning_rate=0.001, 然后每隔5000个step记录结果并选择最好的validation checkpoint. 每个task的最好的performance checkpoint是独立选择的。 . 3.1.3 Vocabulary . T5使用了sentencepice作为tokenization的方案。因为最终要用到translation的方案，所以vocabulary也在German, Fresh, Romanian上fit过一遍。最终vocabulary大小为32k. 所以T5应该没法很好的处理中文和日文了，因为vocabuary里没有这些token, pretrain data里也不包含这些语言。 . 3.1.4 Unsupervised Objective . 最新的研究表明denoising objective相对于left-to-right LM是对pretrain比较有效的。所以T5使用了类似于BERT的objective. 但不同的是，因为T5是一个seq2seq model，所以跟BERT给input上加mask不同，T5是直接在output上做变换。例如: . Original Text: Thank you for inviting me to your party last week. | Inputs: Thank you me to your party week. | Ouput: for inviting last | . 具体做法是mask掉15%的token, 然后在输出时只输出被还原的token. 因为输出变得很短，这样能减少pretrain中的计算量。 . 3.1.5 Baseline Performance . 这里给出了Baseline model上train-finetune的performance, 以及和training from scratch的performance对比。 主要结论就是baseline model train-finetune的效果在几乎所有任务上都要显著比training from scratch好。所以这种seq2seq pretraining的方法看来是work的。 . 3.2 Architectures . 这一节就开始在baseline model上作出一些修改，进行各种个样的实验了。首先是Architecture部分。 . 3.2.1 Model Structures . 这一节讨论了各种模型结构上的不同。 . 首先是attention mask. attention mask是用来限制input和output之间的attention weight, 防止输出时”偷看答案”。 这里讨论的mask有三种: fully visible, causal(只能看到之前的) 和 causal with prefix(某个前缀 fully visible). . . 然后是architecture. 这里讨论了三种: encoder-decoder, language model, prefix LM. 这三个其实跟之前的mask是有关系的。 . 对于encoder-decoder, 因为输出和输入完全没有overlap, 所以encoder可以使用fully visible. decoder在training的时候不应该look into the future, 所以使用了causal. (这个mask应该是加在了decoder的self attention上， 至于decoder和encoder之间的attention应该是fully visible的) . 对于language model, 在predict next token的时候不能看到答案，所以要用causal mask. 注意encoder-decoder中的decoder实际上就是language model. . 因为language model中一个token只能attend到之前的token, 这会造成在一些任务例如翻译中损失很多信息。所以有了prefix LM来解决这个问题。修改mask让input中的前面一段可以fully visible. . . 论文里也提到了一点，就是prefix LM在classification任务中其实和BERT是几乎等效的，考虑以下例子: . BERT: [CLS] I hate pigeons. [SEP] My feelings towards pigeons are filled with animosity. =&gt; entailment . prefix LM: premise: I hate pigeons. hypothesis: My feelings towards pigeons are filled with animosity. =&gt; entailment . 这样，对mask的设定，两者是一样的。 . 3.2.2 Comparing Different Model Structures . 对不同的模型进行对比，应该基于类似的参数量和计算量来进行。但是问题在于，参数量和计算量是不成正比的。例如对于(L+L)层的encoder-decoder模型，它的参数量相当于2L层的language model(只有decoder), 但是计算量只相当于L层的language model. 这是因为对于language model，它必须同时对input和output进行计算; 而encoder只算input, decoder只算output. 基于这个原因，在后面比较的时候，同时列出了计算量和参数量。 . 3.2.3 Objectives . 这里的objective用了两种，denoising(类似BERT) 和 language model(predict next token). . 3.2.4 Results . 这里按照Architecture, Objective, 参数量，计算量去比较了各种实验的performance(Table 2). . 结论: . encoder-decoder的效果比language model和prefix LM要好，即使是在参数量一样，计算量减半的情况下。比较神奇的是，即使把encoder和decoder share参数，效果几乎也一样好。 | denoising效果总是比language model要好。 | 3.3 Unsupervised Objectives . 这里探讨了不同的unsupervised objectives的尝试。 . 3.3.1 Disparate High-Level Approaches . 从结果上来看：BERT style会比prefix LM和deshuffling要好，deshuffling是最差的。 . 3.3.2 Simplifying the BERT Objective . 这里又细分比较了BERT style的不同做法。其实看起来都差不多，最好的一种方法是replace corrupted spans. 这种方法在CoLA上有奇效，可能是因为里面有判断语法的问题，而语法判断和missing token是closely related. . 3.3.3 Varying the Corruption Rate . 这里还实验了不同的corruption rate, 看起来 10% - 25% 效果都差不多。 . 3.3.4 Corrupting Spans . 这里实验了在固定15%的corruption rate下，span length的影响。看起来影响比较小。 . 3.3.5 Discussion . 总结了一下实验的步骤。总的来说就是每次考虑一种变化，在固定最好的approach之后再继续后面的实验。 . . 3.4 Pre-training Data set . 这一节也比较有意思。它比较了不同pretrain dataset的影响。 . 3.4.1 Unlabeled Data Sets . 这里比较了不同的dataset, 比较有意思的发现: (注意因为只pretrain了235≈34B2^{35} approx 34B235≈34B tokens, 所以数据集大小并不代表全train完了) . C4(745 GB) 比 C4 unfiltered(6.1TB)效果好。说明了data cleaning的重要性。 | WebText-like(17GB)的效果很好。这个dataset只包含reddit上score &gt;= 3的内容。说明data quality很重要！读好书，才能成为一个好人。 | Wikipedia + Toronto Books Corpus效果好，单用Wikipedia效果不够好，说明domain knowledge很有用。 | 3.4.2 Pre-training Data set Size . 这里讨论了数据集大小的影响。在固定step=235≈34Bstep = 2^{35} approx 34Bstep=235≈34B tokens的前提下，把C4 truncate到229,227,225,2232^{29}, 2^{27}, 2^{25}, 2^{23}229,227,225,223的大小，分别相当于64,256,1024和4096个epoch. . 结论：随着dataset变小，模型效果下降。结合training loss曲线分析，如果重复过多次，会导致overfitting. 但是对于重复64次，模型效果下降不明显，说明少量重复(稍微多训练几个epoch)对效果影响不大。不过，对于pretraining来说，因为获取unlabeled data比较容易，多用data总是更好的。 . 经验规律：dataset中token的数量小于模型参数量的时候，会容易引发overfit. . 3.5 Training Strategy . 这里讨论了不同的training strategy. . 3.5.1 Fine-tuning Methods . 对于fine tuning, 讨论了几种不同的方法: . all parameters | adapter layers | gradual unfreezing. 这又是一个Howard提出的方法，可以去看fast.ai. 记得这种方法对image model很有效，特别是先unfreeze batch norm, 再unfreeze其他，会让收敛快很多。 | 最后实验的结果： . 跟gradual unfreezing比，其实直接fine tune所有参数效果比较好。不过gradual unfreeze会让训练更快一些。 | adapter layers参数越多效果越好（废话）。 | 3.5.2 Multi-task Learning . 在之前的实验里，都是先pretrain unsupervised再fine tune downstream. 其实也可以用multi task learning 把所有任务放在一起训练. 在这里，模型选择的时候，对每个task选择最好的checkpoint进行evaluate，而不是一共只选一个。 . 对于multi task learning, 一个很重要的问题是怎么选择不同task data的比例。这里提到的几种方法: . Examples-proportioinal mixing. 也就是按照每个task dataset的大小来进行mix。因为所有的dataset里，unsupervised的大小是最大的，并且占据绝对优势，所以这里在计算mix的时候进行了一个clipping, 把dataset example的最大数量给了一个限制K。 | Temperature-scaled mixing. 把某个task的mixingrate取一个1/T1/T1/T power,然后normalize. 这样可以照顾到数据比较少的dataset, 让对应的task可以充分训练。当T=1T=1T=1的时候相当于Examples-proportioinal mixing, 而当T→∞T rightarrow inftyT→∞的时候则相当于Equal mixing。 | Equal mixing. | 总体的结论: multitask training一般会比先pretrain再finetune差. 对不同的任务, K会有不同的最优值，而T有一个共同的最优值T=2T=2T=2. . 3.5.3 Combining Multi-Task Learning with Fine-Tuning . 其实在multi task和pretrain-finetune之外还有一种选择，就是leave one out multi task: pretrain的时候把所有其他task一起train, 然后在target task上fine tune. . 这里做了几种实验: . unsupervised pretrain + finetune (baseline) | multitask (去掉了了pretrain, 直接multitask downstream) | multitask pretrain + finetune | leave one out | supervised multi task pretrain | 结论： multi-task pretrain + finetune结果可以跟baseline差不多. leave one out效果没有下降很多，说明不同的task之间可以benefit。 去掉unsupervised以后效果很差，但是对translation效果影响不大，说明translation对pretraining的要求不高。 . 3.6 Scaling . 这一节回答这么一个问题： 假如你有两倍的计算能力，你应该把它花在哪里？是训练更大的模型，还是训练更长的时间？还是训练几个模型来ensemble? . 结论: . 模型大小和训练时间: 2×size+2×training_steps2 times size + 2 times training _steps2×size+2×training_steps 的效果约等于 4×size+1×training_steps4 times size + 1 times training _steps4×size+1×training_steps. | ensemble: 4×ensemble4 times ensemble4×ensemble 不如模型扩大4倍, 但它的提升是和其他方法正交的。 | 3.7 Putting It All Together . 这里介绍了最终模型使用的结构，参数和训练策略。基本上是从每个实验中取得最好的方法。然后列举了它在大量不同task上的performance。 . 4. Reflection . 这一章讲了T5研究中的一些反思。 . 4.1 Takeaways . 基本上又总结了一遍前面提到的各种发现。 . 4.2 Outlook . 一些存在的问题和之后的方向。 . 模型越大效果肯定越好，但也越不方便不实用，要把模型做小。 | denoising还不够，想要更好的pretrain方法。 | Formalizing the similarity between tasks: 使用in domain data pretrain能提高效果，但目前没有什么方法判断in domain. | Language-agnostic models: 发现只pretrain英语对translation task效果不够好。希望能找到(不需要multi lingual data)的更好方法。 |",
            "url": "https://blog.codescv.com/t5-paper-read.html",
            "relUrl": "/t5-paper-read.html",
            "date": " • Aug 4, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "t5学习笔记",
            "content": "t5是google ai在2020年发布的一个模型，它可以进行任意text2text的任务，并且可以支持multitask learning. 本文会用t5做一些试验。这些试验都在colab中运行。这里的代码参考了t5的官方例子 . Setup . 这里安装了t5所需要的依赖，并配置了一个免费的TPU. . 创建一个GCP Project，并配置一个GCS bucket. | Colab中 Runtime -&gt; Change Runtime Type, 选择TPU Runtime | 这里colab选择的tf版本是2.0，主要是为了在eager mode下连接TPU，并让TPU worker可以写gcs. t5目前基于tf1, 所以之后的代码全部是在tf1下运行的. 过程中会弹出一个验证框，登录google账号并输入验证码就可以了。 . GCS_BUCKET = &#39;your_gcs_bucket_name&#39; %tensorflow_version 2.x !pip install -q t5 import functools import os import time import warnings warnings.filterwarnings(&quot;ignore&quot;, category=DeprecationWarning) import tensorflow.compat.v1 as tf import tensorflow_datasets as tfds import t5 BASE_DIR = f&quot;gs://{GCS_BUCKET}/t5&quot; DATA_DIR = os.path.join(BASE_DIR, &quot;data&quot;) MODELS_DIR = os.path.join(BASE_DIR, &quot;models&quot;) print(&quot;Setting up GCS access...&quot;) import tensorflow_gcs_config from google.colab import auth # Set credentials for GCS reading/writing from Colab and TPU. TPU_TOPOLOGY = &quot;v2-8&quot; try: tpu = tf.distribute.cluster_resolver.TPUClusterResolver() # TPU detection TPU_ADDRESS = tpu.get_master() print(&#39;Running on TPU:&#39;, TPU_ADDRESS) except ValueError: raise BaseException(&#39;ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!&#39;) auth.authenticate_user() tf.config.experimental_connect_to_host(TPU_ADDRESS) tensorflow_gcs_config.configure_gcs_from_colab_auth() tf.disable_v2_behavior() # Improve logging. from contextlib import contextmanager import logging as py_logging tf.get_logger().propagate = False py_logging.root.setLevel(&#39;INFO&#39;) @contextmanager def tf_verbosity_level(level): og_level = tf.logging.get_verbosity() tf.logging.set_verbosity(level) yield tf.logging.set_verbosity(og_level) . Data . 这里用的数据集来自Quora的一个比赛: https://www.kaggle.com/c/quora-question-pairs 比赛中给定两个quora questions, 模型需要判断这两个问题是否具有相同的意义。例如 . q1: how i can be good at handwriting? q2: how can i have good handwriting? output: 1 q1: tea: is it dangerous to boil water in a plastic electric kettle? q2: why should we avoid hot tea in plastic cup? output: 0 . 虽然用t5来解决这么一个问题有点杀鸡用牛刀了，但是我一时间没找到什么有意思的问题，所以就先拿这个试试。 . training set有40万行data， test set有几百万行，因为有很多example是自动生成的，这些example并不会参与最终的score计算，只是为了防止作弊设置的。 . 下载kaggle data: . %%bash pip install kaggle mkdir -p ~/.kaggle echo &#39;{&quot;username&quot;:&quot;your_kaggle_username&quot;,&quot;key&quot;:&quot;your_kaggle_api_key&quot;}&#39; &gt; ~/.kaggle/kaggle.json chmod 600 ~/.kaggle/kaggle.json kaggle competitions download -c quora-question-pairs unzip test.csv.zip unzip train.csv.zip unzip sample_submission.csv.zip . 接下来我们从kaggle data生成tfrecord. 虽然tensorflow也可以直接解析csv文件，但是tf.decode_csv的实现不是很健壮，往往在csv里有特殊字符的时候容易出问题。 . 我们从原来的train.csv生成train.tfrecord和validation.tfrecord, 其中validation的比例为10%. . import pandas as pd import numpy as np VALIDATION_PERCENT = 0.1 def to_tf_example(split, values): def to_str_feature(v): return [str(v).encode(&#39;utf-8&#39;)] if split in (&#39;train&#39;, &#39;validation&#39;): features = { &#39;id&#39;: tf.train.Feature(bytes_list=tf.train.BytesList(value=to_str_feature(values[&#39;id&#39;]))), &#39;question1&#39;: tf.train.Feature(bytes_list=tf.train.BytesList(value=to_str_feature(values[&#39;question1&#39;]))), &#39;question2&#39;: tf.train.Feature(bytes_list=tf.train.BytesList(value=to_str_feature(values[&#39;question2&#39;]))), &#39;is_duplicate&#39;: tf.train.Feature(bytes_list=tf.train.BytesList(value=to_str_feature(values[&#39;is_duplicate&#39;]))), } else: features = { &#39;test_id&#39;: tf.train.Feature(bytes_list=tf.train.BytesList(value=to_str_feature(values[&#39;test_id&#39;]))), &#39;question1&#39;: tf.train.Feature(bytes_list=tf.train.BytesList(value=to_str_feature(values[&#39;question1&#39;]))), &#39;question2&#39;: tf.train.Feature(bytes_list=tf.train.BytesList(value=to_str_feature(values[&#39;question2&#39;]))), } example = tf.train.Example(features=tf.train.Features(feature=features)) return example print(&#39;Writing train/valid tfrecords...&#39;) train_df = pd.read_csv(&#39;train.csv&#39;) with tf.io.TFRecordWriter(&#39;train.tfrecord&#39;) as train_writer, tf.io.TFRecordWriter(&#39;validation.tfrecord&#39;) as validation_writer: for _idx, row in train_df.iterrows(): example = to_tf_example(&#39;train&#39;, row) if np.random.rand() &lt; VALIDATION_PERCENT: writer = validation_writer else: writer = train_writer writer.write(example.SerializeToString()) print(&#39;Writing test tfrecords...&#39;) test_df = pd.read_csv(&#39;test.csv&#39;) with tf.io.TFRecordWriter(&#39;test.tfrecord&#39;) as test_writer: for _idx, row in test_df.iterrows(): example = to_tf_example(&#39;test&#39;, row) test_writer.write(example.SerializeToString()) . 以下是train.csv的内容: . . 接下来把生成的tfrecord都copy到gcs上: . qp_data_base_dir = DATA_DIR + &#39;/quora-question-pairs&#39; print(&#39;copy tfrecord files to GCS:&#39;, qp_data_base_dir) !gsutil cp *.tfrecord {qp_data_base_dir} . Register T5 data Task . t5是一个text to text model，所以input和ouput都是text，而具体的格式可以自由设计。在这个问题中，我们的设计如下: . Input: is duplicate: &lt;question 1&gt; &lt;SEP&gt; &lt;question 2&gt; Output: 0 或 1 . 接下来，我们需要把这个训练任务注册到t5中。在任务中我们指定一个dataset_fn和一个text_preprocessor. 在后面我们可以看到，可以用同一个dataset加不同的preprocessor来训练不同的任务，非常方便。 . def qp_dataset_fn(split, shuffle_files=False): del shuffle_files # unused assert split in (&#39;train&#39;, &#39;validation&#39;, &#39;test&#39;), &quot;Invalid split&quot; ds = tf.data.TFRecordDataset(f&#39;{DATA_DIR}/quora-question-pairs/{split}.tfrecord&#39;) def parse_example(example_proto): feature_description = { &#39;question1&#39;: tf.io.FixedLenFeature([], tf.string, default_value=&#39;&#39;), &#39;question2&#39;: tf.io.FixedLenFeature([], tf.string, default_value=&#39;&#39;), &#39;is_duplicate&#39;: tf.io.FixedLenFeature([], tf.string, default_value=&#39;&#39;), } return tf.io.parse_single_example(example_proto, feature_description) ds = ds.map( parse_example, num_parallel_calls=tf.data.experimental.AUTOTUNE) return ds print(&#39;examples from train:&#39;) for ex in tfds.as_numpy(qp_dataset_fn(&quot;train&quot;).take(5)): print(ex) print(&#39;examples from test:&#39;) for ex in tfds.as_numpy(qp_dataset_fn(&quot;test&quot;).take(5)): print(ex) def qp_preprocessor(ds): def normalize_text(text): &quot;&quot;&quot;Lowercase and remove quotes from a TensorFlow string.&quot;&quot;&quot; text = tf.strings.lower(text) text = tf.strings.regex_replace(text,&quot;&#39;(.*)&#39;&quot;, r&quot; 1&quot;) return text def to_inputs_and_targets(ex): return { &quot;inputs&quot;: tf.strings.join( [&quot;is duplicate: &quot;, normalize_text(ex[&quot;question1&quot;]), &quot; &lt;SEP&gt; &quot;, normalize_text(ex[&quot;question2&quot;])]), &quot;targets&quot;: ex[&quot;is_duplicate&quot;] } return ds.map(to_inputs_and_targets, num_parallel_calls=tf.data.experimental.AUTOTUNE) t5.data.TaskRegistry.add( &quot;qp_is_duplicate&quot;, # Supply a function which returns a tf.data.Dataset. dataset_fn=qp_dataset_fn, splits=[&quot;train&quot;, &quot;validation&quot;, &quot;test&quot;], # Supply a function which preprocesses text from the tf.data.Dataset. text_preprocessor=[qp_preprocessor], # Lowercase targets before computing metrics. postprocess_fn=t5.data.postprocessors.lower_text, # We&#39;ll use accuracy as our evaluation metric. metric_fns=[t5.evaluation.metrics.accuracy], # Not required, but helps for mixing and auto-caching. # num_input_examples=num_nq_examples ) qp_task = t5.data.TaskRegistry.get(&quot;qp_is_duplicate&quot;) ds = qp_task.get_dataset(split=&quot;validation&quot;, sequence_length={&quot;inputs&quot;: 128, &quot;targets&quot;: 5}) print(&quot;A few preprocessed validation examples...&quot;) for ex in tfds.as_numpy(ds.take(5)): print(ex) . 可以从输出中看到qp_task中data的格式如下: . A few preprocessed validation examples... {&#39;inputs_plaintext&#39;: b&#39;is duplicate: how can one overcome the fear of speaking in public? &lt;SEP&gt; how could we overcome the fear of speaking truth?&#39;, &#39;inputs&#39;: array([ 19, 19197, 10, 149, 54, 80, 8269, 8, 2971, 13, 4461, 16, 452, 58, 3, 2, 134, 8569, 3155, 149, 228, 62, 8269, 8, 2971, 13, 4461, 2827, 58, 1]), &#39;targets_plaintext&#39;: b&#39;0&#39;, &#39;targets&#39;: array([ 3, 632, 1])} {&#39;inputs_plaintext&#39;: b&#39;is duplicate: how i can be good at handwriting? &lt;SEP&gt; how can i have good handwriting?&#39;, &#39;inputs&#39;: array([ 19, 19197, 10, 149, 3, 23, 54, 36, 207, 44, 609, 9933, 58, 3, 2, 134, 8569, 3155, 149, 54, 3, 23, 43, 207, 609, 9933, 58, 1]), &#39;targets_plaintext&#39;: b&#39;1&#39;, &#39;targets&#39;: array([209, 1])} ... . 需要注意的是，虽然我们的output是0或1，但是不能把output sequence length设成1，因为output是sentence piece, 从以上输出中看长度是3和2，所以设置一个大于3的值应该没问题。 . Model . 接下来我们开始模型训练。这个模型从pretrained_model_dir读取pretrain的t5模型，然后开始训练我们的text2text任务。 . def get_model(task_name, model_size=&quot;3B&quot;): model_size = &quot;3B&quot; # [&quot;small&quot;, &quot;base&quot;, &quot;large&quot;, &quot;3B&quot;, &quot;11B&quot;] # Public GCS path for T5 pre-trained model checkpoints BASE_PRETRAINED_DIR = &quot;gs://t5-data/pretrained_models&quot; pretrained_model_dir = os.path.join(BASE_PRETRAINED_DIR, model_size) model_dir = os.path.join(MODELS_DIR, task_name, model_size) # Set parallelism and batch size to fit on v2-8 TPU (if possible). # Limit number of checkpoints to fit within 5GB (if possible). model_parallelism, train_batch_size, keep_checkpoint_max = { &quot;small&quot;: (1, 256, 16), &quot;base&quot;: (2, 128, 8), &quot;large&quot;: (8, 64, 4), &quot;3B&quot;: (8, 16, 1), &quot;11B&quot;: (8, 16, 1)}[model_size] tf.io.gfile.makedirs(model_dir) # The models from our paper are based on the Mesh Tensorflow Transformer. model = t5.models.MtfModel( model_dir=model_dir, tpu=TPU_ADDRESS, tpu_topology=TPU_TOPOLOGY, model_parallelism=model_parallelism, batch_size=train_batch_size, sequence_length={&quot;inputs&quot;: 128, &quot;targets&quot;: 5}, learning_rate_schedule=0.003, save_checkpoints_steps=5000, keep_checkpoint_max=keep_checkpoint_max, iterations_per_loop=100, ) return model, pretrained_model_dir, model_dir model, pretrained_dir, model_dir = get_model(&#39;qp_is_duplicate&#39;) model.finetune( mixture_or_task_name=&quot;qp_is_duplicate&quot;, pretrained_model_dir=pretrained_dir, finetune_steps=25000 # 1 epoch ~= 25000 steps ) . 接下来我们在validation set上去看效果: . # Use a larger batch size for evaluation, which requires less memory. model.batch_size = 64 model.eval( mixture_or_task_name=&quot;qp_is_duplicate&quot;, checkpoint_steps=&quot;all&quot;, split=&quot;validation&quot; ) . output: . ... INFO:tensorflow:eval/qp_is_duplicate/accuracy at step 1025000: 90.693 . 可以看到accuracy达到了90.69%. . 原比赛衡量标准是logloss, 但是因为我们直接输出0或1，没法直接比较。我查看了一下讨论，排在11th的选手accuracy大约在0.87(threshold=0.5) / 0.91(best threshold)左右。 . https://www.kaggle.com/c/quora-question-pairs/discussion/33187 . 所以，在没有做任何特征工程，数据分析的情况下，我们只训练一个epoch，并且还没有用上全部数据，validation上的效果已经接近这个比赛的top水平了。 . Multi-task training . 只做一个classification太简单了，接下来我们尝试另一个更有意思的问题。我们来考虑另一个任务：给定一个问题，我们想直接生成一个相同意思，但是表达不同的问题。例如: . Input: what is the proper way to run long distance compared to that of short distance? Output: what is proper long distance running form? . 我们可以利用之前的dataset, 把label为1的样本拿出来做训练。 . def gsq_dataset_fn(split, shuffle_files=False): del shuffle_files # unused assert split in (&#39;train&#39;, &#39;validation&#39;), &quot;Invalid split&quot; ds = tf.data.TFRecordDataset(f&#39;{DATA_DIR}/quora-question-pairs/{split}.tfrecord&#39;) def parse_example(example_proto): feature_description = { &#39;question1&#39;: tf.io.FixedLenFeature([], tf.string, default_value=&#39;&#39;), &#39;question2&#39;: tf.io.FixedLenFeature([], tf.string, default_value=&#39;&#39;), &#39;is_duplicate&#39;: tf.io.FixedLenFeature([], tf.string, default_value=&#39;&#39;), } return tf.io.parse_single_example(example_proto, feature_description) ds = ds.map( parse_example, num_parallel_calls=tf.data.experimental.AUTOTUNE) ds = ds.filter(lambda ex: tf.equal(ex[&#39;is_duplicate&#39;], tf.constant(b&#39;1&#39;))) return ds print(&#39;examples from train:&#39;) for ex in tfds.as_numpy(gsq_dataset_fn(&quot;train&quot;).take(5)): print(ex) def gsq_preprocessor(ds): def normalize_text(text): &quot;&quot;&quot;Lowercase and remove quotes from a TensorFlow string.&quot;&quot;&quot; text = tf.strings.lower(text) text = tf.strings.regex_replace(text,&quot;&#39;(.*)&#39;&quot;, r&quot; 1&quot;) return text def to_inputs_and_targets(ex): return { &quot;inputs&quot;: tf.strings.join( [&quot;generate similar question: &quot;, normalize_text(ex[&quot;question1&quot;])]), &quot;targets&quot;: normalize_text(ex[&quot;question2&quot;]) } return ds.map(to_inputs_and_targets, num_parallel_calls=tf.data.experimental.AUTOTUNE) t5.data.TaskRegistry.add( &quot;generate_similar_question&quot;, # Supply a function which returns a tf.data.Dataset. dataset_fn=gsq_dataset_fn, splits=[&quot;train&quot;, &quot;validation&quot;], # Supply a function which preprocesses text from the tf.data.Dataset. text_preprocessor=[gsq_preprocessor], # Lowercase targets before computing metrics. postprocess_fn=t5.data.postprocessors.lower_text, # We&#39;ll use accuracy as our evaluation metric. metric_fns=[t5.evaluation.metrics.accuracy], # Not required, but helps for mixing and auto-caching. # num_input_examples=num_nq_examples ) qp_task = t5.data.TaskRegistry.get(&quot;generate_similar_question&quot;) ds = qp_task.get_dataset(split=&quot;validation&quot;, sequence_length={&quot;inputs&quot;: 128, &quot;targets&quot;: 128}) print(&quot;A few preprocessed validation examples...&quot;) for ex in tfds.as_numpy(ds.take(5)): print(ex) . 这里的代码跟之前非常相像，没什么可说的，区别主要在input中换了一个任务名，并且output sequence length不一样了。 . 接下来，我们可以不只训练这一个任务，我们可以把这个任务和之前的任务一起训练： . t5.data.MixtureRegistry.remove(&#39;qp_all&#39;) t5.data.MixtureRegistry.add( &quot;qp_all&quot;, [&quot;qp_is_duplicate&quot;, &quot;generate_similar_question&quot;], default_rate=1.0 ) mixed_task = t5.data.MixtureRegistry.get(&quot;qp_all&quot;) ds = mixed_task.get_dataset(split=&quot;validation&quot;, sequence_length={&quot;inputs&quot;: 128, &quot;targets&quot;: 128}) print(&quot;A few mixed task validation examples...&quot;) for ex in tfds.as_numpy(ds.take(10)): print(ex) . 这里我们生成了一个mixed task, 把两个任务混合到一起。这是t5的强大之处，因为可以支持任意的input格式，所以可以把不同的任务放在一起训练。 . 接下来训练模型： . task_name = &#39;qp_all&#39; model_size = &quot;3B&quot; # [&quot;small&quot;, &quot;base&quot;, &quot;large&quot;, &quot;3B&quot;, &quot;11B&quot;] # Public GCS path for T5 pre-trained model checkpoints BASE_PRETRAINED_DIR = &quot;gs://t5-data/pretrained_models&quot; pretrained_model_dir = os.path.join(BASE_PRETRAINED_DIR, model_size) model_dir = os.path.join(MODELS_DIR, task_name, model_size) # Set parallelism and batch size to fit on v2-8 TPU (if possible). # Limit number of checkpoints to fit within 5GB (if possible). model_parallelism, train_batch_size, keep_checkpoint_max = { &quot;small&quot;: (1, 256, 16), &quot;base&quot;: (2, 128, 8), &quot;large&quot;: (8, 64, 4), &quot;3B&quot;: (8, 16, 1), &quot;11B&quot;: (8, 16, 1)}[model_size] tf.io.gfile.makedirs(model_dir) # The models from our paper are based on the Mesh Tensorflow Transformer. model = t5.models.MtfModel( model_dir=model_dir, tpu=TPU_ADDRESS, tpu_topology=TPU_TOPOLOGY, model_parallelism=model_parallelism, batch_size=train_batch_size, sequence_length={&quot;inputs&quot;: 128, &quot;targets&quot;: 128}, learning_rate_schedule=0.003, save_checkpoints_steps=5000, keep_checkpoint_max=keep_checkpoint_max, iterations_per_loop=100, ) model.finetune( mixture_or_task_name=&quot;qp_all&quot;, pretrained_model_dir=pretrained_model_dir, finetune_steps=25000 # 1 epoch ~= 25000 steps ) . 然后我们再evaluate. 注意，我们训练了两个任务，但是evaluate仍然只evaluate第一个任务。 . # Use a larger batch size for evaluation, which requires less memory. model.batch_size = 64 model.eval( mixture_or_task_name=&quot;qp_is_duplicate&quot;, checkpoint_steps=&quot;all&quot;, split=&quot;validation&quot; ) . 可以看到，在只训练了半个epoch的情况下(因为step没变, 从一个task变成两个)accuracy甚至从90.6%上升到了90.93%。 . . 接下来，我们可以测试下模型是不是真的可以同时学会两个任务。 . now = time.time() questions = [ &quot;is duplicate: how have you simplified your life? &lt;SEP&gt; how do i simplify my messed up life?&quot;, &quot;is duplicate: Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me? &lt;SEP&gt; I&#39;m a triple Capricorn (Sun, Moon and ascendant in Capricorn) What does this say about me?&quot;, &quot;is duplicate: How do I read and find my YouTube comments? &lt;SEP&gt; How can I see all my Youtube comments?&quot;, &quot;generate similar question: how have you simplified your life?&quot;, &quot;generate similar question: I am a Capricorn Sun Cap moon and cap rising...what does that say about me?&quot;, &quot;generate similar question: How do I read and find my YouTube comments?&quot; ] # Write out the supplied questions to text files. predict_inputs_path = os.path.join(model_dir, &quot;predict_inputs_%d.txt&quot; % now) predict_outputs_path = os.path.join(model_dir, &quot;predict_outputs_%d.txt&quot; % now) # Manually apply preprocessing by prepending &quot;triviaqa question:&quot;. with tf.io.gfile.GFile(predict_inputs_path, &quot;w&quot;) as f: for q in questions: f.write(q.lower() + &#39; n&#39;) # Ignore any logging so that we only see the model&#39;s answers to the questions. with tf_verbosity_level(&#39;ERROR&#39;): model.batch_size = 8 # Min size for small model on v2-8 with parallelism 1. model.predict( input_file=predict_inputs_path, output_file=predict_outputs_path, # Select the most probable output token at each step. temperature=0, ) # The output filename will have the checkpoint appended so we glob to get # the latest. prediction_files = sorted(tf.io.gfile.glob(predict_outputs_path + &quot;*&quot;)) print(&quot; nPredictions using checkpoint %s: n&quot; % prediction_files[-1].split(&quot;-&quot;)[-1]) with tf.io.gfile.GFile(prediction_files[-1]) as f: for q, a in zip(questions, f): if q: print(&quot;Q: &quot; + q) print(&quot;A: &quot; + a) print() . output: . Predictions using checkpoint 1025000: Q: is duplicate: how have you simplified your life? &lt;SEP&gt; how do i simplify my messed up life? A: 1 Q: is duplicate: Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me? &lt;SEP&gt; I&#39;m a triple Capricorn (Sun, Moon and ascendant in Capricorn) What does this say about me? A: 1 Q: is duplicate: How do I read and find my YouTube comments? &lt;SEP&gt; How can I see all my Youtube comments? A: 1 Q: generate similar question: how have you simplified your life? A: what are some ways to simplify your life? Q: generate similar question: I am a Capricorn Sun Cap moon and cap rising...what does that say about me? A: i am a capricorn sun, cap moon and cap rising. what does that say about me? Q: generate similar question: How do I read and find my YouTube comments? A: how do i read my youtube comments? . 可以看到，模型真的学会了两个任务: 当你问is_duplicate的时候，它知道要输出0或1，而当你问generate similar question的时候，它会去生成一个相似的问题，非常的神奇。 .",
            "url": "https://blog.codescv.com/intro-t5.html",
            "relUrl": "/intro-t5.html",
            "date": " • Aug 2, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://blog.codescv.com/jupyter/test.html",
            "relUrl": "/jupyter/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://blog.codescv.com/markdown/test-markdown-post.html",
            "relUrl": "/markdown/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "分布式Tensorflow在Deep CTR模型上的实践",
            "content": "最近终于把deep模型推上线， 距离之前训练出sparse LR模型并上线已经过去了半年的时间。是时候总结一下了。 . (之前的分布式LR模型的文章在这里) . deep模型的结构 . deep ctr模型基本单元都差不多: linear, embedding, FM, FC, 基本就是这些东西的组合。例如WDL其实可以看作Linear+DNN, deepFM其实就是WDL里把Linear换成FM, sparse特征embedding后直接接FC, 或者embedding交叉后接FC, dense特征直接concat后FC, 等等。 . 在工程上主要需要考虑的地方是，模型总的大小是多少，每次数据传输有多少，计算量的瓶颈在什么地方。在ctr模型里，一般embedding肯定是最大的，但传输的时候，只需要传batch里涉及到的embedding. dense部分则每个batch训练时都要全部传输，所以也不能太大。另外dense部分的计算复杂度远高于embedding, 因为embedding只是查询和concat, 主要时间是花在网络传输上。 . 训练样本的分配 - revisit . 在之前那篇文章里，我使用了一个zookeeper来进行文件分配和状态管理。事实上在阿里巴巴最近开源的x-deeplearning里也是这么实现的。不过，使用zookeeper本身也有其它问题， 假如zookeeper不稳定的话，可能导致训练出问题，有时候文件列表很长造成zookeeper超时，也会给训练造成不必要的麻烦。基于这一点，我设计了另一种解决方案，不需要zookeeper, 也可以进行文件分配，并且更简单: . 每个worker拿到完整的文件列表，然后对文件名hash取模来决定自己要处理哪些文件。这里有一个坑，在python 3.x中，对于不同的虚拟机实例，hash(s)的值是会变化的，要取得固定 的hash，需要用md5之类的。 . 优雅停止 - revisit . 不使用zookeeper的前提下如何让PS感知到worker的停止呢？有两种方案：一种是PS训练完不停止(使用官方的join方法)，直接用一个定时任务去查k8s上master的状态，只要master停止就直接删除任务。另一种方案是把状态写到model_dir里，由于model_dir是一个共享文件夹, 在nfs或hdfs上，所以所有worker都能读到, 也是一种共享状态的方法。需要注意的坑 是worker可能挂掉重启，所以文件里记录的状态信息需要考虑这一点。 . 使用step, 而不是epoch . 在分布式训练中，最好使用step来控制训练的结束，而不是epoch. 为什么呢？因为每个worker速度不一样，如果限定epoch, 它也只是每个worker自己来算，所以速度取决于最慢的那个worker。假如某个worker出问题，或者重启了，那训练可能会很慢甚至永远结束不了。但是step是global的，即使某个worker出问题，只要global step达限，训练就能结束。虽然这样 一来样本不能完全保证每条都跑一样多遍，但是稳定性和速度都有很大提高。 . 模型导出 . 在LR的模型里，上线我没有用tf-serving, 主要因为LR模型很大，并且非0项又很少，用dense存储很不经济。但是deep模型会比LR模型小很多。这是因为LR里有大量交叉特征，这些特征在 deep模型里被处理成了embedding, 所以大大减小了。例如LR里两个特征交叉，权重的大小数量级在 (A⋅B)(A cdot B)(A⋅B) , 而如果是用embedding, 则大小为 (A+B)⋅k(A+B) cdot k(A+B)⋅k (k为embedding的维数), 这两者会有很大的差距。至于deep部分的参数，跟embedding比起来非常小，可以忽略不计。 最终我们线上deep模型大小一般在10-20G左右, 所以使用tf-serving是完全可行的。 . 假如使用Estimator, 那么模型导出已经实现好了。如果不用的话，则需要在做模型导出的时候，创建两张图，一张图用于训练，输入为TFRecord, 输出为optimizer; 另一张图用于导出模型，输入为一个placeholder, 输出为prediction, 两张图需要共享模型变量. 例如这样: . # model function with shared variables def model_fn(features): with tf.variable_scope(reuse=tf.AUTO_REUSE): y_pred = whatever_net(features) return y_pred # training graph features, label = dataset.make_one_shot_iterator().get_next() y_pred = model_fn(features) loss = make_loss(y_pred, label) train_op = optimizer.minimize(loss) # prediction graph example = tf.placeholder(dtype=tf.string) feature_ = parse_example(example) y_ = model_fn(feature_) while training: session.run(train_op) # export model tf.saved_model.simple_save( session, inputs={&#39;example&#39;: example}, outputs={&#39;pred&#39;: y_} ) . 测试模型 . 模型导出后，可以借助command line工具测试一下导出后的模型。使用方法在https://www.tensorflow.org/guide/saved_model#cli_to_inspect_and_execute_savedmodel . train/prediction一致性 . 一般来说使用tf-serving不太容易碰到线上线下不一致的问题。不过我们还是很幸运的遇到了。常见的问题有: . 训练时有dropout, 导出时的图要记得去掉。 | 训练时的batch norm, 导出时要把moving average设置为不更新。 | 性能 . 在LR模型中，由于计算量小，worker的CPU使用普遍很低。在deep模型里CPU使用率较高，一般集群中CPU能基本用满，而内存有较大富裕。 对于大量embedding的模型来说，GPU性价比仍然不高，所以我们也没有用GPU来进行训练。 . 网络带宽仍然是一大瓶颈，多个任务同时训练时，cpu很容易scale而网络却不容易scale。所以推荐样本使用GZIP压缩。 . 训练时如果遇到性能问题，可以先查查各个变量的大小，是否因为传输数据量不合理导致。这个是最常见的问题。推荐使用这里的方法. . 样本处理 . 样本处理中也有一些坑。在tensorflow里，缺失的embedding会变成0向量，无法训练，所以最好在样本处理时就给一个特殊的值，以免出问题。dense特征也需要额外的处理。 . 参数调整 . deep模型里一些参数也会比较影响效果。我这边的经验是: optimizer &gt; learning rate &gt; embedding size &gt; batch size &gt; regularization. 其中dropout基本不需要，用了以后效果极差。因为样本只跑一遍很难overfit, 所以l2 reg基本也不需要。 . Tensorflow的Adam optimizer在分布式上性能很差，应该是个bug, 不知道现在修了没有；据说有个LazyAdam, 我试了一下效果不行。所以最后用的是Adagrad. Learning rate对效果影响还是比较可观的，值得多调调。batch size对效果和训练速度都有影响，也可以调一下。有人反应同步 训练的效果会比异步训练效果好很多，我之前在LR上也发现了这一点，但是同步的稳定性和速度都要差很多，所以最后还是用了异步, 最终线上效果比LR涨了5%-10%的样子。 .",
            "url": "https://blog.codescv.com/dist-tf-deep-ctr.html",
            "relUrl": "/dist-tf-deep-ctr.html",
            "date": " • Jan 18, 2019"
        }
        
    
  
    
        ,"post5": {
            "title": "在Tensorflow中使用python来解析样本",
            "content": "Tensorflow中一般会使用TFRecord作为样本格式，这样可以有feature column, Estimator等比较方便的支持。 但有时TFRecord不能满足需求，或者只是想做一些quick and dirty的实验，这时可能用python解析样本会更方便。但是如果用feed dict + python的话，就用不了Dataset API了。 此时可以用py_func这个API来将任意python函数转换为tf operation. . 例如，我们的样本是如下的格式: . 1.0,2,33,252 0.7,1,34,323,112 . 其中第一个值为label, 后面不定长的整数列表为feature. 要解析这样一个样本，我们可以写一个python函数: . import tensorflow as tf import numpy as np def parse(line): label, *features = line.decode(&#39;utf-8&#39;).split(&#39;,&#39;) return np.float32(label), np.array([np.int64(feature) for feature in features]) . 接下来使用pyfunc来将它和Dataset API结合起来，就可以进行样本的解析了： . ds = tf.data.TextLineDataset(&#39;test.txt&#39;) line = ds.make_one_shot_iterator().get_next() label, feature = tf.py_func(parse, [line], [tf.float32, tf.int64]) try: with tf.Session() as sess: while True: print(sess.run([label, feature])) except tf.errors.OutOfRangeError: pass . 如果用了Dataset的batch方法，parse中的line会变成一个数组，在python中做相应的修改即可。 .",
            "url": "https://blog.codescv.com/tf-pyfunc.html",
            "relUrl": "/tf-pyfunc.html",
            "date": " • Jan 3, 2019"
        }
        
    
  
    
        ,"post6": {
            "title": "使用tfdbg Debug Tensorflow代码",
            "content": "由于Tensorflow使用静态图，先build graph然后run graph, 所以用一般的python debugger不能单步跟踪计算过程。如果想要debug 计算的中间过程, 一种简单的办法是在fetches中加入想要查看的tensor, 但是在调试中如果需要查看很多个tensor的值，这种方法不是很方便。 Tensorflow官方提供了tfdbg这个命令行工具来解决这个问题。 . 准备工作 . 使用tfdbg来包装Session . 对一般使用Session的Tensorflow程序，可以使用LocalCLIDebugWrapperSession来启用tfdbg，例如 . from tensorflow.python import debug as tf_debug sess = tf_debug.LocalCLIDebugWrapperSession(sess) . 使用hook来注入Estimator . 如果使用Estimator, 那么需要使用LocalCLIDebugHook来启用tfdbg, 例如 . from tensorflow.python import debug as tf_debug # Create a LocalCLIDebugHook and use it as a monitor when calling fit(). hooks = [tf_debug.LocalCLIDebugHook()] . tfdbg的使用 . 以debug_mnist.py这个脚本为例。 首先运行脚本: . python debug_mnist.py --debug . 进入tfdbg的主界面: . . 上面一行run #1: 1 fetch (accuracy/accuracy/Mean:0); 2 feeds表示当前这次Session.run的信息, 对应到代码里: . 135 for i in range(FLAGS.max_steps): 136 acc = sess.run(accuracy, feed_dict=feed_dict(False)) 137 print(&quot;Accuracy at step %d: %s&quot; % (i, acc)) . 在run_info里可以看到fetch是accuracy/accuracy/Mean:0, feed有两个:x-input:0和y-input:0. . 使用run命令运行一个完整的step . tfdbg&gt; run . 使用run命令可以进行一次Session.run. 执行后的结果如图: . 在这个界面可以看到运行这个step中所有的Operation, tensor的大小，以及运行时间。 点击其中的Tensor或者运行命令pt &lt;tensor_name&gt;,可以看见某个Tensor的值。例如点击Softmax:0以后，出现如下界面: . . 点击node_info查看该节点的输入输出，以及在代码的什么位置被定义. . 点击list_inputs和list_outputs可以查看输入输出的依赖树. . 在print_tensor界面可以看到这个Softmax的函数输出的形状是(10000, 10), 因为这是一个test batch, batch size是10000. . 使用pf命令可以打印feed, 从而验证这一点: . tfdbg&gt; pf input/x-input:0 . . 导出Tensor到文件 . 当Tensor比较大的时候，如果希望把Tensor导出进行进一步分析, 例如我们想导出hidden/weights/Variables:0, 可以用如下命令: . tfdbg&gt; eval -a &#39;`hidden/weights/Variable:0`&#39; -w &#39;/tmp/variable.npy&#39; 或者 tfdbg&gt; pt -s hidden/weights/Variable:0 -w &#39;/tmp/variable.npy&#39; . . 之后可以用numpy来读取这个变量, 例如 . import numpy as np var = np.load(&#39;/tmp/variable.npy&#39;) . eval命令还可以支持更加复杂的语法, 例如 . tfdbg&gt; eval &quot;np.matmul((`output/Identity:0` / `Softmax:0`).T, `Softmax:0`)&quot; . 单步跟踪 . 使用invoke_stepper命令进入单步模式: . . 接下来使用s命令就可以运行一个step(注意，这里step和之前step的概念不同) 使用s -t &lt;num&gt; 可以运行num个step. 使用exit运行余下的step并退出单步模式。 . filter(类似条件”断点”) . 默认情况下打印的tensor有点多，如果希望按照自己设定的条件来打印相关tensor, 可以使用filter. 例如，设置如下filter: . def my_filter_callable(datum, tensor): return &#39;Softmax&#39; in datum.tensor_name sess = tf_debug.LocalCLIDebugWrapperSession(sess, ui_type=FLAGS.ui_type) sess.add_tensor_filter(&#39;my_filter&#39;, my_filter_callable) . 则只有名称包含Softmax的tensor会被打印: . run -f my_filter . . 再比如，想要运行到包含nan或者inf(一般情况下意味着训练有问题)的tensor可以使用: . tfdbg&gt; run -f has_inf_or_nan . has_inf_or_nan是一个默认被注册的filter. .",
            "url": "https://blog.codescv.com/debug-tf-using-tfdbg.html",
            "relUrl": "/debug-tf-using-tfdbg.html",
            "date": " • Oct 14, 2018"
        }
        
    
  
    
        ,"post7": {
            "title": "Debug Tensorflow的C++代码",
            "content": "单步调试python代码，直接用IDE就可以了(例如Intellij Idea). 但如果要跟踪C++代码，就要使用GDB(Linux使用GDB, Mac OS上要用LLDB). . 准备工作 . 编译安装带调试信息的Tensorflow . 安装编译工具见: https://www.tensorflow.org/install/source . 首先使用-c dbg来编译安装tensorflow(以MacOS上编译tensorflow 1.8.0为例): . bazel build -c dbg //tensorflow/tools/pip_package:build_pip_package || exit 1 rm -rf /tmp/tf_pkg &amp;&amp; bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tf_pkg || exit 1 pip uninstall -y tensorflow || exit 1 echo &quot;Build Successful:&quot; $(ls /tmp/tf_pkg) pip install /tmp/tf_pkg/tensorflow-1.8.0-cp36-cp36m-macosx_10_13_x86_64.whl || exit 1 . 在Python脚本中加上中断 . 我们要在python加载pywrap_tensorflow.so之后用GDB/LLDB连上去, 所以为了让断点能生效，就在import tensorflow之后加个能让程序停 下来的代码(应该还有别的方式，我目前还没发现). 例如: . import tensorflow as tf import os input(&quot;pid: &quot; + str(os.getpid()) +&quot;, press enter to continue&quot;) a = tf.constant([[1,3]]) b = tf.constant([[3],[1]]) c = tf.matmul(a, b) with tf.Session() as sess: print(sess.run(c)) . 调试 . 使用GDB/LLDB 命令行 . 首先启动python程序: . $ python /tmp/test.py pid: 44806, press enter to continue . 我们看到pid是44806, 执行GDB/LLDB连上这个进程: . $ lldb -p 44806 (lldb) process attach --pid 44806 Process 44806 stopped * thread #1, queue = &#39;com.apple.main-thread&#39;, stop reason = signal SIGSTOP frame #0: 0x00007fff707ecbea libsystem_kernel.dylib`__read_nocancel + 10 libsystem_kernel.dylib`__read_nocancel: -&gt; 0x7fff707ecbea &lt;+10&gt;: jae 0x7fff707ecbf4 ; &lt;+20&gt; 0x7fff707ecbec &lt;+12&gt;: movq %rax, %rdi 0x7fff707ecbef &lt;+15&gt;: jmp 0x7fff707e3ae9 ; cerror_nocancel 0x7fff707ecbf4 &lt;+20&gt;: retq Target 0: (Python) stopped. Executable module set to &quot;/Users/chi/.pyenv/versions/3.6.4/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python&quot;. Architecture set to: x86_64h-apple-macosx. (lldb) . 此时Python程序已经被停下来了，接下来可以设置断点: . (lldb) breakpoint set -n TF_NewSession Breakpoint 1: where = _pywrap_tensorflow_internal.so`::TF_NewSession(TF_Graph *, const TF_SessionOptions *, TF_Status *) + 31 at c_api.cc:2474, address = 0x000000011191246f (lldb) breakpoint set -f matmul_op.cc -l 458 Breakpoint 2: 6 locations. (lldb) breakpoint list Current breakpoints: 1: name = &#39;TF_NewSession&#39;, locations = 1, resolved = 1, hit count = 0 1.1: where = _pywrap_tensorflow_internal.so`::TF_NewSession(TF_Graph *, const TF_SessionOptions *, TF_Status *) + 31 at c_api.cc:2474, address = 0x000000011191246f, resolved, hit count = 0 2: file = &#39;matmul_op.cc&#39;, line = 458, exact_match = 0, locations = 6, resolved = 6, hit count = 0 2.1: where = _pywrap_tensorflow_internal.so`tensorflow::MatMulOp&lt;Eigen::ThreadPoolDevice, float, false&gt;::Compute(tensorflow::OpKernelContext*) + 66 at matmul_op.cc:458, address = 0x0000000115428df2, resolved, hit count = 0 2.2: where = _pywrap_tensorflow_internal.so`tensorflow::MatMulOp&lt;Eigen::ThreadPoolDevice, double, false&gt;::Compute(tensorflow::OpKernelContext*) + 66 at matmul_op.cc:458, address = 0x000000011547df42, resolved, hit count = 0 2.3: where = _pywrap_tensorflow_internal.so`tensorflow::MatMulOp&lt;Eigen::ThreadPoolDevice, Eigen::half, false&gt;::Compute(tensorflow::OpKernelContext*) + 66 at matmul_op.cc:458, address = 0x00000001154d14d2, resolved, hit count = 0 2.4: where = _pywrap_tensorflow_internal.so`tensorflow::MatMulOp&lt;Eigen::ThreadPoolDevice, int, false&gt;::Compute(tensorflow::OpKernelContext*) + 66 at matmul_op.cc:458, address = 0x0000000115524702, resolved, hit count = 0 2.5: where = _pywrap_tensorflow_internal.so`tensorflow::MatMulOp&lt;Eigen::ThreadPoolDevice, std::__1::complex&lt;float&gt;, false&gt;::Compute(tensorflow::OpKernelContext*) + 66 at matmul_op.cc:458, address = 0x000000011557a962, resolved, hit count = 0 2.6: where = _pywrap_tensorflow_internal.so`tensorflow::MatMulOp&lt;Eigen::ThreadPoolDevice, std::__1::complex&lt;double&gt;, false&gt;::Compute(tensorflow::OpKernelContext*) + 66 at matmul_op.cc:458, address = 0x00000001155d3a22, resolved, hit count = 0 . 设置了两个断点，第一个是TF_NewSession函数，第二个是matmul_op.cc的第458行. LLDB的相关文档见https://developer.apple.com/library/archive/documentation/General/Conceptual/lldb-guide/chapters/C3-Breakpoints.html. . 接下来可以让程序继续运行. 在LLDB命令行下输入 . (lldb) c Process 44806 resuming . 在python程序中按下回车, 可以看到LLDB已经运行到了断点, 进入了TF_NewSession: . Process 44806 stopped * thread #1, queue = &#39;com.apple.main-thread&#39;, stop reason = breakpoint 1.1 frame #0: 0x000000011191246f _pywrap_tensorflow_internal.so`::TF_NewSession(graph=0x00007faaff47e310, opt=0x00007faafe87d530, status=0x00007faafc4dbc00) at c_api.cc:2474 2471 TF_Session* TF_NewSession(TF_Graph* graph, const TF_SessionOptions* opt, 2472 TF_Status* status) { 2473 Session* session; -&gt; 2474 status-&gt;status = NewSession(opt-&gt;options, &amp;session); 2475 if (status-&gt;status.ok()) { 2476 TF_Session* new_session = new TF_Session(session, graph); 2477 if (graph != nullptr) { Target 0: (Python) stopped. . 接下来就可以单步跟踪了。 . 使用IDE(VSCode) . GDB/LLDB只有命令行界面，使用比较麻烦，如果想用IDE可以使用VSCode. . 安装插件 . 在MacOS上，需要运行 . ln -s /Applications/Xcode.app/Contents/Developer/usr/bin/lldb-mi /usr/local/bin/lldb-mi . 把lldb-mi放进$PATH里，才能和IDE进行连接。 . 首先安装一个叫Native Debug的插件: . 然后点击debug标签下的配置按钮，创建一个debug配置(launch.json): . launch.json的内容如下: . { &quot;version&quot;: &quot;0.2.0&quot;, &quot;configurations&quot;: [ { &quot;name&quot;: &quot;(lldb) Attach&quot;, &quot;type&quot;: &quot;cppdbg&quot;, &quot;request&quot;: &quot;attach&quot;, &quot;program&quot;: &quot;/Users/chi/.pyenv/versions/3.6.4/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python&quot;, &quot;processId&quot;: &quot;${command:pickProcess}&quot;, &quot;MIMode&quot;: &quot;lldb&quot; }, ] } . 设置断点 . 在VSCode里打开tensorflow代码目录，设置断点即可，例如: . 进行调试 . 首先正常启动python程序: . $ python /tmp/test.py pid: 45874, press enter to continue . 然后在VSCode里点击运行按钮，运行刚才的”(lldb) Attach”: 在输入框中找到目标python程序并点击(这里是python test.py的那个), 就可以进入调试模式. 进入python的窗口按下回车，让python程序继续运行，可以看到程序停在TF_NewSession函数上. 接下来就可以使用IDE中的按钮进行单步跟踪或者变量查看了。 .",
            "url": "https://blog.codescv.com/debug-tf-cpp.html",
            "relUrl": "/debug-tf-cpp.html",
            "date": " • Oct 8, 2018"
        }
        
    
  
    
        ,"post8": {
            "title": "Debug Tensorflow PS的数据传输",
            "content": "在分布式训练中，有时会碰到PS传输数据量很大的情况。这时候，可以在代码中加log来帮助找出哪个tensor消耗比较大。 . 在grpc_remote_worker.cc中添加如下代码: . void RecvTensorAsync(CallOptions* call_opts, const RecvTensorRequest* request, TensorResponse* response, StatusCallback done) override { VLOG(1) &lt;&lt; &quot;RecvTensorAsync req: &quot; &lt;&lt; request-&gt;DebugString(); int64 start_usec = Env::Default()-&gt;NowMicros(); // Type-specialized logging for this method. bool logging_active = logger_-&gt;LoggingActive() || VLOG_IS_ON(2) || true; StatusCallback wrapper_done; const StatusCallback* cb_to_use; if (!logging_active) { cb_to_use = &amp;done; // No additional work to do, so just use done directly } else { wrapper_done = [this, request, response, done, start_usec](Status s) { int64 bytes = response-&gt;tensor().TotalBytes(); const string&amp; key = request-&gt;rendezvous_key(); std::vector&lt;string&gt; key_parts = str_util::Split(key, &#39;;&#39;); LOG(INFO) &lt;&lt; &quot;recv tensor name: &quot; &lt;&lt; key_parts[3] &lt;&lt; &quot; src: &quot; &lt;&lt; key_parts[0] &lt;&lt; &quot; dest: &quot; &lt;&lt; key_parts[2] &lt;&lt; &quot; bytes: &quot; &lt;&lt; bytes; . 就可以输出日志: . recv tensor name: xx src: /job:ps/replica:0/task:0/device:CPU:0 dest: /job:worker/replica:0/task:0/device:CPU:0 bytes: 20889600 . 这样从src传输到dest, tensor名称，大小都可以看到了。 .",
            "url": "https://blog.codescv.com/debug-dist-tf-ps.html",
            "relUrl": "/debug-dist-tf-ps.html",
            "date": " • Sep 26, 2018"
        }
        
    
  
    
        ,"post9": {
            "title": "Recursive Genenerators in Python",
            "content": "假设想写一个In order traversal, 在python里是这样: . result = [] def inorder(p): if p is None: return inorder(p.left) result.append(p.val) inorder(p.right) . 如果想用generator实现这个算法应该怎么做呢? . def inorder(p): if p is None: return inorder(p.left) yield p.val inorder(p.right) for v in inorder(tree): print(v) . 会发现只能打印出root节点。问题出在哪里呢？因为yield在递归调用中并不会起作用。在第一次调用inorder的时候，yield出了p.val, 但是里面的inorder调用并不会yield出值来。要想让递归的调用里也能往外yield, 需要显式指明这一点: . def inorder(p): if p is None: return yield from inorder(p.left) yield p.val yield from inorder(p.right) .",
            "url": "https://blog.codescv.com/python-recursive-generators.html",
            "relUrl": "/python-recursive-generators.html",
            "date": " • Aug 14, 2018"
        }
        
    
  
    
        ,"post10": {
            "title": "分布式TensorFlow在Sparse模型上的实践",
            "content": "前言 . 如果你去搜TensorFlow教程，或者分布式TensorFlow教程，可能会搜到很多mnist或者word2vec的模型，但是对于Sparse模型(LR, WDL等)，研究的比较少，对于分布式训练的研究就更少了。最近刚刚基于分布式TensorFlow上线了一个LR的CTR模型，支持百亿级特征，训练速度也还不错，故写下一些个人的心得体会，如果有同学也在搞类似的模型欢迎一起讨论。 . 训练相关 . 集群setup . 分布式TensorFlow中有三种角色: Master(Chief), PS和Worker. 其中Master负责训练的启动和停止，全局变量的初始化等；PS负责存储和更新所有训练的参数(变量); Worker负责每个mini batch的梯度计算。如果去网上找分布式TensorFlow的教程，一个普遍的做法是使用Worker 0作为Master, 例如这里官方给出的例子。我个人比较建议把master独立出来，不参与训练，只进行训练过程的管理, 这样有几个好处: . 独立出Master用于训练的任务分配，状态管理，Evaluation, Checkpoint等，可以让代码更加清晰。 | Master由于任务比worker多，往往消耗的资源如CPU/内存也和worker不一样。独立出来以后，在k8s上比较容易分配不同的资源。 | 一些训练框架如Kubeflow使用Master的状态来确定整个任务的状态。 | 训练样本的分配 . 在sparse模型中，采用分布式训练的原因一般有两种: . 数据很多，希望用多个worker加速。 | 模型很大，希望用多个PS来加速。 | 前一个就涉及到训练样本怎么分配的问题。从目前网上公开的关于分布式TF训练的讨论，以及官方分布式的文档和教程中，很少涉及这个问题，因此最佳实践并不清楚。我目前的解决方法是引入一个zookeeper进行辅助。由master把任务写到zookeeper, 每个worker去读取自己应该训练哪些文件。基本流程如下: . # master file_list = list_dir(flags.input_dir) for worker, worker_file in zip(workers, file_list): self.zk.create(f&#39;/jobs/{worker}&#39;, &#39;,&#39;.join(worker_file).encode(&#39;utf8&#39;), makepath=True) # start master session ... # worker wait_for_jobs = Semaphore(0) @self.zk.DataWatch(f&quot;/jobs/{worker}&quot;) def watch_jobs(jobs, _stat, _event): if jobs: data_paths = jobs.decode(&#39;utf8&#39;).split(&#39;,&#39;) self.train_data_paths = data_paths wait_for_jobs.release() wait_for_jobs.acquire() # start worker session ... . 引入zookeeper之后，文件的分配就变得非常灵活，并且后续也可以加入使用master监控worker状态等功能。 . 使用device_filter . 启动分布式TF集群时，每个节点都会启动一个Server. 默认情况下，每个节点都会跟其他节点进行通信，然后再开始创建Session. 在集群节点多时，会带来两个问题： . 由于每个节点两两通信，造成训练的启动会比较慢。 | 当某些worker挂掉重启（例如因为内存消耗过多），如果其他某些worker已经跑完结束了，重启后的worker就会卡住，一直等待其他的worker。此时会显示log: CreateSession still waiting for response from worker: /job:worker/replica:0/task:x. | 解决这个问题的方法是使用device filter. 例如: . config = tf.ConfigProto(device_filters=[&quot;/job:ps&quot;, f&quot;/job:{self.task_type}/task:{self.task_index}&quot;]) with tf.train.MonitoredTrainingSession( ..., config=config ): ... . 这样，每个worker在启动时就只会和所有的PS进行通信，而不会和其他worker进行通信，既提高了启动速度，又提高了健壮性。 . 给session.run加上timeout . 有时在一个batch比较大时，由于HDFS暂时不可用或者延时高等原因，有可能会导致session.run卡住。可以给它加一个timeout来提高程序的健壮性。 . session.run(fetches, feed_dict, options=tf.RunOptions(timeout_in_ms=timeout)) . 优雅的停止训练 . 在很多分布式的例子里(例如官方的这个)，都没有涉及到训练怎么停止的问题。通常的做法都是这样: . server = tf.train.Server( cluster, job_name=FLAGS.job_name, task_index=FLAGS.task_index) if FLAGS.job_name == &quot;ps&quot;: server.join() . 这样的话，一旦开始训练，PS就会block, 直到训练结束也不会停止，只能暴力删除任务(顺便说一下，MXNet更粗暴，PS运行完import mxnet就block了)。前面我们已经引入了zookeeper, 所以可以相对比较优雅的解决这个问题。master监控worker的状态，当worker全部完成后，master设置自己的状态为完成。PS和worker检测到master完成后，就结束自己。 . def join(): while True: status = self.zk.get_async(&#39;/status/master&#39;).get(timeout=3) if status == &#39;done&#39;: break time.sleep(10) # master for worker in workers: status = self.zk.get_async(f&#39;/status/worker/{worker}&#39;).get(timeout=3) update_all_workers_done() if all_workers_done: self.zk.create(&#39;/status/master&#39;, b&#39;done&#39;, makepath=True) # ps join() # worker while has_more_data: train() self.zk.set(&#39;/status/worker/{worker}&#39;, b&#39;done&#39;) join() . Kubeflow的使用 . Kubeflow是一个用于分布式TF训练提交任务的项目。它主要由两个部分组成，一是ksonnet，一是tf-operator。ksonnet主要用于方便编写k8s上的manifest, tf-operator是真正起作用的部分，会帮你设置好TF_CONFIG这个关键的环境变量，你只需要设置多少个PS, 多少个worker就可以了。个人建议用ksonnet生成一个skeleton, 把tf-operator的manifest打印出来(使用ks show -c default), 以后就可以跳过ksonnet直接用tf-operator了。因为k8s的manifest往往也需要修改很多东西，最后还是要用程序生成，没必要先生成ksonnet再生成yaml. . Estimator的局限性 . 一开始我使用的是TF的高级API Estimator, 参考这里的WDL模型. 后来发现Estimator有一些问题，没法满足目前的需要。 . 一是性能问题。我发现同样的linear model, 手写的模型是Estimator的性能的5-10倍。具体原因尚不清楚，有待进一步调查。 . UPDATE: 性能问题的原因找到了，TL;DR: 在sparse场景使用Dataset API时, 先batch再map(parse_example)会更快。详情见: https://stackoverflow.com/questions/50781373/using-feed-dict-is-more-than-5x-faster-than-using-dataset-api . 二灵活性问题。Estimator API目前支持分布式的接口是train_and_evaluate, 它的逻辑可以从源代码里看到: . # training.py def run_master(self): ... self._start_distributed_training(saving_listeners=saving_listeners) if not evaluator.is_final_export_triggered: logging.info(&#39;Training has already ended. But the last eval is skipped &#39; &#39;due to eval throttle_secs. Now evaluating the final &#39; &#39;checkpoint.&#39;) evaluator.evaluate_and_export() # estimator.py def export_savedmodel(...): ... with tf_session.Session(config=self._session_config) as session: saver_for_restore = estimator_spec.scaffold.saver or saver.Saver( sharded=True) saver_for_restore.restore(session, checkpoint_path) ... builder = ... builder.save(as_text) . 可以看到Estimator有两个问题，一是运行完才进行evaluate, 但在大型的sparse model里，训练一轮可能要几个小时，我们希望一边训练一边evaluate, 在训练过程中就能尽快的掌握效果的变化(例如master只拿到test set, worker只拿到training set, 这样一开始就能evaluate)。二是每次导出模型时，Estimator API会新建一个session, 加载最近的一个checkpoint, 然后再导出模型。但实际上，一个sparse模型在TF里耗的内存可能是100多G，double一下会非常浪费，而且导出checkpoint也极慢，也是需要避免的。基于这两个问题，我暂时没有使用Estimator来进行训练。 . 使用feature column(但不使用Estimator) . 虽然不能用Estimator, 但是使用feature column还是完全没有问题的。不幸的是网上几乎没有人这么用，所以我把我的使用方法分享一下，这其实也是Estimator的源代码里的使用方式: . # define feature columns as usual feature_columns = ... example = TFRecordDataset(...).make_one_shot_iterator().get_next() parsed_example = tf.parse_example(example, tf.feature_column.make_parse_example_spec(feature_columns)) logits = tf.feature_column.linear_model( features=parsed_example, feature_columns=feature_columns, cols_to_vars=cols_to_vars ) . parse_example会把TFRecord解析成一个feature dict, key是feature的名称，value是feature的值。tf.feature_column.linear_model这个API会自动生成一个线性模型，并把变量(weight)的reference, 存在cols_to_vars这个字典中。如果使用dnn的模型，可以参考tf.feature_column.input_layer这个API, 用法类似，会把feature dict转换成一个行向量。 . 模型导出 . 使用多PS时，常见的做法是使用tf.train.replica_device_setter来把不同的变量放在不同的PS节点上。有时一个变量也会很大(这在sparse模型里很常见)，也需要拆分到不同的PS上。这时候就需要使用partitioner: . partitioner = tf.min_max_variable_partitioner( max_partitions=variable_partitions, min_slice_size=64 &lt;&lt; 20) with tf.variable_scope( ..., partitioner = partitioner ): # define model . 由于TensorFlow不支持sparse weight(注意: 跟sparse tensor是两码事，这里指的是被训练的变量不能是sparse的), 所以在PS上存的变量还是dense的，比如你的模型里有100亿的参数，那PS上就得存100亿。但实际上对于sparse模型，大部分的weight其实是0，最终有效的weight可能只有0.1% - 1%. 所以可以做一个优化，只导出非0参数: . indices = tf.where(tf.not_equal(vars_concat, 0)) values = tf.gather(vars_concat, indices) shape = tf.shape(vars_concat) self.variables[column_name] = { &#39;indices&#39;: indices, &#39;values&#39;: values, &#39;shape&#39;: shape, } . 这样就把变量的非0下标，非0值和dense shape都放进了self.variables里，之后就只导出self.variables即可。 . 此外, 在多PS的时候，做checkpoint的时候一定要记得enable sharding: . saver = tf.train.Saver(sharded=True, allow_empty=True) ... . 这样在master进行checkpoint的时候，每个PS会并发写checkpoint文件。否则，所有参数会被传到master上进行集中写checkpoint, 消耗内存巨大。 . 导出优化 . 前面提到的导出非0参数的方法仍有一个问题，就是会把参数都传到一台机器(master)上，计算出非0再保存。这一点可以由grafana上的监控验证: . . 可以看到，每隔一段时间进行模型导出时，master的网络IO就会达到一个峰值，是平时的好几倍。如果能把non_zero固定到PS上进行计算，则没有这个问题。代码如下: . for var in var_or_var_list: var_shape = var._save_slice_info.var_shape var_offset = var._save_slice_info.var_offset with tf.device(var.device): var_ = tf.reshape(var, [-1]) var_indices = tf.where(tf.not_equal(var_, 0)) var_values = tf.gather(var_, var_indices) indices.append(var_indices + var_offset[0]) values.append(var_values) indices = tf.concat(indices, axis=0) values = tf.concat(values, axis=0) . 进行了如上修改之后，master的网络消耗变成下图: . . 参考Tensorboard, 可以看到gather操作确实被放到了PS上进行运算: . . 模型上线 . 我们目前在线上prediction时没有使用tf-serving, 而是将变量导出用Java服务进行prediction. 主要原因还是TF的PS不支持sparse存储，模型在单台机器存不下，没法使用serving. 在自己实现LR的prediction时，要注意验证线上线下的一致性，有一些常见的坑，比如没加bias, default_value没处理，等等。 . 使用TensorBoard来理解模型 . 在训练时用TensorBoard来打出Graph，对理解训练的细节很有帮助。例如，我们试图想象这么一个问题：在分布式训练中，参数的更新是如何进行的？以下有两种方案: . worker计算gradients, ps合并gradients并进行更新 | worker计算gradients, 计算应该update成什么值，然后发给ps进行更新 | 到底哪个是正确的？我们看一下TensorBoard就知道了: 可以看到，FTRL的Operation是在PS上运行的。所以是第一种。 . Metrics . 在TensorFlow中有一系列metrics, 例如accuracy, AUC等等，用于评估训练的指标。需要注意的是TensorFlow中的这些metrics都是在求历史平均值，而不是当前batch。所以如果训练一开始就进行评估，使用TensorFlow的AUC和Sklearn的AUC，会发现TensorFlow的AUC会低一些，因为TensorFlow的AUC其实是考虑了历史的所有样本。正因如此Estimator的evaluate才会有一个step的参数，例如运行100个step，就会得到这100个batch上的metric. . 性能相关 . profiling . 要进行性能优化，首先必须先做profiling。可以使用tf.train.ProfilerHook来进行. . hooks = [tf.train.ProfilerHook(save_secs=60, output_dir=profile_dir, show_memory=True, show_dataflow=True)] . 之后会写出timeline文件，可以使用chrome输入chrome://tracing来打开。 . 我们来看一个例子: 上图中IteratorGetNext占用了非常长的时间。这其实是在从HDFS读取数据。我们加入prefetch后，profile变成下面这个样子: 可以看到，加入prefetch以后HDFS读取延时和训练时间在相当的数量级，延时有所好转。 . 另一个例子: 可以看到, RecvTensor占用了大部分时间，说明瓶颈在PS上。经调查发现是partition太多(64)个而PS只有8个，造成传输数据效率下降。调整partition数量后变成如下: . 使用Grafana进行性能监控 . 可以使用Grafana和heapster来进行集群节点以及Pod的监控，查看CPU,内存,网络等性能指标。 . apiVersion: extensions/v1beta1 kind: Deployment metadata: name: monitoring-grafana namespace: kube-system spec: replicas: 1 template: metadata: labels: task: monitoring k8s-app: grafana spec: containers: - name: grafana image: k8s.gcr.io/heapster-grafana-amd64:v4.4.3 ports: - containerPort: 3000 protocol: TCP volumeMounts: - mountPath: /etc/ssl/certs name: ca-certificates readOnly: true - mountPath: /var name: grafana-storage env: - name: INFLUXDB_HOST value: monitoring-influxdb - name: GF_SERVER_HTTP_PORT value: &quot;3000&quot; # The following env variables are required to make Grafana accessible via # the kubernetes api-server proxy. On production clusters, we recommend # removing these env variables, setup auth for grafana, and expose the grafana # service using a LoadBalancer or a public IP. - name: GF_AUTH_BASIC_ENABLED value: &quot;false&quot; - name: GF_AUTH_ANONYMOUS_ENABLED value: &quot;true&quot; - name: GF_AUTH_ANONYMOUS_ORG_ROLE value: Admin - name: GF_SERVER_ROOT_URL # If you&#39;re only using the API Server proxy, set this value instead: # value: /api/v1/namespaces/kube-system/services/monitoring-grafana/proxy value: / volumes: - name: ca-certificates hostPath: path: /etc/ssl/certs - name: grafana-storage emptyDir: {} apiVersion: v1 kind: Service metadata: labels: # For use as a Cluster add-on (https://github.com/kubernetes/kubernetes/tree/master/cluster/addons) # If you are NOT using this as an addon, you should comment out this line. kubernetes.io/cluster-service: &#39;true&#39; kubernetes.io/name: monitoring-grafana name: monitoring-grafana namespace: kube-system spec: # In a production setup, we recommend accessing Grafana through an external Loadbalancer # or through a public IP. # type: LoadBalancer # You could also use NodePort to expose the service at a randomly-generated port # type: NodePort ports: - port: 80 targetPort: 3000 selector: k8s-app: grafana . 部署到k8s集群之后，就可以分节点/Pod查看性能指标了。 . ps/worker数量的设置 . PS和worker的数量需要如何设置？这个需要大量的实验，没有绝对的规律可循。一般来说sparse模型由于每条样本的特征少，所以PS不容易成为瓶颈，worker:PS可以设置的大一些。在我们的应用中，设置到1:8会达到比较好的性能。当PS太多时variable的分片也多，网络额外开销会大。当PS太少时worker拉参数会发生瓶颈。具体的实验可以参见下节。 . 加速比实验 . 实验方法：2.5亿样本，70亿特征(使用hash bucket后)，提交任务稳定后(20min)记录数据。 . worker PS CPU/worker (cores) CPU/PS (cores) total CPU usage (cores) memory/worker memory/PS total memory k example/s Network IO . 8 | 8 | 1.5-2 | 0.7-0.8 | 19-23 | 2-2.5G | 15G | 150G | 52 | 0.36 GB/s | . 16 | 8 | 1.3-2.6 | 1.0-1.4 | 40 | 2-2.5G | 15G | 170G | 72 | 0.64 GB/s | . 32 | 8 | 1.8-2.3 | 1.5-2.0 | 70 | 2.2-3.0G | 15G | 216G | 130 | 1.2 GB/s | . 64 | 8 | 1.5-2 | 4.0-6.4 | 150 | 2.2-3G | 15G | 288G | 250 | 2.3 GB /s | . 64 | 16 | 1.7 | 2.9 | 169 | 2.2G | 7.5G | 300G | 260 | 2.5 GB/s | . 128 | 16 | 1.5-2 | 3.9-5 | 278 | 2.2G | 7.5G | 469G | 440 | 4.2 GB/s | . 128 | 32 | 1.5 | 2.5-3 | 342 | 2.2G | 4-4.5G | 489G | 420 | 4.1 GB/s | . 160 | 20 | 1.2-1.5 | 3.0-4.0 | 360 | 2-2.5G | 6-7G | 572G | 500 | 4.9 GB/s | . 160 | 32 | 1.2-1.5 | 1.0-1.4 | 388 | 2-2.5G | 4-4.5G | 598G | 490 | 4.8 GB/s | . 结论: . worker消耗的内存基本是固定的；PS消耗的内存只跟特征规模有关; | worker消耗的CPU跟训练速度和数据读取速度有关; PS消耗的CPU和worker:PS的比例有关; | TensorFlow的线性scalability是不错的, 加速比大约可以到0.7-0.8; | 最后加到160worker时性能提升达到瓶颈，这里的瓶颈在于HDFS的带宽(已经达到4.9GB/s) | 在sparse模型中PS比较难以成为瓶颈，因为每条样本的特征不多，PS的带宽相对于HDFS的带宽可忽略不计 | Future Work . 文件分配优化 . 如果worker平均需要1个小时的时间，那么整个训练需要多少时间？如果把训练中的速度画一个图出来，会是类似于下面这样: . . 在训练过程中我发现，快worker的训练速度大概是慢worker的1.2-1.5倍。而整个训练的时间取决于最慢的那个worker。所以这里可以有一个优化，就是动态分配文件，让每个worker尽可能同时结束。借助之前提到的zookeeper, 做到这一点不会很难。 . online learning . 在online learning时，其实要解决的问题也是任务分配的问题。仍由master借助zookeeper进行任务分配，让每个worker去读指定的消息队列分区。 . PS优化 . 可以考虑给TensorFlow增加Sparse Variable的功能，让PS上的参数能够动态增长，这样就能解决checkpoint, 模型导出慢等问题。当然，这样肯定也会减慢训练速度，因为变量的保存不再是数组，而会是hashmap. . 样本压缩 . 从前面的实验看到，后面训练速度上不去主要是因为网络带宽瓶颈了，读样本没有这么快。我们可以看看TFRecord的实现： . message Example { Features features = 1; }; message Feature { // Each feature can be exactly one kind. oneof kind { BytesList bytes_list = 1; FloatList float_list = 2; Int64List int64_list = 3; } }; message Features { // Map from feature name to feature. map&lt;string, Feature&gt; feature = 1; }; . 可以看到，TFRecord里面其实是保存了key和value的，在sparse模型里，往往value较小，而key很大，这样就造成很大的浪费。可以考虑将key的长度进行压缩，减少单条样本的大小，提高样本读取速度从而加快训练速度。 .",
            "url": "https://blog.codescv.com/dist-tf-for-sparse-models.html",
            "relUrl": "/dist-tf-for-sparse-models.html",
            "date": " • May 19, 2018"
        }
        
    
  
    
        ,"post11": {
            "title": "训练AI玩游戏(2)",
            "content": "上次讲完了Deep Q Network的基本理论, 这次我们以经典的Atari游戏为例, 结合代码实现来看看. 完整代码地址: https://github.com/codescv/nesgym/blob/master/src/run-atari.py . 工程细节 . 虽然Deep Q Network的基本原理非常简单, 但也有很多的工程细节值得讨论. . Double Q Network . Deep Q Network的一个重要加强叫做Double Q Network. 它是这样工作的: 我们使用两个结构完全相同的Network q_base 和 q_target. 它们的input都是当前state, 而output是所有action对应的value(一个vector). q_base用于选择当前的最佳action, q_target用于近似代替真实的q. 每次训练时, 根据reward和loss function更新q_base的参数, 而每隔一段时间将q_base的参数完全赋值给q_target. 这样做的好处是让参数更新更加稳定. 有兴趣深入了解的同学可以去看这篇论文, 里面有更详细的解释. . Exploitation vs Exploration . 在训练刚开始时, 我们的q network完全是随机的; 一段时间学习之后, q network的预测会越来越准. 如果我们只用q network来决定当前的行动, 我们很有可能很快就掉进local minima. 是使用当前的最优方案, 还是尝试新的策略? 是每天都吃最喜欢吃的小龙虾, 还是隔几天就去找找新的好吃的? 这是个很常见的问题, 术语中叫Exploitation vs Exploration. . 解决这个问题的方法有多种: 我们使用的是最简单的一种, 称为ϵ epsilonϵ-greedy. 通俗的讲, 就是先扔个色子, 如果等于1我就去找新的地方, 否则我就还是吃我最喜欢的(在这个例子里ϵ=1/6 epsilon=1/6ϵ=1/6). 当然, ϵ epsilonϵ不是固定值; 一开始可以设的大一些, 等到后面见的多了, 身经百战了, 就可以设小一点. . 当然, 还有其他方法, 比如, 我能不能根据当前q的输出按概率去选择action呢? 答案是可以的, 有些研究表明这种方法有时会得到更好的效果. . Experience Replay . 我们回顾一下q network的更新规则: . q(s,a)=r+γ⋅max⁡a′q(s′,a′)q(s, a) = r + gamma cdot max_{a&amp;#x27;}{q(s&amp;#x27;, a&amp;#x27;)}q(s,a)=r+γ⋅a′max​q(s′,a′) . 所以在Double DQN里, 我们的loss可以写成: . loss=(r+γ⋅max⁡a′qtarget(s′,a′)−qbase(s,a))2loss = (r + gamma cdot max_{a&amp;#x27;}{qtarget(s&amp;#x27;, a&amp;#x27;) - qbase(s, a)) ^ 2}loss=(r+γ⋅a′max​qtarget(s′,a′)−qbase(s,a))2 . 因此在更新时, 我们需要一个序列(s, a, r, s’)来计算loss(注意: 我们更新的是q_base的参数, 前面两项是常数). 在实际的工程中, 我们使用一个replay buffer, 记录很多个历史序列, 然后在学习的时候, 从replay buffer中随机sample出一组(s, a, r, s’)的序列来进行训练. 这样能解除样本之间的相关性, 达到比较好的训练效果. . OpenAI Gym Environment . 我们使用OpenAI Gym作为训练环境. 之所以用它是因为它很好的封装了atari游戏的模拟器环境, 让我们可以专注在实现算法的代码上. 一个OpenAI的环境env, 它主要有几个方法: . reset 将重置这个环境, 进入一个新的episode. 返回observation(说明: 在本blog里observation和state是一回事). | step(action) 将在这个环境中进行action, 返回值为一个tuple: (obs, reward, done, info). 其中obs是新的observation, done表示该episode是否结束(比如游戏赢了或者输了), info里是一些跟环境相关的debug信息. | render() 用于渲染当前环境. 然后我们结合代码来看: | . def main(): last_obs = env.reset() for step in range(task.max_timesteps): env.render() action = dqn.choose_action(step, last_obs) obs, reward, done, info = env.step(action) dqn.learn(step, action, reward, done, info) if done: last_obs = env.reset() else: last_obs = obs . 这是我们的主循环, 应该非常好理解, 就不过多解释了. . Deep Learning Framework . Deepmind当年使用的framework是torch, 它是基于lua的. 原始的实现在这里: . https://sites.google.com/a/deepmind.com/dqn/ . 不过如果你去看它的代码, 有可能会一头雾水, 因为当年的deep learning framework不像现在这么傻瓜化, 很多东西需要手动实现, 看起来会非常复杂. . 这篇blog里的代码是基于Keras写的. 主要的原因是我觉得用Keras代码最简单干净, 容易看懂. 之前我还写过一个基于TensorFlow的版本在这里, 如果有兴趣也可以看看: . https://github.com/codescv/dqn-test/blob/master/hw3/run_dqn_atari.py . (这是Berkeley CS 294: Deep Reinforcement Learning的课程作业, 也可以去源地址看看, 自己试试从头写, 收获可能会更大.) . 对Deep Learning Framework我了解的不多, 只用过Keras和TensorFlow, 所以也没法给出更多建议. 之前看cs231n被安利了一把PyTorch, 像是Torch的升级版, 看起来也挺有意思的. 最大的区别是TensorFlow用的是static graph, 而PyTorch是dynamic graph, 有点类似于static language vs dynamic language, 也有点类似于eager evaluation vs lazy evaluation. . Q Network Architecture . 下面我们来看 q network model的代码: . def q_function(input_shape, num_actions): image_input = Input(shape=input_shape) out = Conv2D(filters=32, kernel_size=8, strides=(4, 4), padding=&#39;valid&#39;, activation=&#39;relu&#39;)(image_input) out = Conv2D(filters=64, kernel_size=4, strides=(2, 2), padding=&#39;valid&#39;, activation=&#39;relu&#39;)(out) out = Conv2D(filters=64, kernel_size=3, strides=(1, 1), padding=&#39;valid&#39;, activation=&#39;relu&#39;)(out) out = Flatten()(out) out = Dense(512, activation=&#39;relu&#39;)(out) q_value = Dense(num_actions)(out) return image_input, q_value def q_model(input_shape, num_actions): inputs, outputs = q_function(input_shape, num_actions) return Model(inputs, outputs) . 可以看到, 用Keras构建Neural Network非常的人性化, 你不需要声明一堆变量, 也不用小心翼翼的算每个矩阵是几乘几, 实在是数死早们的福音. (做工程就应该这么做, 不能搞的跟R语言那样.) 当然新版的TensorFlow也加如了Keras的各种API, 实在是很棒棒, 但是它同时还提供了3套大同小异的Layer API(未来或许更多). 也许Google的基因里就写着”分裂”这两个字吧. . 我们来简单分析一下这个Q Network的结构: 输入是当前的observation(numpy array), 然后经过若干convolutional layer, 然后是一个fully connected layer, 最后输出一个大小为num_actions的数组, 每一项对应action的q value. . Double DQN class . 最后, 我们有一个类DoubleDQN类来实现我们的Agent所需要的方法. 从之前main中我们已经看到了两个我们需要实现的方法, 一个是choose_action, 用于选择当前的action, 一个是learn, 用于驱动学习过程. 我们依次来看: . def __init__(self, ...): ... # use multiple frames as input to q network input_shape = image_shape[:-1] + (image_shape[-1] * frame_history_len,) # used to choose action self.base_model = q_model(input_shape, num_actions) self.base_model.compile(optimizer=optimizers.adam(clipnorm=10, lr=1e-4, decay=1e-6, epsilon=1e-4), loss=&#39;mse&#39;) # used to estimate q values self.target_model = q_model(input_shape, num_actions) self.replay_buffer = ReplayBuffer(size=replay_buffer_size, frame_history_len=frame_history_len) ... . 我们截取了__init__里比较重要的部分来看: 首先我们是把最近frame_history_len=4个frame的图像作为q network的输入的. 因为游戏是动态的, 仅靠当前frame并不一定能确定状态(比如速度等). 然后我们使用q_model创建了前面所说的两个q network. 在Optimizer里面有几个参数, 其中这个clipnorm是用来做gradient的截断, 防止巨大跳变. 最后我们创建一个ReplayBuffer来存储Agent运行的历史. . def choose_action(self, step, obs): self.replay_buffer_idx = self.replay_buffer.store_frame(obs) if step &lt; self.training_starts or np.random.rand() &lt; self.exploration.value(step): # take random action action = np.random.randint(self.num_actions) else: # take action that results in maximum q value recent_obs = self.replay_buffer.encode_recent_observation() q_vals = self.base_model.predict_on_batch(np.array([recent_obs])).flatten() action = np.argmax(q_vals) return action . 在choose_action方法中, 我们首先将当前frame的图像存到ReplayBuffer中, 然后我们使用前面介绍的ϵ epsilonϵ-greedy方法选择action. . def learn(self, step, action, reward, done, info=None): self.replay_buffer.store_effect(self.replay_buffer_idx, action, reward, done) if step &gt; self.training_starts and step % self.training_freq == 0: self._train() if step &gt; self.training_starts and step % self.target_update_freq == 0: self._update_target() def _train(self): obs_t, action, reward, obs_t1, done_mask = self.replay_buffer.sample(self.training_batch_size) q = self.base_model.predict(obs_t) q_t1 = self.target_model.predict(obs_t1) q_t1_max = np.max(q_t1, axis=1) q[range(len(action)), action] = reward + q_t1_max * self.reward_decay * (1-done_mask) loss = self.base_model.train_on_batch(obs_t, q) self.latest_losses.append(loss) def _update_target(self): weights = self.base_model.get_weights() self.target_model.set_weights(weights) . 可以看到, 我们是每隔一定的step才进行训练(training_freq=4), 然后每隔很长的step才更新target network的参数(target_update_freq &gt; 1000). 这是为了参数更新更加稳定. . 在_train方法里, 我们使用了前面讲到的loss function来更新参数. . 效果演示 . 训练前 . . 训练后 . . 可以看到，一开始Agent大部分是随机运动，效果很差；经过大约100w个step的training以后，就玩的非常好了。 . Deep Q Network的工程细节还是很多的, 这篇文章也是挑出比较重要的讲一讲, 有兴趣的同学可以去看完整的代码, 以及前面讲的paper. . 写这段代码最大的感受就是神经网络的代码好难debug，常常有bug也不报错，只是结果坏掉，不知道大家有什么这方面的经验吗？例如我在写的时候，一开始犯了个错误，忘记了更新last_obs, 导致agent的输入永远是第1帧。更神奇的是训练了一晚上以后，reward确实有所提高… .",
            "url": "https://blog.codescv.com/training-ai-to-play-games-2.html",
            "relUrl": "/training-ai-to-play-games-2.html",
            "date": " • Aug 16, 2017"
        }
        
    
  
    
        ,"post12": {
            "title": "训练AI玩游戏(1)",
            "content": "2013年，那是一个春天，有一个名不见经传的小公司发表了一篇论文Playing Atari with Deep Reinforcement Learning. 这篇文章主要讲述了他们只用屏幕图像作为输入, 用完全同样的算法训练AI玩了7中不同的Atari游戏的故事. 结果相当不错: AI在其中3种游戏中的表现都超过了人类的水平. 不久之后, 这家公司就被Google收购, 它就是现在大名鼎鼎的Deepmind, 后来做出了AlphaGo的公司. . 这篇blog的主题是训练AI玩游戏, 主要是讲电脑游戏而不是围棋; 不过既然AlphaGo比较有名，就插几句题外话。AlphaGo也用了Reinforcement Learning的方法，但是它的方法 并不适用于多数电脑游戏。可能有些人知道Deepmind, Facebook等一些公司目前在研究星际争霸这个游戏，那么你觉得围棋和星际争霸对于AI来说，哪个更难呢？ . 如果你觉得围棋更难，请你再想想。 . 答案是星际争霸更难，而且是难太多了。至于原因，其实有很多点，我稍微列举一下： . 围棋有理论最优解。理论上一出生，胜负就决定了。可是星际，根本没有最优解。你的打法是否奏效，取决于别人怎么打。 | 搜索空间的差距。围棋虽然搜索空间很大，不可能穷举。但是围棋的搜索空间跟星际比起来。。。简直就是苍茫宇宙里的一颗原子。围棋只需要300多步就可以下完一盘，所以能用monte carlo tree search + self play 这样的方法。而星际就很难了，因为星际理论上可以无限play下去都不结束，而且真的有很多比赛能打成平局，最要命的是你还很难判断什么是平局。也许你能强行截断1个小时的比赛，但是1个小时能玩多少个step的游戏？职业星际 选手400的apm，一小时能操作24000次。如果用围棋的方法，一辈子也训练不出来。 | 围棋是全局视野，你永远知道对手在干嘛。星际是局部视野，你只能考侦察和经验判断。 | 星际有来自东方的神秘力量等不确定因素(好吧这条是我乱说的) | . 人类喜欢从自己的主观感受去评价一个东西的难易，其实这是很错误的。好多人觉得，连人类都玩不好的围棋都被AI打败了，人类的末日就要来了! 其实完全没有。围棋对于AI来说，比超级马里奥都要简单的多。所以， 围棋大师们，对不起了，你们的游戏实在是太简单了！(对AlphaGo有兴趣的同学，可以看看Deepmind的论文，其实不是很难，像我这样初中数学文化水平的人也能看个七七八八。) . 言归正传。既然讲到训练AI玩游戏，这个东西在专业上属于Reinforcement Learning的范畴。Reinforcement Learning是什么呢？ 看下面这张图可能会比较清楚(图片来自David Silver的slides): . . 简单的说，Reinforcement Learning研究的问题就是一个Agent, 在一个环境中，根据环境的状态(states/observations)做出action, 并获取reward的过程。 . 一般讲到Reinforcement Learning, 就要讲Markov Decision Process，讲到Bellman Equation, 然后一大堆公式，但是那样可能比较无趣，所以我决定跳过这个东西，直接讲算法。 . Reinforcement Learning的算法有很多，常见的有两类，一类是Policy Based, 一类是Value Based. 所谓Policy Based就是我建立一个模型，它输出一个概率P(s,a)P(s, a)P(s,a)，也就是在某个state下采取某个action的概率。 所谓Value Based就是我建立一个模型，它输出一个value V(s,a)V(s, a)V(s,a)， 也就是在给定state下采取某个action所能得到的总收益。这两种方法并不是互斥的，比如AlphaGo就同时使用了这两种算法。 . 现在我们来看Deepmind经典的Atari游戏，它用的是一种叫Q Learning的算法。什么是Q learning呢？ 其实很好理解，我们有一个Q函数:q(s,a)q(s, a)q(s,a)，它接受两个输入，sss是当前的state，而a是采取的action, q用来model在这个state下采取这个action后所得到的收益。关于收益，我们一般是这么定义的: . v=rt1+γ⋅rt2+γ2⋅rt3+...v = r_{t1} + gamma cdot r_{t2} + gamma ^ 2 cdot r_{t3} + ...v=rt1​+γ⋅rt2​+γ2⋅rt3​+... . 其中γ gammaγ我们叫做decaying factor, 也就是我们认为远期的reward不如近期reward而加上的一个小于1的参数，例如γ=0.99 gamma = 0.99γ=0.99。我们要设计一个q函数数去最大化我们的收益，所以我们有： . q(s,a)=r+γ⋅max⁡a′q(s′,a′)q(s, a) = r + gamma cdot max_{a&amp;#x27;}{q(s&amp;#x27;, a&amp;#x27;)}q(s,a)=r+γ⋅a′max​q(s′,a′) . 假如我们能够对q建立一张表，然后记录所有的state和action, 然后不停的按上面这个式子进行迭代，问题就转成一个动态规划解决了(base case: 当游戏结束的时候，q=r)。 . 但是事实没有这么简单。考虑这么一个问题： 我们的游戏里有多少种state? 每个state下有多少种action? . 对于atari游戏来说，state是屏幕上像素的所有可能，这是个天文数字。action还好，是手柄上按键的数目（假如不考虑好几个键一起按的情况）。对于星际争霸来说，action space也是个天文数字。所以，q这个函数是 不可能用表格来解决的。那么怎么办呢？这个时候我们就需要建立一个估计函数q′≈qq&amp;#x27; approx qq′≈q，然后用真实的reward和action作为输入来训练它。最常见的approximator就是神经网络了，把神经网络引进来估计q, 这个方法 就叫Deep Q Network。 . 当我们比较准确的估计了q以后，这个问题就很明白了：我只要对一个state输出所有action的q, 然后选取q最大的那个action，就可以达到最优了。 . 好了，虽然实际应用时有很多的细节，不过Deep Q Network基本的理论也就是这样了。具体的东西等下一篇讲到代码的部分再一起讲。有兴趣的同学也可以先看看代码: https://github.com/codescv/nesgym . 一些思考： . 可以看到q learning对游戏本身的model没有任何假设，这是它最强大的地方，AI不care它在玩的是什么，只要你有reward，它就能训练。 | q learning针对action, state和reward进行训练, 但是这些东西并不需要是agent自己经历的，别人的经验也可以拿来train. | reward function至关重要。有一个好的reward function就等于成功了一半。这也是为什么atari游戏比较容易训练，因为atari游戏，例如打砖块和打飞机，reward来的非常快。而超级马里奥 则非常难。因为一关很长，只有最后才有reward(当然还有一个问题是超级马里奥没有全局视野，这点我们以后再说)。 | 虽然q learning已经非常general了，但是它离智能其实还很远。原因还是在于reward. AI必须依赖人类给的reward信号才能训练，否则它并不知道赢是好的，输是坏的。而人类却能很自然的知道这一点。 越研究AI，越觉得人类实在是太了不起了。 | .",
            "url": "https://blog.codescv.com/trainingaitoplaygames1.html",
            "relUrl": "/trainingaitoplaygames1.html",
            "date": " • Aug 11, 2017"
        }
        
    
  
    
        ,"post13": {
            "title": "Linear Algebra Notes",
            "content": "column space and null space . AAA is a m×nm times nm×n matrix R=rref(A)R = rref(A)R=rref(A) with rank rrr . We have the following definitions: column space of AAA: C(A)={Ax⃗∣x⃗∈Rn}C(A) = { A vec{x} | vec{x} in R^n }C(A)={Ax∣x∈Rn} null space of AAA: N(A)={x⃗∣Ax⃗=0⃗}N(A) = { vec{x} | A vec{x} = vec{0} }N(A)={x∣Ax=0} . then: C(A)C(A)C(A) = span(all pivot columns in A) (pivot columns in A means the corresponding columns to those pivot columns in R) . Why the pivot columns in A are the basis of C(A) ? . first we need to prove that the pivot columns are linearly independent: https://www.khanacademy.org/math/linear-algebra/vectors-and-spaces/null-column-space/v/showing-relation-between-basis-cols-and-pivot-cols then we can show that the free columns are linear combinations of the pivot columns: https://www.khanacademy.org/math/linear-algebra/vectors-and-spaces/null-column-space/v/showing-that-the-candidate-basis-does-span-c-a . Thus we have: dim(C(A))dim(C(A))dim(C(A)) = # of pivot variables = rank(A)rank(A)rank(A) . N(A)=N(R)N(A) = N(R)N(A)=N(R) (because elimination does not change the solutions) . dim(N(A))dim(N(A))dim(N(A)) = # of free variables = n−n -n− # of pivot variables = n−rn - rn−r . Linear transformations . visualization and intuition: . https://www.khanacademy.org/math/linear-algebra/matrix-transformations/linear-transformations/a/visualizing-linear-transformations . linear transformation vs matrix-vector product . For any linear transformation TTT: T(x⃗)=T(Ix⃗)=T(x1e1⃗+x2e2⃗+...+xnen⃗)=x1T(e1⃗)+x2T(e2⃗)+...+xnT(en⃗)=[T(e1⃗)T(e2⃗)...T(en⃗)]xT( vec{x}) = T(I vec{x}) = T(x_1 vec{e_1} + x_2 vec{e_2} + ... + x_n vec{e_n}) = {x_1}T( vec{e_1}) + {x_2}T( vec{e_2}) + ... + {x_n}T( vec{e_n}) = [T( vec{e_1}) T( vec{e_2}) ... T( vec{e_n})]xT(x)=T(Ix)=T(x1​e1​​+x2​e2​​+...+xn​en​​)=x1​T(e1​​)+x2​T(e2​​)+...+xn​T(en​​)=[T(e1​​)T(e2​​)...T(en​​)]x we have T(x⃗)=Ax⃗T( vec{x}) = A vec{x}T(x)=Ax, and the columns of AAA are the results of applying the transformation on the standard basis (e1⃗,e2⃗...en⃗)( vec{e_1}, vec{e_2} ... vec{e_n})(e1​​,e2​​...en​​) Hence, linear transformations are always 1 - 1 associated with matrix-vector product. . definition: kernel of a linear transformation . Suppose we have a L.T T(x⃗)=Ax⃗T( vec{x}) = A vec{x}T(x)=Ax Then we define kernel(T)=x⃗∣T(x⃗)=0kernel(T) = { vec{x} | T( vec{x}) = 0}kernel(T)=x∣T(x)=0, which is just null space of A . composition of linear transformations . We have transformations: S(x)=Ax⃗,T(x⃗)=BxS(x) = A vec{x}, T( vec{x}) = BxS(x)=Ax,T(x)=Bx define composition of SSS and TTT: (S∘T)(x⃗)=S(T(x⃗))(S circ T) ( vec{x}) = S(T( vec{x}))(S∘T)(x)=S(T(x)) We can prove that (S∘T)(S circ T)(S∘T) is a linear transformation. So we have a matrix CCC, where (S∘T)(x⃗)=Cx⃗(S circ T) ( vec{x}) = C vec{x}(S∘T)(x)=Cx To find CCC, we apply (S∘T)(S circ T)(S∘T) to III : C=S(T(I))=A(B(I))=[A(Be1⃗)A(Be2⃗)A(Be3⃗)...A(Ben⃗)]=[Ab1⃗Ab2⃗Ab3⃗...Abn⃗]C = S(T(I)) = A(B(I)) = begin{bmatrix}A(B vec{e_1}) &amp; A(B vec{e_2}) &amp; A(B vec{e_3}) &amp; ... &amp; A(B vec{e_n}) end{bmatrix} = begin{bmatrix} A vec{b_1} &amp; A vec{b_2} &amp; A vec{b_3} &amp; ... &amp; A vec{b_n} end{bmatrix}C=S(T(I))=A(B(I))=[A(Be1​​)​A(Be2​​)​A(Be3​​)​...​A(Ben​​)​]=[Ab1​​​Ab2​​​Ab3​​​...​Abn​​​] This is the definition of C=A∗B=[Ab1⃗Ab2⃗Ab3⃗...Abn⃗]C = A * B = begin{bmatrix} A vec{b_1} &amp; A vec{b_2} &amp; A vec{b_3} &amp; ... &amp; A vec{b_n} end{bmatrix}C=A∗B=[Ab1​​​Ab2​​​Ab3​​​...​Abn​​​] . proof of matrix multiplication associativity . Suppose H,G,FH, G, FH,G,F are L.T.s, and A,B,CA, B, CA,B,C are their corresponding matrices. H(x)=Ax,G(x)=Bx,F(x)=CxH(x) = Ax, G(x) = Bx, F(x) = CxH(x)=Ax,G(x)=Bx,F(x)=Cx Then: ((AB)C)x=((H∘G)∘F)(x)=((H∘G)(F(x))=H(G(F(x)))=H((G∘F)(x))=(H∘(G∘F))(x)=(A(BC))x((AB)C) x = ((H circ G) circ F)(x) = ((H circ G)(F(x)) = H(G(F(x))) = H((G circ F)(x)) = (H circ (G circ F))(x) = (A(BC))x((AB)C)x=((H∘G)∘F)(x)=((H∘G)(F(x))=H(G(F(x)))=H((G∘F)(x))=(H∘(G∘F))(x)=(A(BC))x thus we have ((AB)C)=(A(BC))((AB)C) = (A(BC))((AB)C)=(A(BC)) In short, matrix multiplications are associative because L.T.s(which are functions!) are associative. . If A is invertible, then ATAA^T AATA is invertible. . proof: ATAv⃗=0⇒v⃗TATAv⃗=0⇒∣∣Av∣∣2=0⇒Av⃗=0A^T A vec{v} = 0 Rightarrow vec{v}^TA^T A vec{v} = 0 Rightarrow ||Av||^2 = 0 Rightarrow A vec{v} = 0ATAv=0⇒vTATAv=0⇒∣∣Av∣∣2=0⇒Av=0 because AAA is invertible, v⃗ vec{v}v must be 0. This means ATAA^TAATA only has zero vector in the null space, and is thus invertible. . Projections . projection onto a subspace . Suppose we have a matrix AAA and a vector x⃗ vec{x}x, Ay⃗A vec{y}Ay​ is the projection of x⃗ vec{x}x onto the column space of A. By definition of projections, we have: AT(x⃗−Ay⃗)=0A^T( vec{x} - A vec{y}) = 0AT(x−Ay​)=0 So we can solve for y: y⃗=(ATA)−1ATx⃗ vec{y} = (A^TA)^{-1}A^T vec{x}y​=(ATA)−1ATx And the projection will be: xp⃗=A(ATA)−1ATx⃗ vec{x_p} = A(A^TA)^{-1}A^T vec{x}xp​​=A(ATA)−1ATx This shows that projections are linear transformations. . orthogonal complement . VVV is a subspace of SSS, then any vector v⃗∈S vec{v} in Sv∈S can be written as v⃗=p⃗+n⃗ vec{v} = vec{p} + vec{n}v=p​+n where p⃗∈V vec{p} in Vp​∈V and n⃗∈V⊥ vec{n} in V^ botn∈V⊥, V⊥V^ botV⊥ being the orthogonal complement of VVV in SSS: V⊥={x⃗∣x⃗Tv⃗=0∣v⃗∈V}V^ bot = { vec{x} | vec{x}^T vec{v} = 0 | vec{v} in V }V⊥={x∣xTv=0∣v∈V} Here, p⃗ vec{p}p​ is the projection of v⃗ vec{v}v onto VVV, and n⃗ vec{n}n is the projection of v⃗ vec{v}v onto V⊥V^ botV⊥. Suppose n⃗=Bv⃗ vec{n} = B vec{v}n=Bv, then v⃗=A(ATA)−1ATv⃗+Bv⃗ vec{v} = A(A^TA)^{-1}A^T vec{v} + B vec{v}v=A(ATA)−1ATv+Bv This way we can get the projection transformation matrix onto V⊥V^ botV⊥: B=I−A(ATA)−1ATB = I - A(A^TA)^{-1}A^TB=I−A(ATA)−1AT . change of basis . suppose we have a basis B={v1⃗,v2⃗,...vn⃗}B = { vec{v_1}, vec{v_2}, ... vec{v_n} }B={v1​​,v2​​,...vn​​} for RnR^nRn, then any vector v⃗ vec{v}v in RnR^nRn can be represented by BBB as v⃗=c1v1⃗+c2v2⃗+...+cnvn⃗ vec{v} = c_1 vec{v_1} + c_2 vec{v_2} + ... + c_n vec{v_n}v=c1​v1​​+c2​v2​​+...+cn​vn​​ Put in another way, v⃗ vec{v}v is [c1c2...cn] begin{bmatrix} c_1 c_2 ... c_n end{bmatrix}⎣⎢⎢⎢⎡​c1​c2​...cn​​⎦⎥⎥⎥⎤​ with respect to the new coordinate system given by BBB. We give this new vector a notation as [v⃗]B{[ vec{v}]}_B[v]B​, then: . v⃗=C[v⃗]B,C=[v1⃗v2⃗...vn⃗] vec{v} = C{[ vec{v}]}_B, C = begin{bmatrix} vec{v_1} &amp; vec{v_2} &amp; ... &amp; vec{v_n} end{bmatrix}v=C[v]B​,C=[v1​​​v2​​​...​vn​​​] Where v1⃗,v2⃗...vn⃗ vec{v_1}, vec{v_2} ... vec{v_n}v1​​,v2​​...vn​​ are the basis of BBB under the standard coordinates. This is called the change of basis formula. . Now suppose we have a L.T. T(x)=AxT(x) = AxT(x)=Ax in RnR^nRn, and under the coordinate system BBB, the same transformation is [T(x)]B=[Ax]B{[T(x)]}_B = {[Ax]}_B[T(x)]B​=[Ax]B​. Suppose the new transformation matrix is DDD, then: [T(x)]B=D[x]B{[T(x)]}_B = D{[x]}_B[T(x)]B​=D[x]B​ so we have [Ax]B=D[x]B{[Ax]}_B = D{[x]}_B[Ax]B​=D[x]B​ Using the change of basis, we have C−1Ax=DC−1xC^{-1}Ax = DC^{-1}xC−1Ax=DC−1x so D=C−1ACD = C^{-1}ACD=C−1AC This is also the definition that matrices AAA and DDD are similar. . orthonormal basis . B={v1⃗,v2⃗,...vn⃗}B = { vec{v_1}, vec{v_2}, ... vec{v_n} }B={v1​​,v2​​,...vn​​} for subspace VVV of RnR^nRn is orthonormal iff: ∣∣vi⃗∣∣=1|| vec{v_i}|| = 1∣∣vi​​∣∣=1 vi⃗⋅vj⃗=0,i≠j vec{v_i} cdot vec{v_j} = 0, i neq jvi​​⋅vj​​=0,i​=j . now suppose we have x∈Vx in Vx∈V x=c1v1+c2v2+...+cnvnx = c_1v_1 + c_2v_2+ ... +c_nv_nx=c1​v1​+c2​v2​+...+cn​vn​ take dot product of both sides with viv_ivi​ we can get: ci=vi⋅xc_i = v_i cdot xci​=vi​⋅x So the i-th component for vector xxx with respect to BBB is the dot product of the i-th basis and x. . projection onto subspace with orthonormal basis . suppose x∈Rnx in R^nx∈Rn, B={v1⃗,v2⃗,...vn⃗}B = { vec{v_1}, vec{v_2}, ... vec{v_n} }B={v1​​,v2​​,...vn​​} is an orthonormal basis for subspace VVV of RnR^nRn, then xxx can be represented as x=c1v1+c2v2+...+cnvn+wx = c_1v_1 + c_2v_2+ ... +c_nv_n + wx=c1​v1​+c2​v2​+...+cn​vn​+w where w∈V⊥,w⋅vi=0w in V^ bot, w cdot v_i = 0w∈V⊥,w⋅vi​=0. We take dot product of both sides by viv_ivi​: ci=vi⋅xc_i = v_i cdot xci​=vi​⋅x . orthonormal matrices transformations preserves lengths and angles . suppose CCC is orthonormal, we can prove that ∣∣Cx∣∣=∣∣x∣∣||Cx|| = ||x||∣∣Cx∣∣=∣∣x∣∣ x⋅y∣∣x∣∣⋅∣∣y∣∣=Cx⋅Cy∣∣Cx∣∣⋅∣∣Cy∣∣ frac{x cdot y}{||x|| cdot ||y||} = frac{Cx cdot Cy}{||Cx|| cdot||Cy||}∣∣x∣∣⋅∣∣y∣∣x⋅y​=∣∣Cx∣∣⋅∣∣Cy∣∣Cx⋅Cy​ . Gram-Schmidt process . Converts any basis to orthonormal basis for the same subspace. Starting from normalizing the first vector, subtract every vector with the projection onto the span of previous vectors. Examples: https://www.khanacademy.org/math/linear-algebra/alternate-bases/orthonormal-basis/v/linear-algebra-gram-schmidt-process-example . Eigenvalues and Eigenvectors . Eigenvectors of a linear transformation are the non-zero vectors that stay in the same direction (or flip around) when applying the linear transformation. Av=λvAv = lambda{v}Av=λv or: (λI−A)v=0( lambda{I} - A)v = 0(λI−A)v=0 because vvv is not zero and vvv is in the null space of λI−A lambda{I} - AλI−A, λI−A lambda{I} - AλI−A must be singular. . When switching to the eigenvectors of transformation matrix AAA as basis, the new transformation matrix will become a diagonal matrix with values λ1,λ2,...λn lambda_1, lambda_2, ... lambda_nλ1​,λ2​,...λn​. .",
            "url": "https://blog.codescv.com/linear-algebra-notes.html",
            "relUrl": "/linear-algebra-notes.html",
            "date": " • May 26, 2017"
        }
        
    
  
    
        ,"post14": {
            "title": "scala中的for语句和map",
            "content": "Scala中的for语句是一种语法糖。 . case 1: for (x) y . for (a &lt;- x; b &lt;- y; c &lt;- z) expr . 会转成： . x.foreach(a =&gt; y.foreach(c =&gt; z.foreach(expr))) . case 2: for (x) yield y . for (a &lt;- x; b &lt;- y; c &lt;- z) yield expr . 会转成： . x.flatMap(a =&gt; y.flatMap(c =&gt; z.map(expr))) . for中的类型匹配 . for (x: String &lt;- Seq(&quot;a&quot;, &quot;b&quot;, null)) yield x . 会返回List(a, b)。也就是说会对后面的值进行匹配，而null不会匹配String, 因此yield不会出null. . 可以用scala -Xprint:parser 来查看编译器生成的代码，非常有用： . scala&gt; for (x: String &lt;- Seq(&quot;a&quot;, &quot;b&quot;, null)) yield x [[syntax trees at end of parser]] // &lt;console&gt; package $line3 { object $read extends scala.AnyRef { def &lt;init&gt;() = { super.&lt;init&gt;(); () }; object $iw extends scala.AnyRef { def &lt;init&gt;() = { super.&lt;init&gt;(); () }; object $iw extends scala.AnyRef { def &lt;init&gt;() = { super.&lt;init&gt;(); () }; val res0 = Seq(&quot;a&quot;, &quot;b&quot;, null).withFilter(((check$ifrefutable$1) =&gt; check$ifrefutable$1: @scala.unchecked match { case (x @ (_: String)) =&gt; true case _ =&gt; false })).map(((x: String) =&gt; x)) } } } } . 可以明显看到后面会有一个withFilter的操作。for (x: String &lt;- Seq(&quot;a&quot;, &quot;b&quot;, null)) yield x 和 for (x &lt;- Seq(&quot;a&quot;, &quot;b&quot;, null)) yield x 的结果是不同的，这一点要特别注意. . 参考问题：http://stackoverflow.com/questions/41499441/explanation-on-scala-for-comprehension-with-option .",
            "url": "https://blog.codescv.com/scala/fp/scala-for-vs-map.html",
            "relUrl": "/scala/fp/scala-for-vs-map.html",
            "date": " • Jan 7, 2017"
        }
        
    
  
    
        ,"post15": {
            "title": "面向对象与函数式的思考",
            "content": "语序的区别 . 在很长一段时间里，OOP在程序语言里占了统治地位。之所以它会如此流行，不是因为它有多优秀，而是因为它更容易被接受。在OOP里，对象是第一位的，动作是第二位的，这和主流的人类语言最为接近。例如，在面向对象的语言里，要描述”我看书”这个概念，一般是这样的： . i.read(book) (python, c++, java, ...) i read book (ruby, scala, ...) . 这和人类语言的思维非常相似。而在函数式语言里，要表达这个概念就变成了: . read(i, book) 或者 (read i book) . 在英语和中文里，语序都是”我 看 书”， 在日语和韩语里，语序是”我 书 看”, 无论如何，作为主语的”我”总是放在最前面的。而如果让你改口说”看 我 书”，无疑会非常别扭。 . 先有对象还是先有动作 . 在面向对象语言里是先有对象后有方法，方法隶属于对象。这和人类直觉的思维方式是一致的。在OOP中，万物皆对象，数字是一种对象，数组是一种对象，字符串是一种对象， lambda是一种带有apply方法的对象。这很容易接受，对不对？ . 然而到了函数式语言里，万物皆lambda。对象是一个lambda, 对象的的方法是你给这个lambda传递的参数(消息)。任何东西都是lambda实现的！数字是一个lambda，数组是一个lambda(参数是index, 返回值是元素), 甚至if/else都是lambda, 你还能接受吗？ . 不过，虽然这两者的理论都算比较完美，真正实现了面向对象或者函数式的语言并不多。除了面向对象里的Smalltalk, 函数式里的Haskell, 剩下的几乎都是些不能自圆其说的语言(例如C*/Java/Scala/Javascript/Python/Ruby/…以下省略一万字)。举个例子，Smalltalk里，if/else是对象的一个普通的方法，而在Haskell里，if/else是普通的函数。而在大部分声称自己是面向对象或者是函数式的语言里，if/else都是凭空存在的。 .",
            "url": "https://blog.codescv.com/programming/oop-vs-fp.html",
            "relUrl": "/programming/oop-vs-fp.html",
            "date": " • Dec 24, 2016"
        }
        
    
  
    
        ,"post16": {
            "title": "Java中的Variance",
            "content": "Java中的Variance . 默认情况下，Java中的Generics都是Invariant. 例如ArrayList不是ArrayList的子类。如下代码说明了原因： . ArrayList&lt;Number&gt; x = new ArrayList&lt;Integer&gt;(); // 假如能编译通过 x.add(1); x.add(3.5); // 错误！ int y = x.get(0); // 可以 . 这是因为，我们对一个Generic Collection的操作，往里面放东西和从里面往外拿东西所需要的类型是不一样的。假如Array&lt;P&gt;是Array的父类，根据里氏替换原则，Array&lt;P&gt;的对象一定可以被Array的对象替换。 . 假如我们需要往里放东西，我们已知 Array&lt;P&gt;::add(P p), 根据替换原则 Array::add应该能接受P, 而我们已知Array::add能接受C, 所以C &gt;= P. . 假如我们要往外取东西, P p = Array&lt;P&gt;::get, 根据替换原则 Array.get()应该能被P接受, 所以C能被P接受， 则 P &gt;= C. . 综合上面两条，对于一个Generic来说，假如Array&lt;P&gt;是Array的父类，则P = C. 这就是Invariant. . 同时我们可以知道两点结论： . 如果P是C的父类，假如这个Generic，我们只需要往外取东西(函数的返回值)，那么Class&lt;P&gt; 就是 Class 的父类，这就是covariant. 典型的例子: `Iterator&lt;? extends T&gt;`. . 如果P是C的父类，假如这个Generic，我们只需要往里放东西(函数的参数)，那么Class 就是 Class&lt;P&gt; 的父类，这就是contravariant. 典型的例子: `Consumer&lt;? super T&gt;`. . 参考实例代码体会。在这里, List&lt;? extends Number&gt; 可以接受 List, 这就是covariant. Consumer&lt;? super Double&gt; 可以接受 Consumer, 这就是 contravariant. . public class Main { private static List&lt;Double&gt; fList = Arrays.asList(1.0,2.0,3.0); public static void main(String[] args) { System.out.println(intSum(fList)); forEach(new Consumer&lt;Number&gt;() { @Override public void accept(Number number) { System.out.println(&quot;number: &quot; + number); } }); } public static Number intSum(List&lt;? extends Number&gt; l) { int sum = 0; for (Number n : l) { sum += n.intValue(); } return sum; } public static void forEach(Consumer&lt;? super Double&gt; consumer) { for (double x : fList) consumer.accept(x); } } .",
            "url": "https://blog.codescv.com/java/java-variance.html",
            "relUrl": "/java/java-variance.html",
            "date": " • Dec 19, 2016"
        }
        
    
  
    
        ,"post17": {
            "title": "Redis Highlights (2) Redis中的事件处理",
            "content": "Redis中的事件处理 . 和MySQL不同，Redis是一个单线程数据库。所有的读写操作都在主线程里完成。(当然，会有一些操作，例如rdb文件的写操作可以发生在后台线程里) 主线程的循环可以在ae.c里看到大致的流程： . void aeMain(aeEventLoop *eventLoop) { eventLoop-&gt;stop = 0; while (!eventLoop-&gt;stop) { if (eventLoop-&gt;beforesleep != NULL) eventLoop-&gt;beforesleep(eventLoop); aeProcessEvents(eventLoop, AE_ALL_EVENTS); } } . 其中aeProcessEvents就是处理所有事件的函数。Redis中主要两类事件，一是File Events, 就是和客户端网络交互中发生的事件，一是Time Events，目前用来运行serverCron. . aeProcessEvents里关键代码： . shortest = aeSearchNearestTimer(eventLoop); tvp = ... ... numevents = aeApiPoll(eventLoop, tvp); // 处理file events ... ... // 处理time events processTimeEvents(eventLoop) . 所以消息循环的逻辑如下：首先用aeApiPoll获取当前pending的file events，然后处理file events. aeApiPoll的第二个参数表示block的时间，它是根据最近一个timer决定的。假如最近一个timer要在1秒后触发，那么poll的timeout就是1秒。这样做的目的是减少忙等待。 . file events处理完后就会处理time events. 这里只有一个timer就是serverCron，刷新rdb, 刷新各种计时器，以及一些housekeeping的任务都放在这里完成。 .",
            "url": "https://blog.codescv.com/redis/redis-event-loop.html",
            "relUrl": "/redis/redis-event-loop.html",
            "date": " • Dec 13, 2016"
        }
        
    
  
    
        ,"post18": {
            "title": "Redis Highlights (1) Redis中的类型和数据结构",
            "content": "Redis中的类型和数据结构 . redis对外有5中基本类型，分别是string t_string.c, list t_list.c, hash t_hash.c, set t_set.c 和 zset (ordered set) t_zset.c. 这5种类型是“接口”而不是“实现”，因此redis得以根据不同的情形自由选择不同数据结构的实现，这也是redis在设计上的高明之处。 . 5种基本类型对应了int object.c, embstr object.c, raw sds.c, linkedlist adlist.c, ziplist ziplist.c, skiplist t_zset.c, ht dict.c, intset intset.c 这8种数据结构的实现。 . 类型与数据结构实现的对应关系如图。 . . 实用type KEYNAME可以查看某个key对应的类型，而object encoding KEYNAME可以查看该key内部的实现。 . string . string 有三种实现方式，分别是int, embstr和raw. 长度比较短的整数会使用int实现。长度比较短的字符串会使用embstr, 更长的会使用raw。 embstr和raw的区别在于，embstr吧redisObject和sds放在一块连续空间里面，这样申请内存和释放内存都只需要一次调用。带来的坏处是，embstr是只读的，如果调用append等操作则自动升级为raw。 . 对于int实现的string，如果调用strlen和gettrange等会产生额外开销。如果需要强制使用raw来实现, 可以用setrange。 . zset . zset在元素较少时，使用intset实现。 在元素较多时，使用skiplist和dict一起实现。其中skiplist用于提供顺序相关的操作，而dict用于快速查询score. . 关于skiplist: 这是一种用多级链表加快查询速度的数据结构。Insert, delete, search的复杂度均为o(logn), 还可以进行高效的range query, 功能与性能与红黑树，B树相当，但实现更简单。CLRS里有详细的解释。 .",
            "url": "https://blog.codescv.com/redis/redis-types.html",
            "relUrl": "/redis/redis-types.html",
            "date": " • Dec 11, 2016"
        }
        
    
  
    
        ,"post19": {
            "title": "Build Redis with Xcode",
            "content": "Build and Debugging Redis with Xcode . I’ve set up an Xcode project building redis. Debugging, code completion and jumping are all working properly. . Repository: https://github.com/codescv/redis.git . Happy hacking! :-) .",
            "url": "https://blog.codescv.com/redis/xcode-redis.html",
            "relUrl": "/redis/xcode-redis.html",
            "date": " • Dec 9, 2016"
        }
        
    
  
    
        ,"post20": {
            "title": "Python MetaProgramming",
            "content": "class decorators . 像函数一样，class也可以有decorator. 例如: . @debugattr class Point: def __init__(self, x, y): self.x = x self.y = y def debugattr(cls): &#39;&#39;&#39; Decorator that adds logging to attribute access &#39;&#39;&#39; orig_getattribute = cls.__getattribute__ def __getattribute__(self, name): print(&#39;Get:&#39;, name) return orig_getattribute(self, name) cls.__getattribute__ = __getattribute__ return cls . class的decorator也是类似的语法糖，Point = debugattr(Point). 在这个例子里面，我们”打开”了Point这个类，并修改了它的__getattribute__方法的定义。 . metaclass . metaclass是一种class，它创建的instance是class. metaclass几乎总是会继承自type. . 一般来说，创建一个metaclass需要改写__new__方法，来实现自定义的功能: . class debugmeta(type): &#39;&#39;&#39; Metaclass that applies debugging to methods &#39;&#39;&#39; def __new__(cls, clsname, bases, clsdict): clsobj = super().__new__(cls, clsname, bases, clsdict) clsobj = debugmethods(clsobj) return clsobj def debugmethods(cls): &#39;&#39;&#39; Apply a decorator to all callable methods of a class &#39;&#39;&#39; for name, val in vars(cls).items(): if callable(val): setattr(cls, name, debug(val)) return cls . 在上面的例子里展示了如何将class decorator应用在metaclass里。多数情况下，能用metaclass实现的功能用decorator也能实现，但是metaclass的好处 在于会自动继承, 也就是说parent class的metaclass会自动成为subclass的metaclass. 例如： . class Base(metaclass=meta): pass class A(Base): # type(A) = meta &amp;&amp; A&#39;s metaclass = meta pass . decorator/class decorator/metaclass 之比较 . 总结一下，在python中，实现wrapping的方式有三种： . decorator: wrapping functions | class decorators: wrapping classes | metaclasses: wrapping class hierarchies | . 另一个区别：decorator作用于decorated object定义之后，而metaclass作用于之前。 . descriptor . descsriptors用来控制.运算符的语义。 . class Descriptor: def __init__(self, name=None): self.name = name def __set__(self, instance, value): instance.__dict__[self.name] = value def __delete__(self, instance): raise AttributeError(&quot;Can&#39;t delete&quot;) class Stock(Structure): shares = Descriptor(&quot;aa&quot;) . super . super() 会调用下一个mro里对象中的方法(而不是父类)。 . import hooks . sys.meta_path中包含了一个Finder list，可以用来影响module的加载. 例如，以下代码 定义了一个StructImporter，可以从xml中读取数据，并创建一个python module. . class StructImporter: def __init__(self, path): self._path = path def find_module(self, fullname, path=None): name = fullname.rpartition(&#39;.&#39;)[-1] if path is None: path = self._path for dn in path: filename = os.path.join(dn, name+&#39;.xml&#39;) if os.path.exists(filename): return StructXMLLoader(filename) return None import imp class StructXMLLoader: def __init__(self, filename): self._filename = filename def load_module(self, fullname): mod = sys.modules.setdefault(fullname, imp.new_module(fullname)) mod.__file__ = self._filename mod.__loader__ = self code = _xml_to_code(self._filename) exec(code, mod.__dict__, mod.__dict__) return mod import sys def install_importer(path=sys.path): sys.meta_path.append(StructImporter(path)) . 参考 . Python 3 Metaprogramming .",
            "url": "https://blog.codescv.com/python/python-meta.html",
            "relUrl": "/python/python-meta.html",
            "date": " • Nov 8, 2016"
        }
        
    
  
    
        ,"post21": {
            "title": "Haskell Notes (2)",
            "content": "Monads . A monad is an algebraic structure in category theory, and in Haskell it’s used to describe computations as a sequece of steps, and to handle side effects such as state and IO. It is used to allow actions(e.g. mutable variables) to be implemented safely. . hello :: String -&gt; IO String hello x = do putStrLn (&quot;Hello, &quot; ++ x) putStrLn &quot;What&#39;s your name?&quot; name &lt;- getLine return name . A monad in Haskell consists of a do keyword and a sequence of commands. To extract information from a monad, use &lt;-. Monad is the way to implement IO operations. Note the return type of the hello function is IO String, not String. . In fact a do block is just a syntactic sugar. Won’t go over too much, see more Here. . Lazy . Haskell is lazy, meaning that the evaluation of expressions is delayed until actually needed. . -- given definitions: f x y = y bot = bot -- eval: f bot 3 3 -- given definition: ones = 1 :: ones -- eval: ones [1,1,1,.....] head(ones) 1 . bot is the simplest infinite function defined. Because of the laziness, if we evaluate f bot 3 we get 3, but if we evaluate f 3 bot we get an infinite loop. . For another example, we can define the fibonacci sequence lazily: . fibs = 1:1:(zipWith (+) fibs (tail fibs)) . lambda calculus . Lambda calculus is the theory behind functional programming. We can view lambda calculus as a minimum language: . exp = const | var | exp exp | var -&gt; exp . variables . Variables are either bound or free. . In x -&gt; x + y, the variable x is bound and y is free. In | In a + ( a -&gt; 2^a) 3, the first a is free and the other a’s are bound. | . Alpha conversion . Alpha conversion means you can change the bound variables freely without affecting the value of the expression. e.g.: . ( x -&gt; x + 1) 3 -- rename x to y, the expression stays the same. ( y -&gt; y + 1) 3 . Beta conversion . Beta conversion is just applying the lambda expression. It is done by substuting value for the parameters, i.e. the expression ( x -&gt; exp1) exp2 is converted to exp1[exp2|x]. . Eta conversion . Eta conversion says that a function is equivalent to a lambda expression that takes an argument and applies the function to the argument, i.e.: ( x -&gt; f x) is equal to f. As another example, f x y = g y is equivalent to f x = g. . everything is a lambda . In fact, everything (variables, constants, lists, functions, etc.) is just syntactic sugar for a lambda expression. see Link for details. .",
            "url": "https://blog.codescv.com/haskell/haskell-2.html",
            "relUrl": "/haskell/haskell-2.html",
            "date": " • Oct 29, 2016"
        }
        
    
  
    
        ,"post22": {
            "title": "Haskell Notes",
            "content": "defining functions . Haskell is a functional programming language. This basically means functions are first-class citizens. Let’s look at how to define a function. . add x y = x + y add 1 2 3 . functions are partial . In haskell, functions can be partially applied (applied with less arguments than defined) . add1 = add 1 add1 2 3 . If we apply add with only one argument 1, then we get a new function add1 which takes one argument. . infix and prefix . In haskell, infix functions (i.e. operators in other languages) and prefix functions are treated alike. They can be transformed into each other using () and ` ` . -- infix -&gt; prefix 1 + 2 3 (+) 1 2 3 -- prefix -&gt; infix myadd x y = x + y 1 `add` 2 3 . let . The let statement provides syntactic sugar for &quot;defining variables&quot;. . let x = 1 y = 2 add x y = x + y in 1 + (add x y) 4 . I put quotes around “defining variables” because x and y are not variables in other languages’ sense. Instead, they are just bindings local to the statement, which is somewhat unique to haskell. The let statement can also define functions. . closures/anonymous functions/lambdas . -- the follwing are equivalent: add x y = x + y add = x y -&gt; x + y -- the follwing are equivalent: add 1 x add 1 x . lazy evaluation . -- the cycle function take 10 (cycle [1,2,3]) [1,2,3,1,2,3,1,2,3,1] -- one way to define the cycle function cycle x = x ++ cycle x . cycle [1,2,3] generates an infinite list 1,2,3,1,2,3,1,2,3,.... Because Haskell is lazy, applying take 10 to that list actually yields an finite list ``. . type, type class . a type class is like a Java interface a type, a type is like a concrete Java class implementing an interface, . We use data to define a type. . data Point = Point Int Int deriving Show Point 1 2 . The Show typeclass defines a show function which is used to print a data type. We can redefine this function as follows: . data Point = Point Int Int instance Show Point where show (Point x y) = (show x) ++ &quot;,&quot; ++ (show y) . type constructor . data Maybe a = Nothing | Just a . We call Maybe a type constructor. Depending on the type of a, this constructor can end up creating a Maybe Int, Maybe Car, or Nothing. . functors . The Maybe is an instance of Functor type class. A Functor is simply something that you can map on. . class Functor f where fmap :: (a -&gt; b) -&gt; f a -&gt; f b -- lists are instances of functors instance Functor [] where fmap = map -- maybe&#39;s are instances of functors instance Functor Maybe where fmap f (Just x) = Just (f x) fmap f Nothing = Nothing fmap inc (Just 1) . It’s easy (or not so easy) to see that Lists and Maybe&#39;s are instances of Functors. . pattern matching and erlang-like case statements . As seen above, pattern matching can be used in function arguments. It can also be used in case statements and let statements. . let (Point x y) = (Point 1 2) in x + y head&#39; :: [a] -&gt; a head&#39; xs = case xs of [] -&gt; error &quot;No head for empty lists!&quot; (x:_) -&gt; x . :kind :info :type . :kind shows type information on types and type classes | :info shows detailed information on types and type classes. | :type shows type for instances of types, and type classes for types. | .",
            "url": "https://blog.codescv.com/haskell/haskell.html",
            "relUrl": "/haskell/haskell.html",
            "date": " • Oct 21, 2016"
        }
        
    
  
    
        ,"post23": {
            "title": "Python3中排序的cmp函数的替代方法",
            "content": "在python3中的sort函数取消了对cmp函数的支持。在大多数场景下，使用key确实是更好的方法，但是如果我们有逻辑不方便用key表示呢？ 比如以下的例子: . def my_cmp(x, y): if x is None and y is not None: return -1 if x is not None and y is None: return 1 else: if x &gt; y: return 1 elif x &lt; y: return -1 else: return 0 # python2 sorted([1,None,5,2], cmp=my_cmp) . 像这样复杂的逻辑不太容易转成key函数，或者我们不想改变已有使用cmp的代码，该怎么办呢？我们可以自己写一个函数，把cmp函数转换成key函数。 . class KeyWrapper(object): def __init__(self, cmp, val): self.val = val self.cmp = cmp def __lt__(self, other): return self.cmp(self.val, other.val) == -1 def cmp_to_key(func): def key(x): return KeyWrapper(func, x) return key print(sorted([1,3,None,2], key=cmp_to_key(my_cmp))) . 我们在key函数中返回一个KeyWrapper对象，这个对象保存了cmp函数和实际要比较的值的引用，然后改写KeyWrapper的__lt__比较函数，使用cmp函数 进行大小比较即可。 . 事实上python3已经为我们想到了这一点，我们只需要用functools.cmp_to_key函数就可以了。 .",
            "url": "https://blog.codescv.com/python/python3-cmp.html",
            "relUrl": "/python/python3-cmp.html",
            "date": " • Feb 27, 2016"
        }
        
    
  
    
        ,"post24": {
            "title": "ReactiveCocoa 代码阅读笔记 (2) Signal的实现机制",
            "content": "上篇讲了RAC和RACObserve两个宏的实现机制，但都是把RACSignal当作一个黑盒来理解。 本篇详细讲解Signal的内部机制。 . 创建Signal . Signal有很多种创建方式，例如[UITextView rac_textSignal]是用signalForSelector:的方式。这里我们先看看 自定义创建的Signal: . // RACSignal.m + (RACSignal *)createSignal:(RACDisposable * (^)(id&lt;RACSubscriber&gt; subscriber))didSubscribe { return [RACDynamicSignal createSignal:didSubscribe]; } // RACDynamicSignal.m + (RACSignal *)createSignal:(RACDisposable * (^)(id&lt;RACSubscriber&gt; subscriber))didSubscribe { RACDynamicSignal *signal = [[self alloc] init]; signal-&gt;_didSubscribe = [didSubscribe copy]; return [signal setNameWithFormat:@&quot;+createSignal:&quot;]; } . createSignal方法接收一个block, 把这个block存在_didSubscribe属性中。顾名思义，这个block会在subscribe的时候被调用。 . subscribe一个signal . 下面看最常用的subscribeNext的定义 . // RACSignal.m - (RACDisposable *)subscribeNext:(void (^)(id x))nextBlock error:(void (^)(NSError *error))errorBlock completed:(void (^)(void))completedBlock { RACSubscriber *o = [RACSubscriber subscriberWithNext:nextBlock error:errorBlock completed:completedBlock]; return [self subscribe:o]; } . subscribeNext会创建一个RACSubscriber对象， 它保存了nextBlock, errorBlock和completedBlock，这些block将在sendNext, sendError和sendCompleted时调用: . // RACSubscriber.m - (void)sendNext:(id)value { @synchronized (self) { void (^nextBlock)(id) = [self.next copy]; if (nextBlock == nil) return; nextBlock(value); } } - (void)sendError:(NSError *)e { @synchronized (self) { void (^errorBlock)(NSError *) = [self.error copy]; [self.disposable dispose]; if (errorBlock == nil) return; errorBlock(e); } } - (void)sendCompleted { @synchronized (self) { void (^completedBlock)(void) = [self.completed copy]; [self.disposable dispose]; if (completedBlock == nil) return; completedBlock(); } } . 可以看到, sendNext是线程安全的，并且nextBlock是和sendNext在同一个线程上执行的。 . 我们回到RACSignal, subscribeNext会调用subscribe: . // RACDynamicSignal.m - (RACDisposable *)subscribe:(id&lt;RACSubscriber&gt;)subscriber { NSCParameterAssert(subscriber != nil); RACCompoundDisposable *disposable = [RACCompoundDisposable compoundDisposable]; subscriber = [[RACPassthroughSubscriber alloc] initWithSubscriber:subscriber signal:self disposable:disposable]; if (self.didSubscribe != NULL) { RACDisposable *schedulingDisposable = [RACScheduler.subscriptionScheduler schedule:^{ RACDisposable *innerDisposable = self.didSubscribe(subscriber); [disposable addDisposable:innerDisposable]; }]; [disposable addDisposable:schedulingDisposable]; } return disposable; } . RACPassthroughSubscriber可以看作和subscriber功能一样，这里不细讲，所以subscribe:就是调用了RACScheduler.subscriptionScheduler schedule: 并传入一个block, 在这个block中调用了didSubscribe，也就是createSignal中的block。所以我们可以得到一个结论：对于createSignal创建的信号，每subscribe一次，其block就被调用一次。 . RACScheduler.subscriptionScheduler返回一个单例的RACSubscriptionScheduler对象, 其schedule方法: . // RACSubscriptionScheduler.m - (RACDisposable *)schedule:(void (^)(void))block { NSCParameterAssert(block != NULL); if (RACScheduler.currentScheduler == nil) return [self.backgroundScheduler schedule:block]; block(); return nil; } . 这个方法会找RACScheduler.currentScheduler, 这个方法在主线程上或者在schedule内部(schedule中的schedule)会返回非nil, 此时直接调用block; 如果不在主线程上，最外层schedule会使用backgroundScheduler，也就是在一个background queue上执行block. . 因此, 如果createSignal在主线程上执行，(之后调用subscribe时)signal的didSubscribe(也就是createSignal传入的block)也会在主线程上执行。 . 一般自定义的signal会在didSubscribe这个block中(或者其后续)调用[subscriber sendNext]等方法，这样subscribeNext:中传入的block就会执行。应该注意到的是subscirbeNext:中传入的block会和sendNext在同一个线程上执行。 . rac_signalForSelector: . rac_signalForSelector: 这个方法很重要，它为各种UIView的delegate方法封装提供了基础。这个方法产生一个signal, 这个signal会在receiver执行某个selector时发送函数调用的参数。以UITextView为例: . // NSObject+RACSelectorSignal - (RACSignal *)rac_signalForSelector:(SEL)selector; // UITextView+RACSignalSupport.m - (RACSignal *)rac_textSignal { @weakify(self); RACSignal *signal = [[[[[RACSignal defer:^{ @strongify(self); return [RACSignal return:RACTuplePack(self)]; }] concat:[self.rac_delegateProxy signalForSelector:@selector(textViewDidChange:)]] reduceEach:^(UITextView *x) { return x.text; }] takeUntil:self.rac_willDeallocSignal] setNameWithFormat:@&quot;%@ -rac_textSignal&quot;, self.rac_description]; RACUseDelegateProxy(self); return signal; } . 这里会用一个self.rac_delegateProxy作为TextView的proxy，调用[self.rac_delegateProxy signalForSelector:@selector(textViewDidChange:)] 也就是产生了一个信号， 当[delegate textViewDidChange:]调用时触发。 . rac_signalForSelector中创建了一个subject(一个可以手动控制进行sendNext的信号), 做了很复杂的swizzling, 其中和发送信号相关的是RACSwizzleForwardInvocation, 这个方法改写了原本的forwardInvocation. 然后将自己的对应selector的方法替换成forwardInvocation，这个forwardInvocation里面会调用[subject sendNext:]发送信号给subscriber. . 一句话概括：rac_textSignal会”劫持” UITextView.delegate的textViewDidChange:方法，然后在这个方法调用时，把这个方法传入的参数发送给subscriber。这个劫持过程的实现是由[NSObject rac_signalForSelector:]方法完成的。 .",
            "url": "https://blog.codescv.com/ios/reactive-2.html",
            "relUrl": "/ios/reactive-2.html",
            "date": " • Feb 7, 2016"
        }
        
    
  
    
        ,"post25": {
            "title": "ReactiveCocoa 代码阅读笔记 (1) 双向绑定",
            "content": "我们首先从一个双向绑定的例子开始。假设我们的UIViewController里有一个UITextField和一个UILabel， 我们希望在UITextField输入时， UILabel里面同步显示一样的内容。我们通过把UITextField和UILabel绑定到同一个model，也就是UIViewController的name属性上 来实现这一点。 . RAC(self.nameTextfield, text) = RACObserve(self, name); RAC(self.nameLabel, text) = RACObserve(self, name); [self.nameTextfield.rac_textSignal subscribeNext:^(id x) { self.name = x; }]; // this has the same effect as the above // RAC(self, name) = self.nameTextfield.rac_textSignal; . 当model变化时，self.nameTextfield和self.nameLabel.text都会随之变化。而当UITextField中的输入发生变化时，model也会发生变化。 UITextField和model发生了双向绑定，而label和model是单向绑定。 . 我们把上面的代码用clang -E展开，看看到底发生了什么： . [[RACSubscriptingAssignmentTrampoline alloc] initWithTarget:(self.nameTextfield) nilValue:(((void *)0))][@(((void)(__objc_no &amp;&amp; ((void)self.nameTextfield.text, __objc_no)), &quot;text&quot;))] = ({ __attribute__((objc_ownership(weak))) id target_ = (self); [target_ rac_valuesForKeyPath:@(((void)(__objc_no &amp;&amp; ((void)self.name, __objc_no)), &quot;name&quot;)) observer:self]; }); . 简化一下: . [[RACSubscriptingAssignmentTrampoline alloc] initWithTarget:(self.nameTextfield) nilValue:0][@&quot;text&quot;] = ({ __weak id target_ = (self); [target_ rac_valuesForKeyPath:@(&quot;name&quot;) observer:self]; }); . 等等，@(((void)(__objc_no &amp;&amp; ((void)self.nameTextfield.text, __objc_no)), &quot;text&quot;)) 这个是什么东西？ 这个东西其实就等效于@&quot;text&quot;，但是为什么要前面这一长串呢？这是为了在编译期防止你引用了错误的属性，比如你写了@RAC(self.nameTextField, somethingNotexist)，编译器就会给出提示说这个属性不存在。clever. . RAC . 我们来看前半部分，也就是RAC的展开: . [[RACSubscriptingAssignmentTrampoline alloc] initWithTarget:(self.nameTextfield) nilValue:0][@&quot;text&quot;] = ... . RAC宏第一个参数是target，也就是需要绑定的对象；第二个参数是keyPath, 也就是对象中需要绑定的属性名。RAC实际上是创建了一个RACSubscriptingAssignmentTrampoline对象，并调用其setObject:forKeyedSubscript:方法。 . // RACSubscriptingAssignmentTrampoline.m - (void)setObject:(RACSignal *)signal forKeyedSubscript:(NSString *)keyPath { [signal setKeyPath:keyPath onObject:self.target nilValue:self.nilValue]; } // RACSignal+Operations.m - (RACDisposable *)setKeyPath:(NSString *)keyPath onObject:(NSObject *)object nilValue:(id)nilValue { // ... RACDisposable *subscriptionDisposable = [self subscribeNext:^(id x) { // ... __strong NSObject *object __attribute__((objc_precise_lifetime)) = (__bridge __strong id)objectPtr; [object setValue:x ?: nilValue forKeyPath:keyPath]; } error:^(NSError *error) { // ... [disposable dispose]; } completed:^{ [disposable dispose]; }]; // ... } . 可见RAC的下标set操作对右边的信号调用了subscribeNext, 并在所有的next event中通过object setValue:forKeyPath修改属性的值。在这个例子里，也就相当于[self.nameTextField setValue:x forKeyPath: @&quot;text&quot;] . RACObserve . RACObserve的展开: . ({ __weak id target_ = (self); [target_ rac_valuesForKeyPath:@(&quot;name&quot;) observer:self]; }); . rac_valuesForKeyPath:observer:调用了另一个方法: . // NSObject+RACPropertySubscribing.m - (RACSignal *)rac_valuesAndChangesForKeyPath:(NSString *)keyPath options:(NSKeyValueObservingOptions)options observer:(__weak NSObject *)weakObserver { // ... return [[[RACSignal createSignal:^ RACDisposable * (id&lt;RACSubscriber&gt; subscriber) { //... return [self rac_observeKeyPath:keyPath options:options observer:observer block:^(id value, NSDictionary *change, BOOL causedByDealloc, BOOL affectedOnlyLastComponent) { [subscriber sendNext:RACTuplePack(value, change)]; }]; }] takeUntil:deallocSignal] setNameWithFormat:@&quot;%@ -rac_valueAndChangesForKeyPath: %@ options: %lu observer: %@&quot;, self.rac_description, keyPath, (unsigned long)options, strongObserver.rac_description]; // ... } . 这个方法创建了一个RACSignal, 这个Signal在keypath发生变化时会发出“通知”. 这个Signal由RACKVOTrampoline实现，RACKVOTrampoline封装了KVO，将在keypath发生变化时发送信号。 . // NSObject+RACKVOWrapper.m - (RACDisposable *)rac_observeKeyPath:(NSString *)keyPath options:(NSKeyValueObservingOptions)options observer:(__weak NSObject *)weakObserver block:(void (^)(id, NSDictionary *, BOOL, BOOL))block { // ... RACKVOTrampoline *trampoline = [[RACKVOTrampoline alloc] initWithTarget:self observer:strongObserver keyPath:keyPathHead options:trampolineOptions block:^(id trampolineTarget, id trampolineObserver, NSDictionary *change) { // ... 当KVO发生后进入这里，这里会调用前面传入的block } } . rac_textSignal . 知道了RAC和RACObserve，接下来我们来看rac_textSignal的实现： . // UITextField+RACSignalSupport.m - (RACSignal *)rac_textSignal { @weakify(self); return [[[[[RACSignal defer:^{ @strongify(self); return [RACSignal return:self]; }] concat:[self rac_signalForControlEvents:UIControlEventEditingChanged | UIControlEventEditingDidBegin]] map:^(UITextField *x) { return x.text; }] takeUntil:self.rac_willDeallocSignal] setNameWithFormat:@&quot;%@ -rac_textSignal&quot;, self.rac_description]; } . 可见rac_textSignal监控UIControlEventEditingChanged | UIControlEventEditingDidBegin事件，subscriber将获得textfield.text . weakify 和strongify . RAC中常见的@weakify 和 @strongify展开如下： . @weakify @autoreleasepool {} __attribute__((objc_gc(weak))) __typeof__(self) self_weak_ = (self);; @strongify __typeof__(self) self = self_weak_; . 为了看起来美观，用了一个autoreleasepool的hack，使得宏的前面可以用@，有点意思。 . textView . 前面Textfield的rac_textSignal使用监控control events的方式来获取变化。但是UITextView并没有这些event。所以RAC使用了delegate的方式来实现UITextview的rac_textSignal。但是又有一个问题：如果想同时使用delegate怎么办呢？ . delegate forwardInvocation . RACDelegateProxy会用forwardInvocation的方式发送给self.delegate 所以如果需要同时使用，先设置delegate,再使用rac_textSignal即可。 . // UITextView+RACSignalSupport.m - (RACSignal *)rac_textSignal { @weakify(self); RACSignal *signal = [[[[[RACSignal defer:^{ @strongify(self); return [RACSignal return:RACTuplePack(self)]; }] concat:[self.rac_delegateProxy signalForSelector:@selector(textViewDidChange:)]] reduceEach:^(UITextView *x) { return x.text; }] takeUntil:self.rac_willDeallocSignal] setNameWithFormat:@&quot;%@ -rac_textSignal&quot;, self.rac_description]; RACUseDelegateProxy(self); return signal; } . signalForSelector是RAC中一个重要的方法，它的作用简单来说就是勾住一个方法，当这个方法被调用时把所有的参数放在一个tuple里，当做信号发出来。因此，RACUseDelegateProxy(self)把UITextView的delegate指向RACDelegateProxy，而RACDelegateProxy signalForSelector:@selector(textViewDidChange:)就可以捕捉所有原本调用delegate的方法，并发出textView实例作为信号，并在reduceEach中获得其text属性。 . 参考 . 关于ReactiveCocoa: . http://cocoasamurai.blogspot.jp/2013/03/basic-mvvm-with-reactivecocoa.html . http://www.raywenderlich.com/62699/reactivecocoa-tutorial-pt1 .",
            "url": "https://blog.codescv.com/ios/reactive-1.html",
            "relUrl": "/ios/reactive-1.html",
            "date": " • Jan 30, 2016"
        }
        
    
  
    
        ,"post26": {
            "title": "正则表达式中的各种分组方式",
            "content": "在使用正则表达式时，我们常常用()来表示分组，例如: . &gt;&gt;&gt; re.search(r&#39;(a(b))&#39;, &#39;xaby&#39;).groups() (&#39;ab&#39;, &#39;b&#39;) . 有时我们不需要返回这个分组，只是想用括号来表示语义逻辑，例如: . &gt;&gt;&gt; re.search(r&#39;(a(b|c))&#39;, &#39;xaby&#39;).groups() (&#39;ab&#39;, &#39;b&#39;) . 我们只想表示a的后面是b或者c，但是不想把b或者c当作一个分组返回。此时可以使用 非捕获的分组(non-capturing group), 语法为(?:): . &gt;&gt;&gt; re.search(r&#39;(a(?:b|c))&#39;, &#39;xaby&#39;).groups() (&#39;ab&#39;,) . 如果我们希望表达的意思是a的后面有b或c，但匹配后只返回a，可以用positive look ahead, 语法为(?=): . &gt;&gt;&gt; re.search(r&#39;(a(?=b|c))&#39;, &#39;xaby&#39;).groups() (&#39;a&#39;,) . 类似的，如果想匹配a的后面不是某个pattern，可以用negative look ahead, 语法为(?!) . &gt;&gt;&gt; re.search(r&#39;(a(?!c|d))&#39;, &#39;xaby&#39;).groups() (&#39;a&#39;,) . 同样，我们也可以表示a的前面有或者没有某个pattern, 分别为positive look behind 和 negative look behind, 语法为(?&lt;=)和(?&lt;!): . &gt;&gt;&gt; re.search(r&#39;((?&lt;=a)b)&#39;, &#39;xaby&#39;).groups() (&#39;b&#39;,) &gt;&gt;&gt; re.search(r&#39;((?&lt;!c|d)b)&#39;, &#39;xaby&#39;).groups() (&#39;b&#39;,) .",
            "url": "https://blog.codescv.com/regex/regex-groups.html",
            "relUrl": "/regex/regex-groups.html",
            "date": " • Jan 20, 2016"
        }
        
    
  
    
        ,"post27": {
            "title": "U combinator, Y combinator和递归",
            "content": "最近把Y combinator搞清楚了，总算是迈入了函数式编程的大门，可喜可贺！ . Why Y? Why? . Y combinator有什么用？为什么需要它？我们想象这样一个问题：要在scheme里面实现一个计算阶乘的函数，可能会这样写： . (define fact (lambda (n) (if (= n 0) 1 (* n (fact (- n 1)))))) (fact 5) ; =&gt; 120 . 程序没有什么难理解的，使用递归来计算阶乘。但是，如果我们想要在一个匿名函数里实现怎么办？换句话说，怎么在lambda中进行递归？ . U combinator . 要在进行递归，我们必须找到一种让函数调用自己的办法。假设我们有以下函数: . (define fact1 (lambda (f) (lambda (n) (if (= n 0) 1 (* n ((f f) (- n 1))))))) . 不难发现，虽然这个fact1函数并不是我们想要的递归函数， 但是(fact1 fact1) = fact。原因是我们有 . ((fact1 fact) 0) = 1 ((fact1 fact1) n) = n * ((fact1 fact1) n-1) ; 当 n != 1 . 这两点。 因此我们可以得到一个递归的阶乘定义： . (define fact (fact1 fact1)) . 事实上，这种模式我们可以用在任何递归函数上，我们定义 . (define (U x) (x x)) (define fact U(fact1)) . 就可以把这种递归的方式进行推广。这里面的U就是传说中的U combinator。 . Y combinator . 我们刚才提到U combinator可以实现递归，即使编程语言不支持递归，只支持高阶函数。那么Y combinator又是什么呢？ 我们考虑以下的函数: . (define fact2 (lambda (f) (lambda (n) (if (= n 0) 1 (* n (f (- n 1))))))) . 这个函数很像我们的fact函数（注意和fact1的区别！），它和fact函数之间是什么关系呢？ . (fact2 fact) = (lambda (n) (if (= n 0) 1 (* n fact (- n 1)))) = fact . 所以说(fact2 fact) = fact， 也就是说， fact是fact2的 不动点。 . 我们定义一个函数Y, 接受一个函数f为参数，返回f的不动点。这就是Y combinator。 . (Y f) = (f (Y f)) . 所以，假设我们可以得到Y, 那么 . fact = (Y fact2) . 通常所说的Y combinator是这个： . (define Y (lambda (f) (U(lambda (x) (f (lambda (t) ((x x) t))))))) . 具体推导过程这里略过，但是我们可以很容易验证满足(Y f) = f (Y f)成立。 . 但实际上不动点函数有很多种，这里介绍另外一种的推导，首先我们根据定义有 . (define Y (lambda (f) (f (Y f)) . 事实上这个定义在lazy-scheme里面是可以成立的，但是在applicative order中会无限循环，我们需要这样修改： . (define Y (lambda (f) (f (lambda (t) ((Y f) t))))) . 这样的定义已经可以work了，可以试试((Y fact2) 5) = 120。但是我们还得把函数定义里面的Y给消除掉。怎么消除呢？想起前面提到的U combinator了吗？没错，我们就用它: . (define Y (U (lambda (y) (lambda (f) (f (lambda (t) (((y y) f) t))))))) . 用(y y)代替需要递归的函数，然后在外围加一个U，就得到了一个可以工作的Y的定义。这个定义和我们之前介绍的并不一样。其实Y combinator不止一个。 . 推荐阅读 . http://mvanier.livejournal.com/2897.html . Fixed-point combinator . Lambda calculus . http://matt.might.net/articles/js-church/ .",
            "url": "https://blog.codescv.com/functional/combinators.html",
            "relUrl": "/functional/combinators.html",
            "date": " • Jan 8, 2016"
        }
        
    
  
    
        ,"post28": {
            "title": "一个swift编译器范型bug",
            "content": "才写了没几天swift就碰到了一个编译器bug，人品也是杠杠的。我是在写一个core data wrapper的时候碰到这个问题的： . class Wrapper { public func query&lt;T:NSManagedObject&gt;(entity: T.Type) -&gt; QueryOperation&lt;T&gt; { let entityName:String = NSStringFromClass(entity).componentsSeparatedByString(&quot;.&quot;).last! return QueryOperation&lt;T&gt;(entityName: entityName, dq: self) } } . 我本来想这样调用： . Wrapper(...).query(MyCoreDataModel).filter(condition).all() . 这样是没问题的。 后来我又写了另一个函数： . class Wrapper { public func insertObject&lt;T:NSManagedObject&gt;(entity: T.Type, context: NSManagedObjectContext) -&gt; T { let entityName = NSStringFromClass(entity).componentsSeparatedByString(&quot;.&quot;).last! return NSEntityDescription.insertNewObjectForEntityForName(entityName, inManagedObjectContext: context) as! T } } . 然后发现像下面这样调用的时候，竟然编译错误： . Wrapper(...).insertObject(MyCoreDataModel, context: context) . 编译器显示： . Expected member name or constructor call after type name . google了一下，正确的用法应该是: . Wrapper(...).insertObject(MyCoreDataModel.self, context: context) . 也就是说在swift中，传递类对象必须用类名加上.self, 否则会编译错误。而前一个编译成功竟然是bug。不知道是出于什么考虑非要这样, 或者说如果不加的话有什么危害。 . 详细可以看这里的讨论。 .",
            "url": "https://blog.codescv.com/ios/swift-compiler-bug.html",
            "relUrl": "/ios/swift-compiler-bug.html",
            "date": " • Dec 19, 2015"
        }
        
    
  
    
        ,"post29": {
            "title": "swift optionals",
            "content": "swift中一个objective-c没有的概念就是optionals. optionals在作用上类似于objc中的nil pointer, 但更加安全。 . Optional的基础用法 . class Animal { func name() -&gt; String { return &quot;animal&quot; } } var a1: Animal? a1 = Animal() a1!.name() // animal var a2: Animal? a2?.name() // nil a2!.name() // runtime error var a3: Animal = nil // compiler error var a3: Animal = a2 // compiler error . optional有两种状态，一种是nil, 一种是某个对象的值. . 使用在类型名后加?的方式来创建一个optional. . optional可以像赋予对象，也可以赋予nil。 . 普通变量不可以赋nil, 也不可以赋optional。(均为 compiler error) . 当需要取optional里面对象的值时，使用?或者!，该过程称为unwrap，会把optional变为普通变量. ?类似于objc中的nil pointer，当optional变量为nil时进行no-op, 不会报错; 而!当变量为nil时会crash，类似于其他语言中的NPE. . 有了optional后，我们就可以方便的使用类似objc中nil pointer的特性，但是同时又可以用crash来及时的找出程序中的bug，可以说是兼具两者的优点。 . implicit unwrap (隐式解包) . var a2: Animal? a2?.name() // nil var a3: Animal! = a2 // a3 = nil a3.name() // crash var a4: Animal = a2! // crash . 当我们希望把一个optional解开时, 可以用在类名后加!的方式。这样得到的变量a3类型仍为optional，只是在调用其方法时会自动用!来解开，不需要再显式解包。注意a3和a4的不同：a3是一个optional，而a4是一个普通变量，所以在调用a2!的时候会直接crash。 . 隐式解包在和objc代码接口时常常会用到，因为objc里的类变量对于swift看来都是optional。 . optional binding . 使用if let可以把check optional和unwrap一起进行，如果不为nil则调用if内部，同时将optional转化为实际类型: . if let x = someOptional { // someOptional is not nil, x gets the value } . optional chaining . 可以使用?将多个optional调用连接，任何一步optional为nil都为noop。例如： . x.prop1?.prop2?.method() . 方法+? . objc中使用respondsToSelector:来判断一个对象是否能调用某个方法。在swift中，可以直接用someObj.method?()来完成同样的功能，如果method不存在就是no-op. . 构造函数+? . init?() 可以用来定义一个可失败的构造函数。当构造失败时返回nil。 . 类型转换 . swift在进行类型转换时，使用as关键字。向上转换（转换为父类）使用as，编译器可以自动判断其安全性；向下转换时使用as?会生成optional, 转换不成功为nil；使用as!会自动unwrap optional，如果转换失败会crash。 . Optional的定义 . swift 中的Optional 定义如下(简化了很多细节)： . public enum Optional&lt;Wrapped&gt; { case None case Some(Wrapped) } . 可以看到，Optional其实是个enum, 所以以下代码是等效的： . var a1: Animal? a1 = Animal() if let a = a1 { print(a.name()) } // pattern matching if case .Some(let a) = a1 { print(a.name()) } // pattern matching using switch switch(a1) { case .None: print(&quot;nil&quot;) case .Some(let a): print(&quot;animal (a.name())&quot;) } . 所以，?, !等，都是一些语法糖，都可以转换为普通的pattern matching代码。 .",
            "url": "https://blog.codescv.com/ios/swift-optionals.html",
            "relUrl": "/ios/swift-optionals.html",
            "date": " • Nov 21, 2015"
        }
        
    
  
    
        ,"post30": {
            "title": "[iOS] Storyboard 技巧总结",
            "content": "以前做iOS开发的时候主要还是用nib，最近几年Stroyboards变得比较流行。这几天集中学习了Storyboard的相关内容，总结下经验。 . 阻止segue . 有时候在点击一个按钮的时候希望进行一个check，满足条件才继续进行。在iOS6以后可以做到这一点： . - (BOOL)shouldPerformSegueWithIdentifier:(NSString *)identifier sender:(id)sender . 注意如果在代码中出发segue是不会自动调用这个函数的，需要你自己来调用。 . unwind segue . unwind segue用于”回退”一个segue。这个回退不一定是完全的go back。 应用场景： . 注册新用户向导，下一步，下一步，然后点击重置回到第一个Controller | 在一个tab中进行了操作，希望把当前的操作做完后切换到另一个tab的另一个controller. | . 使用多个Stroyboard . Storyboard的一个重要缺点，也是我一直很嫌弃它也没怎么用它的原因在于，它把所有的ViewController放在一个文件里，这样有3个重大缺陷： . 对重用很不利 | 对多人合作时的版本控制merge很不友好。 | ViewController多了以后，想找到相关的东西很费事。 | 解决办法是： 使用多个Storyboard。 . Xcode 7之前 . 将Stoyboard拆成多个文件，例如每个tab一个storyboard。然后使用[UIStroyboard instantiateInitialViewController]和[UIStroyboard instantiateViewControllerWithIdentifier:] 来加载每个storyboard中的controller, 并把这些controller 设置为根tabcontroller的content controller. . 当需要跨storyboard进行segue时，可以创建一个custom segue， 然后改写perform方法，将需要跳转到的scene encode到segue identifier里。可以看这篇文章和代码。 . Xcode 7之后 . Xcode 7增加了一个new feature, 可以直接框选一组scene， 然后在菜单中选Editor -&gt; refactor to storyboard 来创建新的storyboard。refactor之后，segue扔保持其连接。 . 同时，也可以手动从库中拖出storyboard reference 来直接创建跨storyboard的segue，非常方便。 .",
            "url": "https://blog.codescv.com/ios/storyboards.html",
            "relUrl": "/ios/storyboards.html",
            "date": " • Nov 14, 2015"
        }
        
    
  
    
        ,"post31": {
            "title": "[iOS] Blocks 中的递归",
            "content": "一点废话 . 已经有一年多没有写blog了。最近一年人变的好懒，我觉得这样很不好，希望从现在开始可以坚持再写一写blog。 最近又重新在看了很多ios的东西，觉得ios开发还是很有意思的，想自己动手做app，也许哪天能回归也说不定。 . 回归正题 . 今天正好看到blocks，突然想到一个问题：在blocks里面要如何实现递归？我先按自己的想法试了下，用blocks来实现一个递归版本的阶乘函数fact： . int (^fact)(int) = ^int(int a) { if (a == 0) return 1; else return a * fact(a-1); }; NSLog(@&quot;fact(3)=%d&quot;, fact(3)); . 运行以上代码后会crash，报错EXC_BAD_ACCESS，为何？原因是因为非__block变量会在block被创建的时候就直接捕获，而这个时候fact = xxx 这个赋值还没有发生，因此block中的fact变量会永久成为nil。 . 知道原因后，我们可以把fact声名为__block变量，这样该变量会被block放到堆上并进行retain操作，从而保持住值。下面来看修改后的版本： . __block int (^fact)(int) = ^int(int a) { if (a == 0) return 1; else return a * fact(a-1); }; NSLog(@&quot;fact(3)=%d&quot;, fact(3)); . 改成__block后程序可以正确输出fact(3) = 6，但是我们还有一个问题：循环引用。由于fact和block之间互相强引用，会造成内存泄漏。关于这一点，我们可以通过代码来验证： . __weak int (^var)(int); { __block int (^fact)(int) = ^int(int a) { if (a == 0) return 1; else return a * fact(a-1); }; var = fact; NSLog(@&quot;fact(3)=%d&quot;, fact(3)); } NSLog(@&quot;var: %@&quot;, var); . 可以看到，我们声明了一个weak变量，如果里面没有内存泄漏的话，花括号结束后fact离开作用域，应该释放其内存，从而使var变为nil。而这里我们发现var在花括号结束后还有值，说明内存没有被正确的释放掉。 . 假设我们在return 1这里释放内存行不行呢？例如下面： . __weak int (^var)(int); { __block int (^fact)(int) = ^int(int a) { if (a == 0) { fact = nil; return 1; } else return a * fact(a-1); }; var = fact; NSLog(@&quot;fact(3)=%d&quot;, fact(3)); } NSLog(@&quot;var: %@&quot;, var); . 看上去是可以了，然而连续调用两次fact就会crash，因为这时候fact已经被设置为nil了。 . 因此我们需要一个weak变量来指向block，来帮助我们释放内存。最后的解决方案如下： . __weak int (^var)(int); { int (^fact)(int); __block __weak int (^weakFact)(int); weakFact = fact = ^int(int a) { if (a == 0) { return 1; } else return a * weakFact(a-1); }; var = fact; NSLog(@&quot;fact(3)=%d&quot;, fact(3)); } NSLog(@&quot;var: %@&quot;, var); . 我们使用两个变量，一强一弱，用弱指针进行递归，强指针使得block不会提前被释放。当强指针离开作用域时，block被正确释放。 . 可见，要在Objective-C中正确的使用block来递归还是有很多坑的。这一点也是由于其引用计数的策略造成的。 .",
            "url": "https://blog.codescv.com/ios/blocks-recursion.html",
            "relUrl": "/ios/blocks-recursion.html",
            "date": " • Nov 8, 2015"
        }
        
    
  
    
        ,"post32": {
            "title": "Android 源代码上手指南 - Makefile语法",
            "content": "上一篇我们将到android build system, 讲了envsetup.sh和lunch, 在继续研究build过程之前有必要先讲讲Makefile的语法. android使用GNUMake 3.81(不支持其他版本!), 因此有许多GNU的特定语法, 和一般的Makefile长得很不一样. (为了叙述方便, 本文中凡是提到Makefile的地方, 均指GNU Makefile.) Makefile的基本语法是这样的: . TARGET: &lt;DEPENDS&gt; &lt;TAB&gt; instructions . 运行make TARGET命令时, 根据TARGET对应的DEPENDS来确定依赖关系, 根据修改时间来判断某个TARGET是否需要rebuild.如果需要, 则执行TARGET对应的instructions. 这就是Makefile的基本原理. . .PHONY targets . 在Makefile中, 有时会遇到标明为.PHONY的target, 它表示这个target只是一个名字, 不是一个文件. 例如: . clean: rm *.o temp . 这个名为clean的target指示makefile去做一些删除临时文件的指令, 但如果你的目录下刚好有个名为clean的文件, 又因为clean没有任何的dependencies, make会认为clean这个文件是最新的, 不需要执行. 这时我们就要告诉make, clean这个target只是一个名称而已, 并不是一个文件: . .PHONY: clean . multiple targets . 当一行写有个多个target时, 相当于把这些target分开写, 并使用一样的commands. 例如: . foo bar: echo $@ . 等价于: . foo: echo $@ bar: echo $@ . 其中$@会被替换为target名称. . 在GNUMake中, 有一些特定的语法在android中广泛使用, 下面一一介绍一下: . define . 在Makefile中, 定义一个变量可以用=或者:=. 其中=和:=的区别在于, =是recusively expanded variable, 它在每次被用到的时候都会被求值; 而:=是simply expanded variable, 它在定义的时候就被展开. 例如: . x = foo var1 = $(x) var2 := $(x) x = bar target: echo var1: $(var1) echo var2: $(var2) . 执行make会显示: . echo var1: bar var1: bar echo var2: foo var2: foo . 除了=和:=, 还有一个?=, 它只在变量没有被定义时起作用, 常用于设置默认值. . define 也可以用来定义一个变量, 但define更强的一点在于可以定义多行变量. 例如: . define command @echo foo @echo bar endef target: $(command) . 输出: . foo bar . 其中@放在一个命令前, 表示不输出这一行执行的命令. 不过, 在Makefile中有一个call函数, 它可以”调用”一个变量, 因此变量可以用来实现类似于宏展开的功能. 例如: . define concat $1,$2 endef target: @echo $(call concat,foo,bar) . 会输出: . foo,bar . 这里有个坑: $(call concat,foo,bar)注意我没有输入空格, 如果我写成$(call concat, foo, bar), 那么在concat展开的时候, 参数$1就会等于前面带一个空格的_foo, $2就会等于前面带一个空格的_bar(我这里用下划线来表示空格作为警示). 这个在用的时候要格外小心! . include . include用于包含另一个Makefile, 这个在比较大规模的项目中比较有用, 有助于Makefile的重用和模块化. 例如你可以将一些常用的宏扩展和变量定义放在一个definitions.mk中, 然后从你的Makefile中include之. . include some_file.mk -include other_file.mk . 当include一个文件而这个文件不存在时, Make会报错, 而在include之前加上-表示文件不存在时不要报错. 这个常用来包含可选的文件. . if . 在make语法中有一些常用的条件语句, 类似于C语言中的#ifdef和#if等. 例如: . ifeq($(var),foo) echo foo else echo not foo endif . ifeq检测后面两个值是否相等. 类似的还有ifneq, ifdef, ifndef检测一个变量是否被定义, 等等. . 函数 . makefile中内置了一些函数, 方便实现一些常用的功能. 调用一个函数的语法是: . $(function arguments) 或者 ${function arguments} . 下面列举一些常见函数: . 字符串操作 . $(subst from, to, text) 字符串替换 | $(strip string) 去掉开头和末尾的空白. (还记得前面说的那个坑吗?) | $(filter pattern…, text) 取出所有匹配的文本, 丢到所有不匹配的. 例如$(filter %.c %.s,$(sources))取出sources变量中所有以.c和.s结尾的文本. | $(filter-out pattern…, _text) 和上面相反, 取出所有不匹配的. | $(word n, text) 取出text中第n项. | . 文件名操作 . $(dir names…) 取出names中所有文件的目录部分. | $(notdir names…) 和上面相反, 取出非目录部分(纯文件名部分). | $(add_prefix prefix, names…) 给文件加前缀 | $(wildcard pattern) 获取匹配通配符的文件名. 例如objects := $(wildcard *.o)返回所有.o结尾的文件. | . foreach . $(foreach var, list, text) 将var作为形式参数, list作为列表, 扩展text的内容. 相当于python中的: . for var in list: text . call . 在前面提到了, 用于”展开”一个宏(变量) . eval . eval是个很重要的函数, 可以用来实现Makefile的模板. 比较关键的一点是需要知道传给eval的参数被展开了两次, 首先是传给eval之前, 然后再被当作makefile的语句include到当前的地方. 这个我们之后会继续讲到, 先放一个例子在这里. . PROGRAMS = server client server_OBJS = server.o server_priv.o server_access.o server_LIBS = priv protocol client_OBJS = client.o client_api.o client_mem.o client_LIBS = protocol # Everything after this is generic .PHONY: all all: $(PROGRAMS) define PROGRAM_template = $(1): $$($(1)_OBJS) $$($(1)_LIBS:%=-l%) ALL_OBJS += $$($(1)_OBJS) endef $(foreach prog,$(PROGRAMS),$(eval $(call PROGRAM_template,$(prog)))) $(PROGRAMS): $(LINK.o) $^ $(LDLIBS) -o $@ clean: rm -f $(ALL_OBJS) $(PROGRAMS) . 输出调试 . 这些语句被用于输出调试信息: (debug和研究时都非常有用!) . $(warning text…) | $(info text…) | $(origin variable) 可以告诉你一个变量是从哪里来的, 是环境中定义的, 还是makefile中定义的, 还是命令行中定义的. 这个在调试的时候也很有用. 参考这里 | . 运行shell语句 . $(shell command) 用于运行一个shell语句, 然后将输出返回. 这个也非常有用, 例如有了这个你就可以用你自己喜欢的工具例如perl之类来进行字符串操作了, 可以不用费力学习和记忆makefile中相应的函数的语法. | .",
            "url": "https://blog.codescv.com/survival-guide-android-source-code-makefile-syntax.html",
            "relUrl": "/survival-guide-android-source-code-makefile-syntax.html",
            "date": " • Mar 22, 2014"
        }
        
    
  
    
        ,"post33": {
            "title": "Android 源代码上手指南 - Build System初探",
            "content": "Android总体来说是个挺好用的系统, 虽然有不少这样那样让我觉得不喜欢的地方. 不过好在它是开源的, 于是很多不爽的地方可以自己改. 研究Android源代码也有一段时间了, 在github上开了个坑, 把自己的一些想法加了进去, 取名为yacm: . http://github.com/codescv/yacm . 之所以叫yacm主要是因为它是基于CyanogenMod改的 (yet another cyanogenmod). 里面加了一些我觉得很需要但是CM没有提供的功能, 例如: . 去掉了拨VPN必须设置锁屏密码的限制. (Android脑残点之一) | 将媒体扫描改为白名单模式, 只有白名单内的几个文件夹(如DCIM, Pictures等)会被扫描, 再也不用担心相册里出来一大堆乱七八糟的广告图片什么的了. (Android脑残点之二) | 拨号盘增加首字拼音搜索. (中国用户必备) | 增加了一些方便开发者的工具, 例如下拉快捷菜单中的ADB开关, 电源按钮开关等 | . 在研究的过程中也走了不少弯路, 这里尽量把这些东西都记下来, 一方面提醒自己, 一方面也希望对后来人有所帮助. . 首先, 研究android代码应该从哪里开始? 我首先是找到了几本国内的android源代码分析的书, 一般套路是先讲讲怎么编译, 然后就直接开始研究代码了. 其实这种方式很不好, 因为android工程极其庞大(我测试了一下, 光makefile就有几千个), 无论你怎么分析源代码也只是九牛一毛, 完全抓不住主线. 即使把这些书都看完了, 也还是停留在纸上谈兵的阶段. . 后来我找到一本书叫做 Embedded Android, 这本书对我启发非常大, 主要是它真的很懂刚上手Android源代码的程序员需要什么. 首先大概的介绍了一下android的架构, 然后就开始将构建系统(Build System), 然后讲各种android specific的知识点, 例如硬件, shell命令, init.rc, system server等等, 几乎很少涉及代码. 所谓构建系统就是讲android中这么多的工程是如何build到一起的, 哪些工程最后会进入最终的系统包. 事实上, 对于很多程序员来说, android中最陌生的是构建系统, 因为里面用了非常多的古怪语法(你一开始都不敢相信这些是Makefile), 而Java或者C代码很多情况下不需要解释和分析都能看懂. 只有先了解了构建系统, 才能有方向, 否则以上来就研究代码必然是像无头苍蝇乱撞, 似懂非懂的改了一通代码, 结果发现这段代码根本没编译进最后的项目. . 所以说, 我的这个blog系列也准备先抛开代码不谈, 先介绍构建系统, 然后做一些小的修改, 例如预装一些软件, 修改一些默认设置等等, 先玩转构建系统, 搞清来龙去脉, 之后会发现hack代码是水到渠成的事. . 本文假定你已经从CM上sync了最新的代码, 并且能够编译成功了. 由于我的手机是HTC One, 所以我的代码是基于HTC One来改的, 不过其他手机原理也都是一样的. 如果你还没有编译成功, 这里有个传送门. . 当你需要编译android源代码的时候, 其实有三步, 缺一不可: . source build/envsetup.sh lunch cm_m7-userdebug mka bacon . 第一步是加载了 build/envsetup.sh中的各种函数 (后面会讲到), 第二步主要是检验makefile的合理性(在一定程度上), 以及设置和具体device相关的各种环境变量, 第三步是真正的build. 接下来我们一一分析. . source build/envsetup.sh . 这一步是运行了build/envsetup.sh这个文件. 在我的机器上, 运行输出是这样的: . including device/generic/armv7-a-neon/vendorsetup.sh including device/generic/goldfish/vendorsetup.sh including device/generic/mips/vendorsetup.sh including device/generic/x86/vendorsetup.sh including vendor/cm/vendorsetup.sh including sdk/bash_completion/adb.bash including vendor/cm/bash_completion/git.bash including vendor/cm/bash_completion/repo.bash . 说明这个文件会加载一些其他的文件, 比如很多目录下的vendorsetup.sh. 最后三个是用来做命令行补全的, 例如adb rem&lt;TAB&gt; 会给你补全成adb remount. 先不管. . 我们看看envsetup.sh的内容, 这个文件总的来说是一堆util函数/命令的集合. 其中有几个特别有用的: . 懒人必备型 croot: 切换到顶部. android目录很深, 经常进去就回不去了, 于是这个可以一键回家. mm: 单独编译某个工程. godir: 匹配一个目录名称, 进入该目录. (例如godir m7会匹配到device/htc/m7, 虽然也会有很多其他目录) 关键命令 lunch - 后面要用的, 用来选择设备 mka - 约等于一个智能多进程的make . 刨去各种函数的定义, 这个脚本的副作用很少, 只有在文件底部有这么几行: . for f in `/bin/ls vendor/*/vendorsetup.sh vendor/*/*/vendorsetup.sh device/*/*/vendorsetup.sh 2&gt; /dev/null` do echo &quot;including $f&quot; . $f done unset f . 这其实是加载了vendor和device下的vendorsetup.sh脚本, 也就解释了之前的输出. . 这个文件有很多函数(2000行), 我们从哪里入手呢? 我们按时间顺序, 先看看各个被include的vendorsetup.sh文件做了什么吧: . for combo in $(curl -s https://raw.github.com/CyanogenMod/hudson/master/cm-build-targets | sed -e &#39;s/#.*$//&#39; | grep cm-11.0 | awk {&#39;print $1&#39;}) do add_lunch_combo $combo done . 其实各个子文件夹下的vendorsetup.sh的内容都差不多, 都是在调用add_lunch_combo, 只是后面的参数不一样. 我们从名字就可以猜出, add_lunch_combo其实是在增加lunch的品种–也就是增加一种设备选项而已. 而cm下的这个文件, 事实上是从网络上一个文件里读到了所有cm官方支持的设备品种, 然后把它们都加上, 这样就可以在lunch的时候选择了. . 我们看一下add_lunch_combo的定义, 其实很简单: . unset LUNCH_MENU_CHOICES function add_lunch_combo() { local new_combo=$1 local c for c in ${LUNCH_MENU_CHOICES[@]} ; do if [ &quot;$new_combo&quot; = &quot;$c&quot; ] ; then return fi done LUNCH_MENU_CHOICES=(${LUNCH_MENU_CHOICES[@]} $new_combo) } . 只是把new_combo加到一个全局变量LUNCH_MENU_CHOICES里面而已. . lunch . 目前为止我们并没有做什么事情, 只是定义了很多函数, 然后添加了一些combo. 而且我们知道接下来就只要看lunch这一个函数就可以了. 这个函数做了些什么呢? . function lunch() { # ... # 前面只是验证参数合法性, 略 # selection也就是运行lunch带的参数, 如果selection = cm_m7-userdebug, 则 product = cm_m7 local product=$(echo -n $selection | sed -e &quot;s/-.*$//&quot;) # 验证product是否合法 check_product $product # roomservice (见后面分析) if [ $? -ne 0 ] then # if we can&#39;t find a product, try to grab it off the CM github # 一看到上面这句注释就知道不是重点, 略 else build/tools/roomservice.py $product true fi # variant = userdebug local variant=$(echo -n $selection | sed -e &quot;s/^[^ -]*-//&quot;) # 验证variant的合法性 check_variant $variant #... # 这三个全局变量很重要, 后面到处都在用! export TARGET_PRODUCT=$product export TARGET_BUILD_VARIANT=$variant export TARGET_BUILD_TYPE=release # 不要在意这种细节 fixup_common_out_dir # 见后面分析 set_stuff_for_environment # 一看就知道没啥用 printconfig } . 这里罗嗦几句: . roomservice是用来获取特定设备的dependencies的, 例如htc手机需要htc的kernel, 它的代码从哪里获取呢? 就是roomservice管理, 依据cm.dependencies文件获取到repo地址, 然后放到.repo/local_manifest里面. 噢对了, 如果你还不熟悉repo, 请出门右转 . set_stuff_for_environment 里面设置了一些shell prompt, path什么的. 如果你跟我一样设置了自己的shell提示符, 那么最好设置一个STAY_OFF_MY_LAWN环境变量,这样android就不会乱搞shell prompt了. . 另外还有个值得一提的函数是get_build_var, 它在很多地方都用到, 例如check_product等等. 它的作用是获取一个build variable的值. . function get_build_var() { # ... CALLED_FROM_SETUP=true BUILD_SYSTEM=build/core make --no-print-directory -C &quot;$T&quot; -f build/core/config.mk dumpvar-$1 } . 可以看到get_build_var其实是执行了 build/core/config.mk 中的 dumpvar-$1 目标(替换成实际的环境变量, 例如dump-TARGET_PRODUCT). 关于makefile的语法规则我们在后面会更深入讨论. . 综上所述, lunch命令其实是设置了很多的环境变量, 而这些环境变量在后面的make会被用到. 想看看到底设置了多少环境变量吗? . set &gt; /tmp/result . 打开/tmp/result文件看看, 可以看到所有被设置过的环境变量值, 这些值在以后make的阶段都有很重要的用途. . 唔, 现在已经很晚了, 那么今天就先讲到这里吧, 后面就是重头戏mka bacon了, 不过在这之前可能要先介绍下make的各种语法. 各位晚安~! .",
            "url": "https://blog.codescv.com/android/android-build-system.html",
            "relUrl": "/android/android-build-system.html",
            "date": " • Mar 21, 2014"
        }
        
    
  
    
        ,"post34": {
            "title": "Bash在cd进入目录时自动启动脚本",
            "content": "玩android代码进入目录后总是需要source build/envsetup.sh, 比较烦, 能不能在cd进入时自动完成这个功能呢? . 可以的, 首先我们在.bashrc中重新定义cd这个函数: . mycd() { cd $@ local hook_file=.cd_hook if [[ -f $hook_file ]]; then . $hook_file fi } alias cd=&#39;mycd&#39; . 这样, 在进入一个目录后就会检测该目录下是否有.cd_hook文件, 如果有的话就自动source之. 然后我们在源代码目录下新建一个.cd_hook文件: . type lunch &gt;/dev/null 2&gt;&amp;1 # 用检测lunch函数是否有的方式确定是不是已经source过了 if [[ $? -ne 0 ]]; then source build/envsetup.sh fi . 注意, 这样修改cd是有一定安全性问题的, 因为恶意程序可以在目录下放置.cd_hook文件来做任何事情. 因此最好还是不要alias cd. 在实际使用的时候, 我用的是alias c=&#39;mycd&#39;, 这样我只有用”c”命令进入目录时会运行.cd_hook. . 顺便提一句, 在脚本中, 为了防止alias, 可以用 转义. 例如 ls ~会确保调用的不是alias后的ls. .",
            "url": "https://blog.codescv.com/bash-cd-hook.html",
            "relUrl": "/bash-cd-hook.html",
            "date": " • Mar 8, 2014"
        }
        
    
  
    
        ,"post35": {
            "title": "优雅的repo sync自动重试",
            "content": "最近在玩Android源代码, 比较头疼的是代码同步实在太痛苦, google的服务器三番两次的被墙, repo 工具又不是很健壮常常会一卡就是几个小时, 必须手动kill了再重试. . 于是想写个脚本自动检测, 如果检测到卡住了, 自动kill了重试. . 第一个版本 . 这里有几个问题要解决: . 1 怎么确定repo sync卡住了? . 这个地方我想到的方法是直接用ifstat检测网速, 正常情况下网络流量是有一定值的(例如100k~300k左右的下载速度), 而如果repo sync卡住了, 就会网络流量很低. . 这样理论上不是很准, 因为系统上可能有其他程序在跑, 不过就实际情况下来说也够用了. . 2 怎么kill ? . 首先想到的办法是直接ps | grep [r]epo 然后全部kill掉. 有些不完美: 如果系统上同时有好几个repo程序在跑就一起kill掉了. 容易误伤. | . 虽然有些缺陷, 我们还是写出一个基本可以工作的版本: . #!/bin/bash # FIXME: 只允许同时一个repo运行 kill_prog() { # 用ps找出所有的repo, 然后kill掉 PID=`ps aux |grep python|grep [r]epo |awk &#39;{print $2}&#39;` [[ -n $PID ]] &amp;&amp; kill $PID } start_sync() { repo sync &amp; } restart_sync() { kill_prog start_sync } # 如果网络流量在retry_delay时间内小于min_speed, 则认为repo sync已经卡住了 min_speed=&quot;50&quot; retry_delay=600 ((counter=0)) ((n_retries=0)) restart_sync while [[ 1 ]]; do # 用ifstat检测网速 speed=`ifstat 1 1 | tail -n 1 | awk &#39;{print $1}&#39;` result=$(echo &quot;$speed &lt; $min_speed&quot; | bc) if [[ $result == &quot;1&quot; ]]; then ((counter++)) else ((counter=0)) fi if ((counter &gt; retry_delay)); then ((counter=0)) echo &quot;netspeed low. restart!&quot; ((n_retries++)) restart_sync fi done echo &quot;completed with $n_retries retries&quot; . 在上面这个版本中, 我们使用ifstat来检测网速, 没秒检测一次, 如果持续一段时间没流量, 就kill掉所有repo进程. 这里有个问题: 如果repo sync正常退出了, 那么也就没有流量了, 所以这个脚本会无限执行, 永远不会停止. 同时, 我们需要知道repo退出时, 是成功的退出, 还是出错退出? 要检测返回值就必须用wait, 然而wait会使主进程 block住, 主进程就无法再去检测了. 为了解决这个问题, 可以使用bash coproc . Bash Coproc . 简单的说bash coproc就是创建一个子进程, 并可以取到子进程的一些信息, 可以和子进程通信. 这里我们需要的是子进程的pid信息, 只要有了这个, 我们就可以在不用wait的情况下知道子进程是否还活着, 如果已经退出了, 我们再用wait去取它的返回值就可以了. 创建coproc的语法如下: . coproc &lt;name&gt; command . 其中为你给他起的名字(可选), command为子进程运行的命令. 运行完后, 可以在$name_PID中取到子进程的PID. . 第二个版本 . 有了coproc, 我们可以写出第二个版本: . #!/bin/bash # 当前 repo sync 进程的 pid PID= kill_prog() { # kill 当前repo sync子进程 echo &quot;kill : $PID&quot; [[ -n $PID ]] &amp;&amp; kill $PID } start_sync() { # 启动子进程(使用coproc) coproc syncproc { repo sync; } PID=$syncproc_PID } restart_sync() { kill_prog start_sync } # 如果网络流量在retry_delay时间内小于min_speed, 则认为repo sync已经卡住了 min_speed=&quot;50&quot; retry_delay=300 ((counter=0)) ((n_retries=0)) restart_sync while [[ 1 ]]; do # 用ifstat检测网速 speed=`ifstat 1 1 | tail -n 1 | awk &#39;{print $1}&#39;` result=$(echo &quot;$speed &lt; $min_speed&quot; | bc) if [[ $result == &quot;1&quot; ]]; then ((counter++)) else ((counter=0)) fi if [[ `ps -p $PID| wc -l` == &quot;1&quot; ]]; then # 检测到子进程已经退出(ps已经查不到它了) # 用wait取得子进程返回值 wait $PID if [[ $? -eq 0 ]]; then echo &quot;sync successful&quot; break else echo &quot;sync failed&quot; ((counter=0)) ((n_retries++)) restart_sync continue fi fi if ((counter &gt; retry_delay)); then ((counter=0)) echo &quot;netspeed low. restart!&quot; ((n_retries++)) restart_sync fi done echo &quot;completed with $n_retries retries&quot; . 在这个版本里, 我们知道子进程的pid, 可以有针对性的kill, 并且在子进程正常退出时, 也能检测到, 并及时退出程序. 好了, 现在终于可以自动sync代码了, 下篇就开始写关于android的blog:-) .",
            "url": "https://blog.codescv.com/android/elegant-repo-sync-retries.html",
            "relUrl": "/android/elegant-repo-sync-retries.html",
            "date": " • Mar 7, 2014"
        }
        
    
  
    
        ,"post36": {
            "title": "排列问题",
            "content": "Kth Permutation . 有一组数例如”123456”, 它们可以组成 n!种排列, 将这些数从小到大排列: . &quot;123456&quot; &quot;123465&quot; &quot;123546&quot; ... . 问题: 在这些排列中,如何求出第k大的数(k从0开始)? . 思考: 考虑简单的情况 n = 3: . perm k k / (n-1)! 123 0 0 132 1 0 213 2 1 231 3 1 312 4 2 321 5 2 . 由于在n个元素组成的排列中, 以某个元素i开头的排列共有(n-1)!个. 不难得出结论: . 第k大的排列以 perm[k/(n-1)!] 开头. | 第k大的n个元素的排列, 等于以perm[k/(n-1)!]开头的元素, 上去掉这个元素后剩下的元素组成的第 k % (n-1)! 大的排列. | 于是就有了一个很简单的递归关系: . kth_perm(perm, k) = perm[k/(n-1)!] + kth_perm(perm[0..i-1] + perm[i..n-1], k % (n-1)!) . 根据这个关系可以写出kth_permutation的代码: . vector&lt;int&gt; kth_permutation(const vector&lt;int&gt; &amp;vec, int k) { if (k == 0) return vec; int n = vec.size(); k %= fact(n); int t = fact(n-1); int i = k / t; int kk = k % t; vector&lt;int&gt; result; result.push_back(vec[i]); vector&lt;int&gt; left; copy(vec.begin(), vec.begin()+i, back_inserter(left)); copy(vec.begin()+i+1, vec.end(), back_inserter(left)); left = kth_permutation(left, kk); copy(left.begin(), left.end(), back_inserter(result)); return result; } . Permutation Rank . 给定一个排列”231”, 它是”123”的排列中第几大的? . 由于2是第1大的, 因此有 2! * 1 个首位比 2 小的. . 由于3是剩下的数中第1大的, 因此有 1! * 1 个第二位比3小的. . 因此比”231”小的 有 2! * 1 + 1 ! * 1 = 3个, 因此”231”是第3大的. (均从0开始) . 一个比较naive的实现： . int permutation_rank(vector&lt;int&gt; &amp;vec) { vector&lt;int&gt; vec2(vec.begin(), vec.end()); sort(vec2.begin(), vec2.end()); map&lt;int, int&gt; ranks; for (int i = 0; i &lt; vec2.size(); i++) { ranks[vec2[i]] = i; } int rank = 0; int n = vec.size(); for (int i = 0; i &lt; vec.size(); i++) { rank += fact(n-1) * ranks[vec[i]]; for (map&lt;int, int&gt;::iterator it = ranks.begin(); it != ranks.end(); ++it) { if (it-&gt;first &gt; vec[i]) it-&gt;second--; } n--; } return rank; } . Next permutation . 已知一个排列”146532”, 如何求出比它大的下一个排列? 这个问题被称为next_permutation. . 一个简单的方法是, 使用前面的结论, 首先求出rank, 然后求第rank+1的排列. next_permutation(vec): k = rank(vec) return kth_permutation(vec, k+1) . 不过也有一个更加subtle, 效率也更高的方法: . 首先找出最大的i ,使得 a[i] &lt; a[i+1], 例如 1 4 6 5 3 2 i 此时, a是所有以a[0..i]开头的排列中最大的. 也就是说, 1 4 6 5 3 2是所有 1 4 开头排列中最大的. (如果这一步找不到, 说明原序列倒序排列, 已经是最大的一个排列了.) . 现在我们要使a变大, 但是变大的幅度要尽可能小, 因此我们要找到一个刚好比a[i]大的数, 也就是比a[i]大的数中最小的一个. . 因此,我们找出最大的j, 使得 a[i] &lt; a[j]: 1 4 6 5 3 2 i j . a[j]就是比a[i]大的数中最小的一个. 我们把它和a[i]交换: 1 5 6 4 3 2 i j . 此时, a是 a[0..i]开头的数中最大的一个, 这不满足我们的要求, 我们希望它是最小的一个. 因此, 我们将a[i+1..n-1]倒序: 1 5 2 3 4 6 . 即为所求. c++实现: . void reverse(vector&lt;int&gt; &amp;num, int i) { for(int j = num.size() - 1; j &gt; i; j--) { swap(num[i], num[j]); i++; } } void nextPermutation(vector&lt;int&gt; &amp;num) { if (num.size() &lt;= 1) { return; } // find max i where num[i] &lt; num[i+1] int i; for (i = num.size()-2; i &gt;= 0 &amp;&amp; num[i] &gt;= num[i+1]; i--); if (num[i] &gt;= num[i+1]) { reverse(num, 0); return; } // find max j where num[i] &lt; num[j] int j; for (j = num.size()-1; num[i] &gt;= num[j]; j--); swap(num[i], num[j]); reverse(num, i+1); } .",
            "url": "https://blog.codescv.com/permutations.html",
            "relUrl": "/permutations.html",
            "date": " • Sep 7, 2013"
        }
        
    
  
    
        ,"post37": {
            "title": "为virtualbox中的Linux虚拟机磁盘扩容",
            "content": "最近想自己hack一下android的源代码, 可是发现代码好大啊, 编译一下大概需要40G的空闲空间. 而之前装好的Linux本来硬盘就只有40G, 于是研究了一下, 发现是可以直接扩容的, 虽然步骤有点麻烦. . 备份数据 . 虽然说按照这个步骤做应该不会出什么问题, 但是重装系统毕竟是个麻烦事, 所以在扩容之前, 先做一下备份! 避免产生人品问题而后悔莫及. 毕竟备份虚拟机还是很省事的, 直接整个目录拷贝一份即可. . 扩大VirtualDisk . 我们先把VirtualBox里的这块硬盘扩大: . C: cd &quot;C: Program Files Oracle VirtualBox&quot; VBoxManage modifyhd &quot;D: VM mint-xfce mint-xfce.vdi&quot; --resize 1000000 . 正常的话, 系统会输出: . 0%...10%...20%...30%...40%...50%...60%...70%...80%...90%...100% 然后就退出了. . 我们可以去VirtualBox里看看硬盘大小: . . 已经变成1000G了(其实我不小心多打了1个0, 不过无所谓了). . 在Linux中创建新的分区 . 进入Linux, 切换为root用户, 运行fdisk -l: . chi-VirtualBox ~ # fdisk -l Disk /dev/sda: 1048.6 GB, 1048576000000 bytes 255 heads, 63 sectors/track, 127482 cylinders, total 2048000000 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk identifier: 0x00067091 Device Boot Start End Blocks Id System /dev/sda1 * 2048 499711 248832 83 Linux /dev/sda2 501758 83884031 41691137 5 Extended /dev/sda5 501760 83884031 41691136 8e Linux LVM Disk /dev/mapper/mint--vg-root: 40.5 GB, 40542142464 bytes 255 heads, 63 sectors/track, 4928 cylinders, total 79183872 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk identifier: 0x00000000 Disk /dev/mapper/mint--vg-root doesn&#39;t contain a valid partition table Disk /dev/mapper/mint--vg-swap_1: 2147 MB, 2147483648 bytes 255 heads, 63 sectors/track, 261 cylinders, total 4194304 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk identifier: 0x00000000 Disk /dev/mapper/mint--vg-swap_1 doesn&#39;t contain a valid partition table . 可以看到, 虽然硬盘的大小已经扩大了, 但是分区表还是没变. 所以现在我们去创建一个新的分区. 在我的Linux Mint里面有图形化界面可以创建, 所以我就不用fdisk了, 免得出错, 反正记得要 创建一个primary分区, 分区类型要选lvm(8e), 完成之后, 里面的分区应该是这样: . chi-VirtualBox ~ # fdisk -l Disk /dev/sda: 1048.6 GB, 1048576000000 bytes 255 heads, 63 sectors/track, 127482 cylinders, total 2048000000 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk identifier: 0x00067091 Device Boot Start End Blocks Id System /dev/sda1 * 2048 499711 248832 83 Linux /dev/sda2 501758 83884031 41691137 5 Extended /dev/sda3 83886080 2047999999 982056960 8e Linux LVM /dev/sda5 501760 83884031 41691136 8e Linux LVM Disk /dev/mapper/mint--vg-root: 40.5 GB, 40542142464 bytes 255 heads, 63 sectors/track, 4928 cylinders, total 79183872 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk identifier: 0x00000000 Disk /dev/mapper/mint--vg-root doesn&#39;t contain a valid partition table Disk /dev/mapper/mint--vg-swap_1: 2147 MB, 2147483648 bytes 255 heads, 63 sectors/track, 261 cylinders, total 4194304 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk identifier: 0x00000000 Disk /dev/mapper/mint--vg-swap_1 doesn&#39;t contain a valid partition table . 其中/dev/sda3是我们新创建的分区. . 创建和扩展lvm分区 . 扩大volume group . 由于使用了lvm, 我们可以方便的扩展已有分区. 首先查看已有的volume group: . chi-VirtualBox ~ # vgdisplay Volume group VG Name mint-vg System ID Format lvm2 Metadata Areas 1 Metadata Sequence No 3 VG Access read/write VG Status resizable MAX LV 0 Cur LV 2 Open LV 2 Max PV 0 Cur PV 1 Act PV 1 VG Size 39.76 GiB PE Size 4.00 MiB Total PE 10178 Alloc PE / Size 10178 / 39.76 GiB Free PE / Size 0 / 0 VG UUID aR8Gl4-toof-YxoL-Jo2L-X8it-wfEe-B1YxUF . 可以看到, 已有一个volume group, 我们可以把/dev/sda3加到这个volume group里面: . chi-VirtualBox ~ # vgextend mint-vg /dev/sda3 No physical volume label read from /dev/sda3 Writing physical volume data to disk &quot;/dev/sda3&quot; Physical volume &quot;/dev/sda3&quot; successfully created Volume group &quot;mint-vg&quot; successfully extended . 我们再看看volume group的情况: . chi-VirtualBox ~ # vgdisplay Volume group VG Name mint-vg System ID Format lvm2 Metadata Areas 2 Metadata Sequence No 4 VG Access read/write VG Status resizable MAX LV 0 Cur LV 2 Open LV 2 Max PV 0 Cur PV 2 Act PV 2 VG Size 976.32 GiB PE Size 4.00 MiB Total PE 249937 Alloc PE / Size 10178 / 39.76 GiB Free PE / Size 239759 / 936.56 GiB VG UUID aR8Gl4-toof-YxoL-Jo2L-X8it-wfEe-B1YxUF . 可以看到, 原来的volume group已经被我们扩大到了976.32GB. . 扩大 logical volume . 首先, 我们查看目前的logical Volume: . chi-VirtualBox ~ # lvdisplay Logical volume LV Path /dev/mint-vg/root LV Name root VG Name mint-vg LV UUID 2hNBQf-1zGs-3nZF-4feA-uR9W-WVDQ-32lLm4 LV Write Access read/write LV Creation host, time mint, 2013-07-23 09:26:24 +0800 LV Status available # open 1 LV Size 37.76 GiB Current LE 9666 Segments 1 Allocation inherit Read ahead sectors auto - currently set to 256 Block device 252:0 Logical volume LV Path /dev/mint-vg/swap_1 LV Name swap_1 VG Name mint-vg LV UUID fD6Mwy-IYEy-ZxXN-aPPW-WNn7-catg-o1bRsX LV Write Access read/write LV Creation host, time mint, 2013-07-23 09:26:24 +0800 LV Status available # open 2 LV Size 2.00 GiB Current LE 512 Segments 1 Allocation inherit Read ahead sectors auto - currently set to 256 Block device 252:1 . 可以看到, 系统中只有一个logical Volume, 它是/root的挂载点. 我们现在要做的就是将这个logical volume 扩大. 在logical volume扩大时, 会自动使用volume group中的空间. . chi-VirtualBox ~ # lvextend -L 100G /dev/mint-vg/root Extending logical volume root to 100.00 GiB Logical volume root successfully resized chi-VirtualBox ~ # lvdisplay Logical volume LV Path /dev/mint-vg/root LV Name root VG Name mint-vg LV UUID 2hNBQf-1zGs-3nZF-4feA-uR9W-WVDQ-32lLm4 LV Write Access read/write LV Creation host, time mint, 2013-07-23 09:26:24 +0800 LV Status available # open 1 LV Size 100.00 GiB Current LE 25600 Segments 2 Allocation inherit Read ahead sectors auto - currently set to 256 Block device 252:0 Logical volume LV Path /dev/mint-vg/swap_1 LV Name swap_1 VG Name mint-vg LV UUID fD6Mwy-IYEy-ZxXN-aPPW-WNn7-catg-o1bRsX LV Write Access read/write LV Creation host, time mint, 2013-07-23 09:26:24 +0800 LV Status available # open 2 LV Size 2.00 GiB Current LE 512 Segments 1 Allocation inherit Read ahead sectors auto - currently set to 256 Block device 252:1 . 可以看到, logical volume 已经成功的变大为了100GB. . 扩大ext4分区 . 最后一步, 我们需要把logical volume上的ext4分区也扩大, 这样才能真正的存下文件. . chi-VirtualBox ~ # resize2fs /dev/mint-vg/root resize2fs 1.42.5 (29-Jul-2012) Filesystem at /dev/mint-vg/root is mounted on /; on-line resizing required old_desc_blocks = 3, new_desc_blocks = 7 The filesystem on /dev/mint-vg/root is now 26214400 blocks long. chi-VirtualBox ~ # df -h Filesystem Size Used Avail Use% Mounted on /dev/mapper/mint--vg-root 99G 26G 69G 28% / none 4.0K 0 4.0K 0% /sys/fs/cgroup udev 985M 4.0K 985M 1% /dev tmpfs 201M 896K 200M 1% /run none 5.0M 0 5.0M 0% /run/lock none 1002M 72K 1002M 1% /run/shm none 100M 16K 100M 1% /run/user /dev/sda1 228M 75M 141M 35% /boot . 可以看到/下面的空间已经成功扩大了. . 关于lvm的一些知识… . 相信看了上面的步骤你已经和我一样晕了…下面我来试图解释一下lvm中的各种术语 . 首先是volume group, 这个你可以想成是扩展分区, 不同的是volume group可以跨多块硬盘多个设备(这是lvm的牛逼之处), 用vgdisplay可以查看. | 然后是physical volume, 这个就是传统意义上的分区, 例如/dev/sda5, 用pvdisplay可以查看 | 然后是logical volume, 这个就是lvm中的逻辑分区, 地位相当于逻辑分区. 你硬盘上的目录, 就是挂载在logical volume上的. 用lvdisplay可以查看. | . lvm有什么好处? . 简单的讲, 就是你可以分出多个物理上的逻辑分区(例如 /dev/sda1 - /dev/sda5), 然后把它们放到同一个volume group中, 再从volume group中划分出不同的logical volume, 用于不同目录的挂载. 这样, 相当于在物理上的逻辑分区之上又抽象出了新的逻辑分区, 新的逻辑分区可以任意变化大小, 甚至可以横跨不同的硬盘, 非常灵活. . 一句话总结一下: 用fdisk创建physical volume, physical volume 组成 volume group, volume group划分为若干logical volume, 目录挂载在logical volume之上. .",
            "url": "https://blog.codescv.com/resizing-disk-size-in-virtualbox.html",
            "relUrl": "/resizing-disk-size-in-virtualbox.html",
            "date": " • Aug 20, 2013"
        }
        
    
  
    
        ,"post38": {
            "title": "关于python decorators",
            "content": "高阶函数 . 在python里面decorator看起来是个挺高端大气上档次的词, 不过这个东西在函数式语言里面其实很稀松平常, 用一句话概括起来就是高阶函数的语法糖. 什么叫高阶函数(high order procedures)呢? 就是返回值或者参数是函数的函数. 比如我们现在有个函数hello_world: . def hello_word(): print &quot;hello world!&quot; . 这个函数的个功能是打印一行字”hello world”. 现在我们想写一个函数, 它的功能是把hello_word这行字打印10遍, 怎么做呢? 简单: . def hello_world_10(): for i in range(10): hello_world() . 现在我们站的更高一点, 希望写一个更通用的函数, 可以将任意函数执行任意10次, 要怎么办呢? . def ten_times(func): def new_fun(): for i in range(10): func() return new_fun hello_world_10 = ten_times(hello_world) hello_world_10() . 可以看到, ten_times这个函数比我们一般的函数更”高级”一些, 它接受的参数是一个函数, 把这个函数进行一些加工(执行10次)后, 返回一个新的函数. 这个函数就是高阶函数. . decorator . 理解了高阶函数后, 我们就可以看看decorator了. 我们把前一个例子改为: . @ten_times def hello_world_10(): print &quot;hello world!&quot; . 这样, 新的hello_world_10和原来的版本完全等价. 所以说, @ten_times其实是以下代码的语法糖: . def hello_world_10(): print &quot;hello world!&quot; hello_world_10 = ten_times(hello_world_10) . 原函数带参数 . 假设我们有如下add函数: . def add(a, b): return a+b . 这一次, 我们的ten_times函数希望把原函数的_返回值_乘以10, 怎么做呢? . def ten_times(func): def new_func(*args, **kwargs): return 10 * func(*args, **kwargs) return new_func @ten_times def add_10(a, b): return a+b . 由此可见, 我们在decorator的定义中, 将新函数的输入传递给原函数即可. . decorator带参数 . 这次我们想把ten_times写的更加general一些, 我们希望它可以接受一个参数times, 可以将任意函数执行的结果乘以任意次. 首先我们用高阶函数的方式: . def times(time): def times_f(func): def new_func(*args, **kwargs): return time * func(*args, **kwargs) return new_func return times_f def add(a, b): return a+b ten_times = times(10) add_10 = ten_times(add) . 现在有点复杂了, 对吧? times首先接受一个参数time, 返回一个函数times_f, 这个times_f函数接受一个函数func作为参数, 然后又返回一个新的函数new_func. 换句话说, 这个函数返回了一个函数, 后者又返回一个函数. 注意到new_func引用了times_f中的局部变量func, 这样的函数被称为闭包(closure). 用decorator的语法糖要怎么写呢? . @times(10) def add_10(a, b): return a+b . 从之前的经验我们可以看到, ten_times这个函数需要返回一个函数, 所以这里我们知道times(10)也是一个函数,它的返回值是一个函数, 由此可知times是一个返回(返回函数的函数)的函数, 也印证了前面的分析. . @times(10) def add_10(a, b): return a+b . 一点小问题 . 我们还是回到比较简单的ten_times这个函数: . @ten_times def add(a, b): return a+b &gt;&gt;&gt; add.__name__ &#39;new_func&#39; . 我们看到, add的__name__属性变成了new_func! 仔细想想, 这很正常, 因为ten_times返回的函数就是new_func呀! 我们先暴力fix一下: . def ten_times(func): def new_func(*args, **kwargs): return 10 * func(*args, **kwargs) new_func.__name__ = func.__name__ return new_func @ten_times def add_10(a,b): return a+b &gt;&gt;&gt; add_10 &lt;function add_10 at 0x10da62140&gt; . 的确, 这个问题解决了. 不过, 还有__doc__等等属性呢? 每次声明decorator的时候都要这么搞一下, 不蛋疼么? 是的, 我们可以把这些fix的代码放到一起: . def fix_props(func): def wrapper(f): def new_f(*args, **kwargs): return f(*args, **kwargs) new_f.__name__ = func.__name__ new_f.__doc__ = func.__doc__ return new_f return wrapper def ten_times(func): @fix_props(func) def new_func(*args, **kwargs): return 10 * func(*args, **kwargs) return new_func . 这样, 就帮我们完成了这些琐碎的小问题. fix_props这个decorator可以将原函数的属性复制到新的wrapper函数, 使得原函数的docstring, name等不会丢失. 万一python升级了, 函数又有新的属性了, 肿么办?幸运的是, python里面有专门的库函数可以解决这个问题: . from functools import wraps def ten_times(func): @wraps(func) def new_func(*args, **kwargs): return 10 * func(*args, **kwargs) return new_func . 这个wraps的原理就和我们上面的fix_props类似. . 应用 . decorator在python里有挺多应用的, 最典型的是@classmethod和@property. 例如: . class A(object): def __init__(self): self.__x = 0 @property def x(self): return self.__x @x.setter def x(self, xval): self.__x = xval @classmethod def my_name(cls): return &quot;A&quot; &gt;&gt;&gt; A.my_name() &#39;A&#39; &gt;&gt;&gt; a = A() &gt;&gt;&gt; a.x = 1 &gt;&gt;&gt; a.x 1 .",
            "url": "https://blog.codescv.com/python/python-decorators.html",
            "relUrl": "/python/python-decorators.html",
            "date": " • Aug 11, 2013"
        }
        
    
  
    
        ,"post39": {
            "title": "使用Key Indexing来避免初始化",
            "content": "在c/c++中, 使用new或者malloc来分配int数组是很快的, 因为没有初始化过程. 但是在使用时会有问题: 如果 不初始化, 如何知道哪一项的数据是有效的? 如果初始化, 那么效率又会比较低下. . 假设有一个比较大数组a[0..n-1], 数组里每项是一个int变量, 但只有k项被使用(k « n). 如何设计一个数据 结构, 避免初始化a中所有的元素, 又能正确的访问数组中的每一项? 假设内存足够. . 可以使用两个长度为n的辅助数组from和to. 例如: . 0 1 2 3 4 5 6 7 8 9 a = [?|1|?|?|2|?|?|9|?|?|] from = [?|0|?|?|2|?|?|1|?|?|] to = [1|7|4|?|?|?|?|?|?|?|] top . from数组表示的是, a[i]被初始化时, a数组中元素的个数, 也就是说, a[i]是第from[i]个被初始化的. . to数组表示的是, 第i个被初始化的元素, 它在a中的下标是to[i]. . top表示的是to数组中有效元素的个数, 指向to数组中所有有效元素的下一项, 它的值等于目前a数组中有效元素的个数. . 图中为先后设置a[1]=1; a[7]=9; a[4]=2;后, a, from, to中的结果. . 在设置一个元素时, 使用如下方法: . a[i] = number; if (!is_initialized(i)) { from[i] = top; to[top] = i; top++; } . a[1] = 1后, from[1] = 0, to[0] = 1, top = 1; | a[7] = 9后, from[7] = 1, to[1] = 7, top = 2; | a[4] = 2后, from[4] = 2, to[2] = 4, top = 3. | . 如何判断一个a[i]是否被初始化? 注意到如果a[i]被初始化了, 一定有to[from[i]] == i. 但是光有这个还不够, 因为from[i]可能指向to中某一个没初始化的位置, 使得to[from[i]] == i碰巧成立. 所以, 还得加上一个条件, 就是from[i] &lt; top. 这样, 如果from[i]不在0..top-1之内, 那么就一定没被初始化. 如果from[i]在0..top之内, 有两种情况, 一种是i在1,4,7之内, 这样一定有to[from[i]] == i, 那么a[i]已经被初始化过了; 一种是i不在1,4,7之内, 那么一定有to[from[i]] != i. 因此要判断a[i]是否被初始化, 可以用from[i] &lt; top &amp;&amp; to[from[i]] == top. . 这个技巧被称为key indexing, 在Programming Pearls里有提到过. . 换个角度想想, key indexing其实是hashing的一个特例! 它的hash function就返回原本的key. 这里的i就是key, from数组其实是hash function, 而to数组告诉我们元素i是否被初始化过. 于是from, to这两个数组可以用一个hash table来代替: . a[i] = number; init_hash[i] = true; if (init_hash[i]) { // 第i个元素被初始化过了 ... } . 因此, 如果需要更省空间话, 可以用hashing来优化. .",
            "url": "https://blog.codescv.com/algorithms/key-indexing.html",
            "relUrl": "/algorithms/key-indexing.html",
            "date": " • Aug 4, 2013"
        }
        
    
  
    
        ,"post40": {
            "title": "使用virtualenv创建隔离的python环境",
            "content": "python各种版本之间的不兼容性着实让人头疼, 工作中使用python常常需要一个团队中统一python以及各种库的版本. 在ruby中有一个好用的工具rvm可以在用户的家目录下安装一个(或多个)本地的ruby, 然后各个工程可以使用自己的ruby和gem的版本.Python里面有没有类似的工具呢? 有的, 这就是virtualenv. . 本地安装python . 虽然virtualenv可以直接从系统本身的python中使用, 但是这偏离了我们的初衷: 使用完全隔离的, 和系统默认的python无关的环境. 为什么要这样呢? 因为系统中有很多程序依赖于系统的python, 如果我们替代掉系统的python, 会导致一些程序出错. 例如在centos上, 如果把系统默认的python给换掉, ibus和yum就很有可能起不来了. 如果使用系统的python版本, 又可能让我们开发的应用程序跑不起来, 比较头疼. 所以最好的解决方案还是本地安装一个和系统无关的python. . 如果在ubuntu上编译安装python, 建议先安装几个依赖包, 否则编译出来的python会缺少一些功能, 例如没有readline的话, 命令行就不能补全了, 这很蛋疼, 必须预防: . apt-get install build-essential libncursesw5-dev libreadline5-dev libssl-dev libgdbm-dev libc6-dev libsqlite3-dev tk-dev libgdbm-dev libsqlite3-dev libbz2-dev . 安装python可以直接从源代码去安装: . mkdir ~/src wget http://www.python.org/ftp/python/2.7.2/Python-2.7.2.tgz tar -zxvf Python-2.7.2.tar.gz cd Python-2.7.2 mkdir ~/.localpython ./configure --prefix=/home/&lt;user&gt;/.localpython make make install . 把2.7.2换成你需要的python版本. 另外, 执行完configure之后不要急着make, 先确认一下输出中有没有提示你缺少某些模块. 如果有的话, 把这些依赖装上吧, 免得以后麻烦. . 安装virtualenv . cd ~/src wget http://pypi.python.org/packages/source/v/virtualenv/virtualenv-1.5.2.tar.gz#md5=fbcefbd8520bb64bc24a560c6019a73c tar -zxvf virtualenv-1.5.2.tar.gz cd virtualenv-1.5.2/ ~/.localpython/bin/python setup.py install . 注意把1.5.2换成最新的virtualenv版本. 装好以后, 就可以从~/.localpython目录执行virtualenv了. . virtualenv的基本使用 . 使用virtualenv命令可以创建一个隔离环境. 后面可以带参数, 例如: . virtualenv &lt;envname&gt; --prompt=test . 其中--prompt后的参数表示在使用该环境后的提示符, 也就是在该环境被激活以后, shell提示符前面会多出来的一串, 用来提示你当前用的是哪个虚拟环境. 执行之后, 会在&lt;envname&gt;目录下创建一个虚拟环境, 把需要的脚本文件都拷贝到这个目录下. 在激活这个环境之后, 各种python包就会安装到这个目录下. . 要激活一个虚拟环境, 使用 source env_dir/bin/activate. 这个脚本会帮你设置一些环境变量, 把python映射到这个目录下. 运行之后会看到shell提示符的变化. . 要退出一个虚拟环境, 使用 deactivate命令. 仔细观察一下, 这是个在activate脚本中定义的函数, 会帮你还原各种相关环境变量. . 自动安装项目依赖 . 有了virtualenv之后, 使用pip安装各种包就会安装到当前的虚拟环境下, 从而跟系统的python无关(跟自己在家目录下本地安装的python也无关!). 因此, 可以在不同的虚拟环境下装不同的包, 而不用担心冲突. 不过还有一点不方便的是, python里面没有类似于bundler这种东西, 不能比较方便的自动安装依赖. 不过好在virtualenv可以自定义bootstrap脚本, 方便用户在创建环境时自动安装依赖. 例如: . import virtualenv, textwrap output = virtualenv.create_bootstrap_script(textwrap.dedent(&quot;&quot;&quot; import os, subprocess packages = [ &quot;MySQL-python==1.2.4&quot;, &quot;pycrypto==2.6&quot;, &quot;pyflakes==0.6.1&quot;, &quot;pymongo==2.4.1&quot;, &quot;python-memcached==1.48&quot;, &quot;redis==2.7.1&quot;, &quot;simplejson==3.1.3&quot;, &quot;SQLAlchemy==0.7.4&quot;, &quot;thrift==0.9.0&quot;, &quot;tornado==2.4.1&quot;, &quot;xpinyin==0.4.5&quot;, ] def after_install(options, home_dir): for p in packages: while True: ret = subprocess.call([join(home_dir, &#39;bin&#39;, &#39;pip&#39;), &#39;install&#39;, p]) if ret == 0: break else: print &quot;ret = &quot;, ret print &quot;Try again: &quot; + p &quot;&quot;&quot;)) f = open(&#39;create_env.py&#39;, &#39;w&#39;).write(output) . 在after_install函数中, 我们使用pip安装指定的包(注意这里有个重试的功能, 天朝的网络你懂的), 也可以做任何其他的操作, 十分灵活(当然, 也十分土). 把以上代码保存为boot.py, 运行一下会产生一个create_env.py的脚本, 然后使用这个脚本去代替virtualenv来创建环境, 就可以在环境建好之后自动装好这些依赖. .",
            "url": "https://blog.codescv.com/python/using-virtualenv.html",
            "relUrl": "/python/using-virtualenv.html",
            "date": " • Jul 28, 2013"
        }
        
    
  
    
        ,"post41": {
            "title": "二级指针在链表操作中的应用",
            "content": "故事起源于Linus大神在回答水友提问的时候有这么一段: http://meta.slashdot.org/story/12/10/11/0030249/linus-torvalds-answers-your-questions . At the opposite end of the spectrum, I actually wish more people understood the really core low-level kind of coding. Not big, complex stuff like the lockless name lookup, but simply good use of pointers-to-pointers etc. For example, I’ve seen too many people who delete a singly-linked list entry by keeping track of the “prev” entry, and then to delete the entry, doing something like . if (prev) prev-&gt;next = entry-&gt;next; else list_head = entry-&gt;next; . and whenever I see code like that, I just go “This person doesn’t understand pointers”. And it’s sadly quite common. . People who understand pointers just use a “pointer to the entry pointer”, and initialize that with the address of the list_head. And then as they traverse the list, they can remove the entry without using any conditionals, by just doing a “*pp = entry-&gt;next”. . 大意是说，当从一个单向链表中删除一个元素时，很多人会使用一个prev指针来记录被删除的那个节点的前一个元素的位置，然后使用prev-&gt;next = entry-&gt;next的方式去删除。这样的人简直弱爆了，根本就不懂得指针。 . 可能这样说还不是很清楚，让我们来看个比较完整的例子吧： . Node* find_and_delete(Node *head, int target) { Node *prev = NULL; Node *entry = head; while (entry != NULL) { if (entry-&gt;val == target) { if (prev) { prev-&gt;next = entry-&gt;next; } else { head = entry-&gt;next; } break; } prev = entry; entry = entry-&gt;next; } return head; } . 函数find_and_delete完成了这么一个功能：遍历一个单向链表，如果找到和给出的target相等的值，就从链表中删除这个节点。上述方法是一般c语言教科书（例如谭浩强之类）里的标准做法。但是这种方法遭到了Linus大神的强烈鄙视和唾弃，认为这是不懂指针的人的做法。Linus提到了一个使用二级指针的方法*pp = entry-&gt;next可以让我们不需要判断就可以删除，这是怎么做到的呢？ . void find_and_delete2(Node **head, int target) { while (*head != NULL) { Node *entry = *head; if (entry-&gt;val == target) { *head = entry-&gt;next; break; } head = &amp;(entry-&gt;next); } } . 在find_and_delete2函数中，巧妙的使用了一个二级指针，从而直接修改了当前entry指针的指向，代码精简了很多。不过这个代码并非很容易正确实现（大家可以自己试试，能不能一遍写正确）。(特别的，想想head = &amp;(entry-&gt;next)，和*head = entry-&gt;next有什么区别？ ) . 不得不说玩kernel的大神对指针的理解确实比我辈强的多，学一招还挺有用的。学了这招以后去leetcode上切了一题，也用了二级指针:-) . https://github.com/codescv/leetcode/blob/master/AddTwoNumbersNoNew.cpp . 最后，上个完整的测试代码例子，供懒得敲代码的人玩耍： . #include &lt;stdio.h&gt; #include &lt;stdlib.h&gt; #include &lt;string.h&gt; struct NodeStruct { int val; struct NodeStruct *next; }; typedef struct NodeStruct Node; Node* find_and_delete(Node *head, int target) { Node *prev = NULL; Node *entry = head; while (entry != NULL) { if (entry-&gt;val == target) { if (prev) { prev-&gt;next = entry-&gt;next; } else { head = entry-&gt;next; } break; } prev = entry; entry = entry-&gt;next; } return head; } void find_and_delete2(Node **head, int target) { while (*head != NULL) { Node *entry = *head; if (entry-&gt;val == target) { *head = entry-&gt;next; break; } head = &amp;(entry-&gt;next); } } Node *make_list(int a[], int n) { Node *result = NULL; Node *runner; int i; if (n == 0) { return NULL; } result = (Node *)malloc(sizeof(Node)); result-&gt;val = a[0]; runner = result; for (i = 1; i &lt; n; i++) { runner-&gt;next = (Node *)malloc(sizeof(Node)); runner-&gt;next-&gt;val = a[i]; runner = runner-&gt;next; } return result; } void print_list(Node *l) { while (l) { printf(&quot;%d&quot;, l-&gt;val); if (l-&gt;next) { printf(&quot;-&gt;&quot;); } else { printf(&quot; n&quot;); } l = l-&gt;next; } } int main(int argc, const char *argv[]) { int a[] = {1,2,3,4,5}; Node *l = make_list(a, 5); print_list(l); l = find_and_delete(l, 3); print_list(l); l = find_and_delete(l, 1); print_list(l); Node *l2 = make_list(a,5); print_list(l2); find_and_delete2(&amp;l2, 3); print_list(l2); find_and_delete2(&amp;l2, 1); print_list(l2); return 0; } .",
            "url": "https://blog.codescv.com/c/linus-linked-list.html",
            "relUrl": "/c/linus-linked-list.html",
            "date": " • Jul 20, 2013"
        }
        
    
  
    
        ,"post42": {
            "title": "The Architecture of Ejabberd",
            "content": "We have taken a quick tour through the socket infrastructure provided with ejabberd, and wrote a few listeners as an exercise. This time let’s take a look at the ejabberd as a whole and try to figure out what it is about. . Overview . . Like many servers, ejabberd can inherently be broken up into three layers: . The Data Layer. This layer handles how data get stored in databases, and ensures the integrity and constraints of data. | The Logic Layer. This layer is the most complicated of all. It handles all the XMPP logics, as well as many other features, e.g.: http bindings, access controls, extensible modules and hooks, etc. | The Interface Layer. This is the layer we have just talked about. It handles all the incoming data from and the outgoing data to the client. | . The Data Layer . Ejabberd primarily use mnesia as its “database”. Mnesia is in fact a high-performance key-value pair storage system built into the erlang library. It provides many features such as: . Replication. Tables may be replicated at several nodes. | Atomic transactions. A series of table manipulation operations can be grouped into a single atomic transaction. | Location transparency. Programs can be written without knowledge of the actual location of data. | Extremely fast real time data searches. | . Some of its features, such as replication and realtime searches, make it eligible for an extremely extensible database system. However, ejabberd does not enforce its use; it provides ODBC interfaces to allow utilizing of other databases whenever possible. For example, sometimes it might be more convienient to allow the user to authenticate using data stored in an existing database server, or there may be some relational data that need to be used for some purpose. Generally, mnesia is suitable for real fast key-value searches, but performs pretty badly when you use it for “relational” lookups (using the mnesia:select the wrong way). Avoid relational data whenever possible (use key lookup) if you want to make good use of it. . The Logic Layer . The logic layer is the main and most import part of the ejabberd system. Some of its functionalities include: . Jabber Logics. The client to server connection (typically on tcp/5222) is handled by the module ejabberd_c2s. The server to server connection (typically on tcp/5269) is handled by the modules ejabberd_s2s, ejabberd_s2s_in, ejabber_s2s_out. The HTTP bindings are handled by the ejabberd_http module. | Router. The router handles the routing of most of the messages, i.e., when a Jabber client sends a to another entity, how is the message routed to the correct destination? First ejabberd determines whether the message is a local or a remote one. It does so by looking at the &quot;to&quot; attribute to see if the host implied by the &quot;to&quot; attribute is hosted in itself. If so, the message is local; and is handled by ejabberd_local; otherwise it is treated as an s2s message. | Modules. In additional to the core Jabber and Router logics, there is a large part of the ejabberd which can be plugged in only when necessary, and they are called modules. Modules can be started / stopped dynamically at any time, thus making the ejabberd server highly extensible even at runtime. Modules are widely used for various extensions (the so-called &#39;XEP&#39;s). | Hooks. You want to change the core logics in the ejabberd server? No problem. Hooks are everywhere to help you. A hook is a way by which you can change the behaviour of ejabberd by injecting your new code into the system, without changing any existing code. For example, if you want to roll up your message filter to filter out messages you don’t want, you can add a module hooking to the “filter_packet/3” hook. If you want to keep track of all the messages clients sent, you can write a function hooking to ‘user_send_packet/3’. | Access Control. All users (including real jabber client users as well as administrators) are stored the same way in ejabberd. Their privileges are determined by the groups they belong to. Hence, the access control modules in ejabberd allow us to distinguish users with their groups, thus providing different services for different users. | Utils and Libraries. Some other libraries and utils exist in ejabberd for common purposes, e.g.: XML processing, SASL authentication, Encodings, Logger, etc. It is worth noting the ejabberd_logger is a very good logger module perfectly usable by any other projects. | . The Interface Layer . This layer handles incoming data from and outgoing data from the client. Its main functionaly is to listen on a local port waiting for client connections, establish a connection any clients or servers when necessary, and exchange data. It supports TCP/TLS connections as well as UDP transport, as is described in our last discussions. It serves as the interface between the outer world and the ejabberd server. .",
            "url": "https://blog.codescv.com/erlang/ejabberd/the-architecture-of-ejabberd.html",
            "relUrl": "/erlang/ejabberd/the-architecture-of-ejabberd.html",
            "date": " • Jun 6, 2012"
        }
        
    
  
    
        ,"post43": {
            "title": "More on Echo Service: Using TLS and XML Streams",
            "content": "Using TLS . OK. We have written a simple echo service, serving both on tcp and udp. Now we want our echo service to be working for TLS, too. . The tls module fits well into implementing TLS based secure connections, for it has the following advantages over the defaut erlang ssl module: . It is implemented using C and has better performance. | It supports starttls i.e. start tls over the tcp connection depending on situations, without having to re-establish a new connection. | To enable tls connections, use tls:tcp_to_tls/2 to transform a tcp socket to a tls one: . %% file: echo_service.erl init([{SockMod, CSock}, Opts]) -&gt; ?ERROR_MSG(&quot;start with sockmod: ~p csock: ~p opts: ~p&quot;, [SockMod, CSock, Opts]), State = #state{sockmod=SockMod, csock=CSock, opts=Opts}, NewState = set_opts(State), {ok, state_name, NewState}. handle_info({ _, _, Packet}, StateName, #state{sockmod=SockMod, csock=CSock}=State) -&gt; case SockMod of tls -&gt; case tls:recv_data(CSock, Packet) of { _, &lt;&lt;&gt;&gt;} -&gt; ok; { _, Data} -&gt; SockMod:send(CSock, Data) end; _ -&gt; SockMod:send(CSock, Packet) end, activate_socket(State), {next_state, StateName, State}; activate_socket(#state{csock={tlssock, _, _}=TLSSock}) -&gt; tls:setopts(TLSSock, [{active, once}]); activate_socket(#state{csock=CSock}) -&gt; inet:setopts(CSock, [{active, once}]). set_opts(#state{csock=CSock, opts=Opts} = State) -&gt; TLSEnabled = lists:member(tls, Opts), if TLSEnabled -&gt; TLSOpts = lists:filter(fun({certfile, _ }) -&gt; true; ( _ ) -&gt; false end, [verify_none | Opts]), {ok, TLSSock} = tls:tcp_to_tls(CSock, TLSOpts), NewState = State#state{sockmod=tls, csock=TLSSock}, activate_socket(NewState), NewState; true -&gt; Opts1 = lists:filter(fun(inet) -&gt; false; (tls) -&gt; false; ({ ip, _ }) -&gt; false; ( _ ) -&gt; true end, Opts), inet:setopts(CSock, Opts1), activate_socket(State), State end. . in set_opts/1, we use tls:tcp_to_tls/2 to transform the accepted tcp socket in to a tls socket, then we use tls:recv_data/2 to receive all the tls data. tls:tls_recv_data will automatically do the handshakes needed, returning data if presents (handshake data excluded). Finally, we use tls:send/2 to send any data back to the client. . Note: complete code listing available from https://github.com/codescv/ejabberd on branch echo_service. . Using xml_stream . Now let’s take our echo service up to the next level: what about receiving xml streams as input , and echoing xml stanzas? . That’s also very simple. . %% file: echo_service.erl socket_type() -&gt; xml_stream. init([{SockMod, CSock}, Opts]) -&gt; ?ERROR_MSG(&quot;start with sockmod: ~p csock: ~p opts: ~p&quot;, [SockMod, CSock, Opts]), State = #state{sockmod=SockMod, csock=CSock, opts=Opts}, NewState = set_opts(State), {ok, process, NewState}. process({xmlstreamelement,El}, #state{sockmod=SockMod, csock=CSock} = State) -&gt; ?ERROR_MSG(&quot;element: ~p ~p ~p&quot;, [SockMod, CSock, El]), SockMod:send(CSock, xml:element_to_binary(El)), {next_state, process, State}; process(Event, State) -&gt; ?ERROR_MSG(&quot;event ~p ~p&quot;, [Event, State]), {next_state, process, State}. activate_socket(#state{sockmod=ejabberd_socket}) -&gt; ok; activate_socket(#state{sockmod=tls, csock=TLSSock}) -&gt; tls:setopts(TLSSock, [{active, once}]); activate_socket(#state{sockmod=gen_tcp, csock=CSock}) -&gt; inet:setopts(CSock, [{active, once}]). set_opts(#state{sockmod=ejabberd_socket}=State) -&gt; State; set_opts(#state{csock=CSock, opts=Opts} = State) -&gt; TLSEnabled = lists:member(tls, Opts), if TLSEnabled -&gt; TLSOpts = lists:filter(fun({certfile, _ }) -&gt; true; (_) -&gt; false end, [verify_none | Opts]), {ok, TLSSock} = tls:tcp_to_tls(CSock, TLSOpts), NewState = State#state{sockmod=tls, csock=TLSSock}, activate_socket(NewState), NewState; true -&gt; Opts1 = lists:filter(fun(inet) -&gt; false; (tls) -&gt; false; ({ip, _ }) -&gt; false; ( _ ) -&gt; true end, Opts), inet:setopts(CSock, Opts1), activate_socket(State), State end. . We first change the socket_type() to return xml_stream, which tells ejabberd to use ejabberd_receiver as our receiver. Then we override the fsm state call back process/2 to process any incoming xml stanzas. Note that we do no-op for activate_socket and set_opts, for any incoming data are automatically taken care of by the ejabberd_receiver module. . To test it, let’s run nc to connect to the 5555 port: . nc localhost 5555 &lt;?xml version=&#39;1.0&#39;?&gt;&lt;stream:stream to=&quot;localhost&quot; xmlns=&quot;jabber:client&quot; xmlns:stream=&quot;http://etherx.jabber.org/streams&quot; version=&quot;1.0&quot;&gt; &lt;body&gt;hello&lt;/body&gt; . If everything goes well, the server answers with reply: . &lt;body&gt;hello&lt;/body&gt; . Note: complete code listing available at: https://github.com/codescv/ejabberd on branch xml_stream_echo_service. . To sum up . We have modified our echo service module to accept tls connections as well as xml_stream stanzas. Next time we’ll be talking about something else, but also fun! .",
            "url": "https://blog.codescv.com/erlang/ejabberd/more-on-echo-service-using-tls-and-xml-streams.html",
            "relUrl": "/erlang/ejabberd/more-on-echo-service-using-tls-and-xml-streams.html",
            "date": " • May 29, 2012"
        }
        
    
  
    
        ,"post44": {
            "title": "Writing a Simple Echo Service Module",
            "content": "In this blog, I will continue my discussion on ejabberd’s socket infrastructure. For the sake of simplicity, let’s write a simple echo service module, which receives any packet from the client, and echo the packet back. . Register our listener . First, we must register our service in the ejabberd’s config file: . %% file: ejabberd.cfg {listen, [ %% ... {5555, echo_service, []}, %% … ]} . Quick and dirty echo service . Our echo service will be listening at 5555/tcp. Now let’s write a handler for this port, we start out with a gen_fsm skeleton: . %%%- %%% @author Chi Zhang &lt;elecpaoao@gmail.com&gt; %%% @copyright (C) 2012, Chi Zhang %%% @doc %%% echo service demo %%% @end %%% Created : 24 May 2012 by Chi Zhang &lt;elecpaoao@gmail.com&gt; %%% file: echo_service.erl %%%- -module(echo_service). -behaviour(gen_fsm). %% API -export([start_link/2]). -export([start/2, socket_type/0]). %% gen_fsm callbacks -export([init/1, state_name/2, state_name/3, handle_event/3, handle_sync_event/4, handle_info/3, terminate/3, code_change/4]). -define(SERVER, ?MODULE). -include(&quot;ejabberd.hrl&quot;). -record(state, {sockmod, csock, opts}). %%%=================================================================== %%% API %%%=================================================================== start(SockData, Opts) -&gt; start_link(SockData, Opts). socket_type() -&gt; raw. start_link(SockData, Opts) -&gt; gen_fsm:start_link(?MODULE, [SockData, Opts], []). %%%=================================================================== %%% gen_fsm %%%=================================================================== init([{SockMod, CSock}, Opts]) -&gt; ?ERROR_MSG(&quot;start with sockmod: ~p csock: ~p opts: ~p&quot;, [SockMod, CSock, Opts]), State = #state{sockmod=SockMod, csock=CSock, opts=Opts}, activate_socket(State), {ok, state_name, State}. state_name(_Event, State) -&gt; {next_state, state_name, State}. state_name(_Event, _From, State) -&gt; Reply = ok, {reply, Reply, state_name, State}. handle_event(_Event, StateName, State) -&gt; {next_state, StateName, State}. handle_sync_event(_Event, _From, StateName, State) -&gt; Reply = ok, {reply, Reply, StateName, State}. handle_info({_, CSock, Packet}, StateName, #state{sockmod=SockMod}=State) -&gt; ?ERROR_MSG(&quot;received: ~p&quot;, [Packet]), SockMod:send(CSock, Packet), activate_socket(State), {next_state, StateName, State}; handle_info({tcp_closed, _CSock}, _StateName, State) -&gt; ?ERROR_MSG(&quot;client closed: ~p&quot;, [State]), {stop, normal, State}; handle_info(_Info, StateName, State) -&gt; ?ERROR_MSG(&quot;received: ~p&quot;, [_Info]), {next_state, StateName, State}. terminate(_Reason, _StateName, _State) -&gt; ?ERROR_MSG(&quot;terminated ~p&quot;, [_Reason]), ok. code_change(_OldVsn, StateName, State, _Extra) -&gt; {ok, StateName, State}. %%%=================================================================== %%% Internal functions %%%=================================================================== activate_socket(#state{csock=CSock}) -&gt; inet:setopts(CSock, [{active, once}]). . Pretty simple, right? With only very few lines of the default gen_fsm code changed, we have a fully working echo service! . The first thing you may have noticed is the start/2 and socket_type/0 call. Why is this neccessary? Recall from ejabberd_socket: . %% file: ejabberd_socket.erl start(Module, SockMod, Socket, Opts) -&gt; case Module:socket_type() of xml_stream -&gt; MaxStanzaSize = case lists:keysearch(max_stanza_size, 1, Opts) of {value, {_, Size}} -&gt; Size; _ -&gt; infinity end, {ReceiverMod, Receiver, RecRef} = case catch SockMod:custom_receiver(Socket) of {receiver, RecMod, RecPid} -&gt; {RecMod, RecPid, RecMod}; _ -&gt; RecPid = ejabberd_receiver:start( Socket, SockMod, none, MaxStanzaSize), {ejabberd_receiver, RecPid, RecPid} end, SocketData = #socket_state{sockmod = SockMod, socket = Socket, receiver = RecRef}, case Module:start({?MODULE, SocketData}, Opts) of {ok, Pid} -&gt; case SockMod:controlling_process(Socket, Receiver) of ok -&gt; ok; {error, _Reason} -&gt; SockMod:close(Socket) end, ReceiverMod:become_controller(Receiver, Pid); {error, _Reason} -&gt; SockMod:close(Socket), case ReceiverMod of ejabberd_receiver -&gt; ReceiverMod:close(Receiver); _ -&gt; ok end end; independent -&gt; ok; raw -&gt; case Module:start({SockMod, Socket}, Opts) of {ok, Pid} -&gt; case SockMod:controlling_process(Socket, Pid) of ok -&gt; ok; {error, _Reason} -&gt; SockMod:close(Socket) end; {error, _Reason} -&gt; SockMod:close(Socket) end end. . As is shown above, if the Module:socket_type() returns the atom ‘raw’, then the module will be used without ejabberd_receiver, which is kinda what we want, because we want to take control over everything. After ejabberd_socket calls Module:start/2, passing the socket module(gen_tcp here) and the socket, it will call SockMod:controlling_process to direct any messages with the socket to the Pid returned by Module:start/2, which is, in our case, the echo_service gen_fsm process. . When the echo_service fsm starts, the socket is in passive mode, that is, it won’t get any data until the recv() function is called. As a result, we set the socket to be active once, and the data can be received. Everytime we receive a new packet, we generate and send response, and then set the socket to be active once again. . Finally, don’t forget to turn off the FSM when {tcp_closed, Sock} is received. That prevents you from leaking processes. . That should sound reasonable enough. But wait, what if we want our echo service to be working on UDP also? . Adding UDP Transport . Simple. Let’s see how UDP sockets in ejabberd are handled: . %% file: ejabberd_listener.erl init_udp(PortIP, Module, Opts, SockOpts, Port, IPS) -&gt; case gen_udp:open(Port, [binary, {active, false}, {reuseaddr, true} | SockOpts]) of {ok, Socket} -&gt; %% Inform my parent that this port was opened succesfully proc_lib:init_ack({ok, self()}), udp_recv(Socket, Module, Opts); {error, Reason} -&gt; socket_error(Reason, PortIP, Module, SockOpts, Port, IPS) end. udp_recv(Socket, Module, Opts) -&gt; case gen_udp:recv(Socket, 0) of {ok, {Addr, Port, Packet}} -&gt; case catch Module:udp_recv(Socket, Addr, Port, Packet, Opts) of {&#39;EXIT&#39;, Reason} -&gt; ?ERROR_MSG(&quot;failed to process UDP packet:~n&quot; &quot;** Source: {~p, ~p}~n&quot; &quot;** Reason: ~p~n** Packet: ~p&quot;, [Addr, Port, Reason, Packet]); _ -&gt; ok end, udp_recv(Socket, Module, Opts); {error, Reason} -&gt; ?ERROR_MSG(&quot;unexpected UDP error: ~s&quot;, [format_error(Reason)]), throw({error, Reason}) end. . That’s pretty neat. Instead of spawning a new process for every new connection like the tcp, udp sockets are handled by only one process for each port. . To use UDP transport, we simply add udp_recv to our Module: . %% file: echo_service.erl udp_recv(Socket, Addr, Port, Packet, Opts) -&gt; ?ERROR_MSG(&quot;udp receive: socket ~p addr ~p port ~p packet ~p opts ~p&quot;, [Socket, Addr, Port, Packet, Opts]), gen_udp:send(Socket, Addr, Port, Packet). . That’s enough for most purposes, but you must be careful: if the Module:udp_recv/5 call blocks, it blocks any other data to be handled. Hence, in real life applications, get ready to spawn multiple processes to handle the UDP requests! . Using customized socket options . The ejabberd_listener’s listen options fits our needs in most cases. What if we want customized socket options, other than what the default options, say, we want {packet, 4} to be set before we receive any data from the socket? . That’s easy. First add the options in the config file: . %% file: ejabberd.cfg {listen, [ %% ... {5556, echo_service, [{packet, 4}]}, %% … ]} . Then we add a setopts step in our echo service module: . %% file: echo_service.erl init([{SockMod, CSock}, Opts]) -&gt; ?ERROR_MSG(&quot;start with sockmod: ~p csock: ~p opts: ~p&quot;, [SockMod, CSock, Opts]), State = #state{sockmod=SockMod, csock=CSock, opts=Opts}, set_opts(State), activate_socket(State), {ok, state_name, State}. set_opts(#state{csock=CSock, opts=Opts}) -&gt; Opts1 = lists:filter(fun(inet) -&gt; false; ({packet, _}) -&gt; true; ( _ ) -&gt; false end, Opts), inet:setopts(CSock, Opts1). . We have added a filter to filter the options provided, allowing only valid options to be set. Now the echo service listening on 5556/tcp will require a 4-octet header stating the whole packet length, cool isn’t it? . To sum up . We have written a very simple echo service to learn how to use the ejabberd’s socket infrastructure. To write a simple TCP service, we only need to implement the socket_type() to return raw, and spawn a process handling the socket in Mod:start/2. To write a simple UDP service, we only need to provide a udp_recv/5 callback. Things we haven’t covered yet: . What about the TLS transport ? (hint:use the tls module included in ejabberd.) | How to separte socket receiving data and socket handling logic? (hint: start and return your own receiver in your Mod:start). | How to use the builtin ejabberd_receiver and ejabberd_socket? (hint: return xml_stream for socket_type/0). | The above questions are left out as an exercise. Hack on! . Note: All the code in this blog can be accessed at: https://github.com/codescv/ejabberd on branch echo_service. .",
            "url": "https://blog.codescv.com/erlang/ejabberd/writing-a-simple-echo-service-module.html",
            "relUrl": "/erlang/ejabberd/writing-a-simple-echo-service-module.html",
            "date": " • May 25, 2012"
        }
        
    
  
    
        ,"post45": {
            "title": "The Ejabberd Socket Infrastructure",
            "content": "Welcome to my first blog about ejabberd source code hacking. In these series of blogs, I want to take notes about how the ejabberd works, and how to hack it to get customized features. . The source code version in discussion is ejabberd 2.1.10 release, which is the latest stable version at this time. . Intro . The first step to source hacking is code reading. Ejabberd is a big project (with 80k+lines of erlang code), so it’s impossible for us to understand it all at once. Like reading a book, we need to get the key idea first. . What is the key idea of this XMPP(Jabber) server, then? Let’s review some of the key usages of a typical XMPP session: . Alice started an XMPP client on her computer, which establishes a TCP connection to xmpp.example.org:5222. | The server authenticates the client by exchanging XML stanzas. | Alice uses the XMPP client to exchange messages/presences/iqs with the server, completing tasks such as instant messaging and presence notifying. | The essential task of the server, then, is to listen on a specific port, wait for clients or other servers to connect to it, and then exchange information using specific data formats(in the XMPP’s situation, XML stanzas). . That said, let’s examine the listener/socket code of ejabberd first, throwing aside other features along the way. . Notice: for the sake of clearity, I intentionally omitted a lot of code which are either unrelated to the topic discussed or only used for error handling. . Binding listener ports . In ejabberd, the whole server is packaged into a single OTP application. . The modules related to ejabberd’s socket infrastructure are: ejabberd_listener, ejabberd_socket and ejabberd_receiver. . The ejabberd_listener listens on every port specified in the ejabberd configuration file, spawns a process for each port and then accepts the sockets. When it finishes, it starts ejabberd_socket which in turn starts two processes: ejabberd_receiver and logic module according to the config (ejabberd_c2s for 5222, for example). The ejabberd_receiver is responsible for receiving any incoming packets and then forwarding them to the logic module. The logic module parses and handles the packets, and sends responses and requests using the ejabberd_socket utils. . Let’s start with the application module: . %% file: ejabberd_app.erl start(normal, _Args) -&gt; Sup = ejabberd_sup:start_link(), ejabberd_listener:start_listeners(). . In ejabberd_sup:start_link/0, ejabberd_listener:start_link/0 is invoked: . %% ejabberd_sup.erl init([]) -&gt; Listener = {ejabberd_listener, {ejabberd_listener, start_link, []}, permanent, infinity, supervisor, [ejabberd_listener]}. . Inside ejabberd_listener:init/0, the tcp and udp ports are bound according to the config file: . %% file: ejabberd_listener.erl init(_) -&gt; ets:new(listen_sockets, [named_table, public]), bind_tcp_ports(), {ok, { {one_for_one, 10, 1}, []}}. bind_tcp_ports() -&gt; case ejabberd_config:get_local_option(listen) of Ls -&gt; lists:foreach( fun({Port, Module, Opts}) -&gt; bind_tcp_port(Port, Module, Opts) end, Ls) end. bind_tcp_port(PortIP, Module, RawOpts) -&gt; %% portip has the following format: {5222, {0,0,0,0},tcp} {Port, IPT, IPS, IPV, Proto, OptsClean} = parse_listener_portip(PortIP, RawOpts), _Opts, SockOpts} = prepare_opts(IPT, IPV, OptsClean), %% save parsed listener options into ets table listen_tcp(PortIP, Module, SockOpts, Port, IPS). listen_tcp(PortIP, Module, SockOpts, Port, IPS) -&gt; gen_tcp:listen(Port, [binary, {packet, 0}, {active, false}, {reuseaddr, true}, {nodelay, true}, {send_timeout, ?TCP_SEND_TIMEOUT}, {keepalive, true} | SockOpts]). . After ejabberd_listener:init/0 is finished, all the ports specified in the config file are opened(in the listening state), but not accepting any incoming connections yet. . Accept incoming connections . Next, the sockets start accepting incoming connections in ejabberd_listener:start_listeners/0: . %% file: ejabberd_listener.erl start_listeners() -&gt; %% load listeners config from ets table Ls2 = lists:map( fun({Port, Module, Opts}) -&gt; start_listener(Port, Module, Opts) end end, Listeners). start_listener(Port, Module, Opts) -&gt; ChildSpec = {Port, {?MODULE, start, [Port, Module, Opts]}, transient, brutal_kill, worker, [?MODULE]}, supervisor:start_child(ejabberd_listeners, ChildSpec). start(Port, Module, Opts) -&gt; proc_lib:start_link(?MODULE, init, [Port, Module, Opts]). init(PortIP, Module, RawOpts) -&gt; {Port, IPT, IPS, IPV, Proto, OptsClean} = parse_listener_portip(PortIP, RawOpts), {Opts, SockOpts} = prepare_opts(IPT, IPV, OptsClean), init_tcp(PortIP, Module, Opts, SockOpts, Port, IPS) init_tcp(PortIP, Module, Opts, SockOpts, Port, IPS) -&gt; ListenSocket = listen_tcp(PortIP, Module, SockOpts, Port, IPS), proc_lib:init_ack({ok, self()}), accept(ListenSocket, Module, Opts). accept(ListenSocket, Module, Opts) -&gt; case gen_tcp:accept(ListenSocket) of {ok, Socket} -&gt; ejabberd_socket:start(Module, gen_tcp, Socket, Opts), accept(ListenSocket, Module, Opts); {error, Reason} -&gt; accept(ListenSocket, Module, Opts) end. . As we see the code above, the start_listeners/0 call spawns a new process for each listened ports, and then link it to the ejabberd_listeners supervisor, which is started in ejabberd_listener:start_link/0. This may seem odd to you at first, as it DOES for me, since normally a supervisor’s callbacks and its workers’ callbacks ought to be put in separate modules. Don’t write code like that; it causes confusions. . If you are careful enough, you might notice that the sockets are listened twice(once in bind_tcp_ports/0, once in start_listeners/0). I am not sure why it is implemented that way; it works perfectly if I removed one of them. . Start handling requests . Anyway, let’s continue to see what happens when a socket is accepted: . %% file: ejabberd_socket.erl start(Module, SockMod, Socket, Opts) -&gt; ReceiverMod = ejabberd_receiver, %% see explanation RecPid = ReceiverMod:start(Socket, SockMod, none, MaxStanzaSize), SocketData = #socket_state{sockmod = SockMod, socket = Socket, receiver = RecPid}, case Module:start({?MODULE, SocketData}, Opts) of {ok, Pid} -&gt; SockMod:controlling_process(Socket, Receiver) of ReceiverMod:become_controller(Receiver, Pid); {error, _Reason} -&gt; ReceiverMod:close(Receiver); end; . When a new socket is accepted, ejabberd_socket:start/4 is invoked to handle the socket events. ejabberd (brightly) splitted this task into two subtasks: one is to handle all the data transmition(sending and receiving, encapsulating the low-level socket implementation), the other is to parse and handle the message corresponding to the data. Hence, as we see here, a receiver process (ReceiverMod) and a logic handling process (Module) are spawned. The receiver module defaults to ejabberd_receiver, which receives XML stanzas and forwards the stanzas to the logic module. The logic module, on the other hand, may be ejabberd_c2s.erl or ejabberd_s2s depending for different tasks. . What ‘SockMod:controlling_process(Socket, Receiver)’ does is to direct all data sent to the socket to the receiver mod, which then can handle all the incoming data. After the logic module starts, ‘ReceiverMod:become_controller(Receiver, Pid)’ is called to let the receiver know where to forward incoming message to. . In a word, ejabberd starts a receiver and a handler when a socket is accepted, the receiver handles all incoming data, does some preprocessing(such as parsing the XML) and forward the message to the handler, the handler decides how to handle the message, and (maybe) use the ejabberd_socket util to send response data. If you want to extend the ejabberd to use other protocols than XMPP: this is the place to start. Write a customized receiver module to parse the protocol, a customized handler module to handle all the requests and you are done. More on this topic later. . To sum up . We have taken a quick tour through the socket infrastructure in ejabberd. We learned that the ejabberd uses three modules: ejabberd_listener, ejabberd_socket and ejabberd_receiver to handle all the socket related stuff. ejabberd_listener binds and listens on ports, ejabberd_socket starts the receiver and the handler, and provides utils for outgoing data, the receiver handles and parses all incoming data, and forwards messages to the logic module. The logics, on the other handle, are handled by logic modules accroding to the config file. There are, however, many things that we left out, including: . customization of receiver/logic modules. | congestion control (shapers). | how the logic modules interacts with the socket infrastructure. | So get your hands on the code piece by piece, make discoveries, and have fun! :) .",
            "url": "https://blog.codescv.com/erlang/ejabberd/ejabberd-notes-the-listener-infrastructure.html",
            "relUrl": "/erlang/ejabberd/ejabberd-notes-the-listener-infrastructure.html",
            "date": " • May 23, 2012"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Chi is now working at Google, before which he has worked for Xiaomi, Amazon and PDD as a software engineer. His experiences include data and machine learning, especially in Ads and NLP. . You can learn more about him in his LinkedIn page. .",
          "url": "https://blog.codescv.com/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  

  

  
  

  
      ,"page13": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://blog.codescv.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}